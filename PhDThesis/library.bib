Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Nuanain2014,
address = {Aalborg, Denmark},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and O'Sullivan, Liam},
booktitle = {AM '14 Proceedings of the 9th Audio Mostly: A Conference on Interaction With Sound},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Sullivan - 2014 - Real-time Algorithmic Composition with a Tabletop Musical Interface - A First Prototype and Performance.pdf:pdf},
isbn = {9781450330329},
keywords = {algorithmic composition,computer music,interfaces,table-},
title = {{Real-time Algorithmic Composition with a Tabletop Musical Interface - A First Prototype and Performance}},
year = {2014}
}
@article{Hulsen2008,
abstract = {BACKGROUND: In many genomics projects, numerous lists containing biological identifiers are produced. Often it is useful to see the overlap between different lists, enabling researchers to quickly observe similarities and differences between the data sets they are analyzing. One of the most popular methods to visualize the overlap and differences between data sets is the Venn diagram: a diagram consisting of two or more circles in which each circle corresponds to a data set, and the overlap between the circles corresponds to the overlap between the data sets. Venn diagrams are especially useful when they are 'area-proportional' i.e. the sizes of the circles and the overlaps correspond to the sizes of the data sets. Currently there are no programs available that can create area-proportional Venn diagrams connected to a wide range of biological databases. RESULTS: We designed a web application named BioVenn to summarize the overlap between two or three lists of identifiers, using area-proportional Venn diagrams. The user only needs to input these lists of identifiers in the textboxes and push the submit button. Parameters like colors and text size can be adjusted easily through the web interface. The position of the text can be adjusted by 'drag-and-drop' principle. The output Venn diagram can be shown as an SVG or PNG image embedded in the web application, or as a standalone SVG or PNG image. The latter option is useful for batch queries. Besides the Venn diagram, BioVenn outputs lists of identifiers for each of the resulting subsets. If an identifier is recognized as belonging to one of the supported biological databases, the output is linked to that database. Finally, BioVenn can map Affymetrix and EntrezGene identifiers to Ensembl genes. CONCLUSION: BioVenn is an easy-to-use web application to generate area-proportional Venn diagrams from lists of biological identifiers. It supports a wide range of identifiers from the most used biological databases currently available. Its implementation on the World Wide Web makes it available for use on any computer with internet connection, independent of operating system and without the need to install programs locally. BioVenn is freely accessible at http://www.cmbi.ru.nl/cdd/biovenn/.},
author = {Hulsen, Tim and de Vlieg, Jacob and Alkema, Wynand},
doi = {10.1186/1471-2164-9-488},
file = {:Users/carthach/Documents/Mendeley Desktop/Hulsen, de Vlieg, Alkema - 2008 - BioVenn - a web application for the comparison and visualization of biological lists using area-propor.pdf:pdf},
isbn = {1471-2164 (Electronic)$\backslash$r1471-2164 (Linking)},
issn = {1471-2164},
journal = {BMC genomics},
pages = {488},
pmid = {18925949},
title = {{BioVenn - a web application for the comparison and visualization of biological lists using area-proportional Venn diagrams.}},
volume = {9},
year = {2008}
}
@article{Wessel1979,
author = {Wessel, David},
file = {:Users/carthach/Documents/Mendeley Desktop/wessel-1979.pdf:pdf},
journal = {Computer Music Journal},
number = {2},
pages = {45--52},
title = {{Timbre space as a musical control structure}},
volume = {3},
year = {1979}
}
@book{Sicko2010,
abstract = {When it was originally published in 1999, Techno Rebels became the definitive text on a hard-to-define but vital genre of music. Author Dan Sicko demystified techno's characteristics, influences, and origins and argued that although techno enjoyed its most widespread popularity in Europe, its birthplace and most important incubator was Detroit. In this revised and updated edition, Sicko expands on Detroit's role in the birth of techno and takes readers on an insider's tour of techno's past, present, and future in an enjoyable account filled with firsthand anecdotes, interviews, and artist profiles.Techno Rebels begins by examining the underground 1980s party scene in Detroit, where DJs and producers like the Electrifying Mojo, Ken Collier, The Wizard, and Richard Davis were experimenting with music that was a world apart from anything happening in New York or Los Angeles. He details the early days of the "Belleville Three"—Juan Atkins, Derrick May, and Kevin Saunderson—who created the Detroit techno sound and became famous abroad as the sound spread to the UK and Europe. In this revised edition, Sicko delves deeper into the Detroit story, detailing the evolution of the artists and scene into the mid-1990s, and looks to nearby Ann Arbor to consider topics like the Electrifying Mojo's beginnings, the role of radio station WCBN, and the emergence of record label Ghostly International. Sicko concludes by investigating how Detroit techno functions today after the contrived electronica boom of the late 1990s, through the original artists, new sounds, and Detroit's annual electronic music festival.Ultimately, Sicko argues that techno is rooted in the "collective dreaming" of the city of Detroit—as if its originators wanted to preserve what was great about the city—its machines and its deep soul roots. Techno Rebels gives a thorough picture of the music itself and the trailblazing musicians behind it and is a must-read for all fans of techno, popular music, and contemporary culture.},
author = {Sicko, Dan},
file = {:Users/carthach/Documents/Mendeley Desktop/Sicko - 2010 - Techno Rebels The Renegades of Electronic Funk.pdf:pdf},
isbn = {0814334385},
pages = {163},
title = {{Techno Rebels: The Renegades of Electronic Funk}},
year = {2010}
}
@article{ONuanain2017a,
abstract = {In this article, we summarize recent research examining concatenative synthesis and its application and relevance in the composition and production of styles of electronic dance music. We introduce the conceptual underpinnings of concatenative synthesis and describe key works and systematic approaches in the literature. Our system, RhythmCAT, is proposed as a user-friendly system for generating rhythmic loops that model the timbre and rhythm of an initial target loop. The architecture of the system is explained, and an extensive evaluation of the system's performance and user response is discussed based on our results.},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Herrera, Perfecto and Jord{\`{a}}, Sergi},
doi = {10.1162/COMJ},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Herrera, Jord{\`{a}} - 2017 - Rhythmic Concatenative Synthesis for Electronic Music Techniques, Implementation, and Evaluation.pdf:pdf},
isbn = {8187672641},
issn = {1531-5169},
journal = {Computer Music Journal},
number = {2},
pages = {21--37},
title = {{Rhythmic Concatenative Synthesis for Electronic Music: Techniques, Implementation, and Evaluation}},
volume = {41},
year = {2017}
}
@article{Mikula2008a,
author = {Mikula, Luka},
file = {:Users/carthach/Documents/Mendeley Desktop/Mikula - 2008 - CONCATENATIVE MUSIC COMPOSITION BASED ON RECONTEXTUALISATION UTILISING RHYTHM-SYNCHRONOUS FEATURE EXTRACTION.pdf:pdf},
number = {September},
title = {{CONCATENATIVE MUSIC COMPOSITION BASED ON RECONTEXTUALISATION UTILISING RHYTHM-SYNCHRONOUS FEATURE EXTRACTION}},
year = {2008}
}
@article{Abdulla2002,
author = {Abdulla, Waleed H.},
file = {:Users/carthach/Documents/Mendeley Desktop/10.1.1.413.452.pdf:pdf},
isbn = {9608052718},
journal = {Advances in Communications and Software Technologies},
pages = {231--236},
title = {{Auditory Based Feature Vectors for Speech Recognition Systems}},
url = {https://drive.google.com/file/d/0B3N-C9xfG{\_}CPMlhYWWN2Vy0wUEU/view},
year = {2002}
}
@article{Goto2001a,
author = {Goto, Masataka},
doi = {10.1076/jnmr.30.2.159.7114},
file = {:Users/carthach/Documents/Mendeley Desktop/Goto - 2001 - An Audio-based Real-time Beat Tracking System for Music With or Without Drum-sounds.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {jun},
number = {2},
pages = {159--171},
title = {{An Audio-based Real-time Beat Tracking System for Music With or Without Drum-sounds}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.30.2.159.7114},
volume = {30},
year = {2001}
}
@article{Haworth2015,
abstract = {This paper presents new research in genre theory and analysis using digital ethnography methods. First, I address some of the problems and contradictions of genre in music, focusing on questions of agency, mediation, and the disavowal of genre by practitioners, art theorists, and critics. Second, I assess recent attempts to revive genre theory and make it into a workable concept for music, arguing that, alongside a focus on promiscuity, social mediation, and temporality, an attentiveness to ‘where' it is that genre communities come together - including both offline and online spaces - is essential. In the third section, I examine the art music genre of microsound, providing a brief genealogy of the genre from its coinage by Iannis Xenakis, to its takeup on the .microsound mailing list, to its present-day form as a concept embodying sound art, DSP research, avant-garde composition, and post-techno styles. Following the rhetoric of the genre itself, I propose that microsound can be conceived of as a ‘cloud' or ‘network' of human and non-human actors: none of them indigenous to the genre, all having different levels of influence, and all - crucially - contributing something essential to what the genre 'is'. The fourth section uses Richard Rogers' Issuecrawler (an online ethnography software) to analyse microsound's social mediations as they are performed online. Using the .microsound mailing list as a knowledge source, I analyse patterns of online interlinkage amongst artists, record labels, festivals, magazines, concert halls, and other institutions associated with the genre, furthering the insights of the earlier genealogy of microsound using these empirical methods. Further work would multiply the types of association amongst actors to include formal connections amongst artists and texts, amongst},
author = {Haworth, Christopher},
file = {:Users/carthach/Documents/Mendeley Desktop/pdf{\_}EMS15{\_}Haworth.pdf:pdf},
journal = {The Electroacoustic Music Studies Network Conference,},
number = {June},
pages = {1--10},
title = {{Participation Over Belonging : Analysing Microsound Using Digital Methods Introduction}},
url = {www.ems-network.org},
year = {2015}
}
@article{Papadopoulos1999,
abstract = {In this paper we survey the use of different AI methods for algorithmic composition, present their advantages and disadvantages, discuss some important general issues and propose desirable future prospects},
author = {Papadopoulos, George and Wiggins, Geraint},
file = {:Users/carthach/Documents/Mendeley Desktop/Papadopoulos, Wiggins - 1999 - AI Methods for Algorithmic Composition A Survey, a Critical View and Future Prospects.pdf:pdf},
journal = {AISB Symposium on Musical Creativity},
pages = {110--117},
title = {{AI Methods for Algorithmic Composition : A Survey, a Critical View and Future Prospects}},
year = {1999}
}
@article{Hackbarth2013,
abstract = {Presents AudioGuide, an innovative application for sound synthesis which aims to heighten compositional control of morphology in electronic music. We begin with a discussion of the challenges of managing detail when composing with computers, emphasizing the need for more tools which help the composer address the intricacies of sonic evolution. AudioGuide's approach—using a sound file as a method for specifying morphological shape—provides a simple yet exacting medium for representing temporal ideas. Using the spectral structure of a soundfile, AudioGuide organizes a user-defined collection of prerecorded sounds to create a similar morphological contour. Our matching strategy accounts for the spectral content, temporal evolution, and superimposition of sonic elements. We provide two examples which illuminate the capabilities of the algorithm from within a musical context.},
author = {Hackbarth, Benjamin and Schnell, Norbert and Esling, Philippe and Schwarz, Diemo},
doi = {10.1080/07494467.2013.774513},
file = {:Users/carthach/Documents/Mendeley Desktop/index.pdf:pdf},
issn = {07494467},
journal = {Contemporary Music Review},
keywords = {Audio Features,Concatenative Synthesis,Gestural Transcription,Timbre},
number = {1},
pages = {49--59},
title = {{Composing morphology: Concatenative synthesis as an intuitive medium for prescribing sound in time}},
volume = {32},
year = {2013}
}
@inproceedings{barbieri2012markov,
author = {Barbieri, Gabriele and Pachet, Fran{\c{c}}ois and Roy, Pierre and Esposti, Mirko Degli},
booktitle = {Proceedings of the 20th European Conference on Artificial Intelligence},
organization = {IOS Press},
pages = {115--120},
title = {{Markov constraints for generating lyrics with style}},
year = {2012}
}
@book{Reynolds2013,
author = {Reynolds, Simon},
publisher = {Faber {\&} Faber},
title = {{Energy Flash: A Journey Through Rave Music and Dance Culture}},
year = {2013}
}
@inproceedings{Hagberg2008,
address = {Pasadena, CA USA},
author = {Hagberg, Aric A and Schult, Daniel A and Swart, Pieter J},
booktitle = {Proceedings of the 7th Python in Science Conference},
editor = {Varoquaux, Ga{\"{e}}l and Vaught, Travis and Millman, Jarrod},
pages = {11--15},
title = {{Exploring Network Structure, Dynamics, and Function using NetworkX}},
year = {2008}
}
@article{Logan2000,
abstract = {We examine in some detail Mel Frequency Cepstral Coefficients (MFCCs) - the dominant features used for speech recognition - and investigate their applicability to modeling music. In particular, we examine two of the main assumptions of the process of forming MFCCs: the use of the Mel frequency scale to model the spectra; and the use of the Discrete Cosine Transform (DCT) to decorrelate the Mel-spectral vectors.},
author = {Logan, Beth},
doi = {10.1.1.11.9216},
file = {:Users/carthach/Documents/Mendeley Desktop/Logan - 2000 - Mel Frequency Cepstral Coefficients for Music Modeling.pdf:pdf},
journal = {International Symposium on Music Information Retrieval},
pages = {11p.},
title = {{Mel Frequency Cepstral Coefficients for Music Modeling}},
url = {http://ismir2000.ismir.net/papers/logan{\_}paper.pdf},
volume = {28},
year = {2000}
}
@article{Singh2012,
author = {Singh, Lavneet and Chetty, Girija and Singh, Savleen},
file = {:Users/carthach/Documents/Mendeley Desktop/3{\_}1{\_}79{\_}JISC.pdf:pdf},
journal = {Journal of Information Systems and Communication},
keywords = {- automatic speech recognition,erb gammatone filtering,hidden markov mod-,mel frequency cepstrum coefficients,mfcc,s},
number = {1},
pages = {358--364},
title = {{A Novel Algorithm Using MFCC and ERB Gammatone Filters in Speech Recognition}},
volume = {3},
year = {2012}
}
@article{Scipy2014,
author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
title = {{SciPy: Open Source Scientific Tools for Python}},
year = {2014}
}
@article{Delgado2009,
author = {Delgado, Miguel and Fajardo, Waldo and Molina-Solana, Miguel},
doi = {10.1016/j.eswa.2008.05.028},
file = {:Users/carthach/Documents/Mendeley Desktop/Delgado, Fajardo, Molina-Solana - 2009 - Inmamusys Intelligent multiagent music system.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {apr},
number = {3},
pages = {4574--4580},
publisher = {Elsevier Ltd},
title = {{Inmamusys: Intelligent multiagent music system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417408002194},
volume = {36},
year = {2009}
}
@article{grey1977perceptual,
author = {Grey, John M and Moorer, James A},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {454--462},
publisher = {ASA},
title = {{Perceptual evaluations of synthesized musical instrument tones}},
volume = {62},
year = {1977}
}
@book{ross2007rest,
author = {Ross, Alex},
publisher = {Macmillan},
title = {{The rest is noise: Listening to the twentieth century}},
year = {2007}
}
@article{Oliveira2012,
abstract = {In this paper we propose an audio beat tracking system, IBT, for multiple applications. The proposed system integrates an automatic monitoring and state recovery mechanism, that applies (re-)inductions of tempo and beats, on a multi-agent-based beat tracking architecture. This system sequentially processes a continuous onset detection function while propagating parallel hypotheses of tempo and beats. Beats can be predicted in a causal or in a non-causal usage mode, which makes the system suitable for diverse applications. We evaluate the performance of the system in both modes on two application scenarios: standard (using a relatively large database of audio clips) and streaming (using long audio streams made up of concatenated clips). We show experimental evidence of the usefulness of the automatic monitoring and state recovery mechanism in the streaming scenario (i.e., improvements in beat tracking accuracy and reaction time). We also show that the system performs efficiently and at a level comparable to state-of-the-art algorithms in the standard scenario. IBT is multi-platform, open-source and freely available, and it includes plugins for different popular audio analysis, synthesis and visualization platforms.},
author = {Oliveira, J L and Davies, M E P and Gouyon, F and Reis, L P},
doi = {10.1109/TASL.2012.2210878},
file = {:Users/carthach/Documents/Mendeley Desktop/Oliveira et al. - 2012 - Beat Tracking for Multiple Applications A Multi-Agent System Architecture With State Recovery.pdf:pdf},
issn = {1558-7916},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {Algorithm design and analysis,Audio beat tracking,IBT,Monitoring,Prediction algorithms,Real time systems,Speech,Standards,Timing,audio analysis,audio beat tracking system,audio clips,audio streaming,audio synthesis,automatic monitoring,beat-synchronous applications,concatenated clips,continuous onset detection function,diverse application,large database,multi-agent systems,multiagent system architecture,multimedia databases,music,musical rhythm,object tracking,parallel hypotheses,signal detection,state recovery mechanism,very large databases,visualization},
number = {10},
pages = {2696--2706},
title = {{Beat Tracking for Multiple Applications: A Multi-Agent System Architecture With State Recovery}},
volume = {20},
year = {2012}
}
@inproceedings{Nuanain2016a,
address = {Brisbane, Australia},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {New Interfaces for Musical Expression},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Jord{\`{a}}, Herrera - 2016 - An Interactive Software Instrument for Real-time Rhythmic Concatenative Synthesis.pdf:pdf},
title = {{An Interactive Software Instrument for Real-time Rhythmic Concatenative Synthesis}},
year = {2016}
}
@inproceedings{Biles2001,
address = {San Francisco, USA},
author = {Biles, John A},
booktitle = {Proceedings of the 2001 Genetic and Evolutionary Computation Conference Workshop Program},
title = {{Autonomous GenJam: eliminating the fitness bottleneck by eliminating fitness}},
year = {2001}
}
@book{Metzer2003,
abstract = {Throughout the 20th century, musicians frequently incorporated bits of works by other musicians into their own compositions and performances. When a musician borrows from a piece, he or she draws upon not only a melody but also the cultural associations of the original piece. By working with and altering a melody, a musician also transforms those associations. This vibrant practice is examined, considered how musicians used quotation to participate in the cultural dialogues sustained around such areas as race, childhood, madness, and the mass media. The focus is broad, discussing pieces in a spectrum of musical styles—classical, experimental, jazz, and popular—as well as works in the other arts. Chapters are organized around topics such as childhood and nostalgia in the works of Charles Ives, quotations in Duke Ellington's Black and tan fantasy, sampling and thievery and hip hop and beyond, and borrowing in Sandra Bernhard's Without you I'm nothing. A review article is cited as RILM [ref]2004-04715[/ref].},
author = {Metzer, David},
booktitle = {New perspectives in music history and criticism},
doi = {10.1525/jams.2005.58.3.736.2},
isbn = {0-521-82509-1},
keywords = {28: Western art music -- History. 1910 to present,38: Jazz and blues -- General,39: Popular music -- General,Bernhard,Charles -- works -- use of quotation,Edward Kennedy (Duke) -- works -- Black,Ellington,Ives,S.,Sandra -- performances -- Without you I',cultural studies -- quotation -- relation to ident,jazz -- quotation -- 20th c.,mass media -- relation to musical quotation -- 20t,performers--popular music -- Bernhard,popular music--general -- quotation -- 20th c. --,psychology -- madness -- depicted through quotatio,quotation -- 20th c. -- relation to culture,twentieth-century music -- quotation -- relation t},
title = {{Quotation and cultural meaning in twentieth-century music}},
url = {http://proxy.uchicago.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=rih{\&}AN=2003-05030{\&}site=ehost-live{\&}scope=site},
year = {2003}
}
@article{Baya,
author = {Bay, Mert and Burgoyne, John Ashley and Crawford, Tim and Roure, David De and Downie, J Stephen and Ehmann, Andreas and Fields, Benjamin and Fujinaga, Ichiro and Page, Kevin and Smith, Jordan B L},
file = {:Users/carthach/Documents/Mendeley Desktop/Bay et al. - Unknown - Structural Analysis of Large Amounts of Music Information.pdf:pdf},
pages = {1--15},
title = {{Structural Analysis of Large Amounts of Music Information}}
}
@article{Shao2009,
abstract = {A conventional automatic speech recognizer does not perform well in the presence of noise, while human listeners are able to segregate and recognize speech in noisy conditions. We study a novel feature based on an auditory periphery model for robust speech recognition. Specifically, gammatone frequency cepstral coefficients are derived by applying a cepstral analysis on gammatone filterbank responses. Our evaluations show that the proposed feature performs considerably better than conventional acoustic features. We further demonstrate that integrating the proposed feature with a computational auditory scene analysis system yields promising recognition performance.},
author = {Shao, Yang and Jin, Zhaozhang and Wang, Deliang and Srinivasan, Soundararajan},
doi = {10.1109/ICASSP.2009.4960661},
file = {:Users/carthach/Documents/Mendeley Desktop/04960661.pdf:pdf},
isbn = {9781424423545},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Auditory feature,Computational auditory scene analysis,Gammatone frequency cepstral coefficients,Robust speech recognition},
number = {1},
pages = {4625--4628},
title = {{An auditory-based feature for robust speech recognition}},
year = {2009}
}
@article{Eigenfeldt2012,
abstract = {We present a music composition system in which musical motives are treated as individuals within a population, and that the audible evolution of populations over time are of musical interest. The system additionally uses genetic algorithms to generate high level musical aspects that control how the population is presented, and how it may be combined with other populations. These algorithms feature fitness functions that adapt based upon context: specifically, by using an analysis of the evolving population, the fitness functions adjust their constituent parameters in selecting strong individuals.},
author = {Eigenfeldt, Arne and Pasquier, Philippe},
doi = {10.1007/978-3-642-29142-5_7},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt, Pasquier - 2012 - Populations of populations Composing with multiple evolutionary algorithms.pdf:pdf},
isbn = {9783642291418},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biologically inspired music,evolutionary music,genetic algorithms},
pages = {72--83},
title = {{Populations of populations: Composing with multiple evolutionary algorithms}},
volume = {7247 LNCS},
year = {2012}
}
@phdthesis{Golod2009,
author = {Golod, Daniil},
file = {:Users/carthach/Documents/Mendeley Desktop/Golod - 2009 - The k -best paths in Hidden Markov Models . Algorithms and Applications to Transmembrane Protein Topology Recognition.pdf:pdf},
school = {University of Waterloo},
title = {{The k -best paths in Hidden Markov Models . Algorithms and Applications to Transmembrane Protein Topology Recognition}},
year = {2009}
}
@book{Cope1991,
author = {Cope, David},
publisher = {A-R Editions},
title = {{Computers and Musical Style}},
year = {1991}
}
@inproceedings{Nuanain2016b,
address = {Paris, France},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {International Workshop on Musical Metacreation (MUME)},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Jord{\`{a}}, Herrera - 2016 - Towards User-Tailored Creative Applications of Concatenative Synthesis in Electronic Dance Music.pdf:pdf},
isbn = {9780864913975},
title = {{Towards User-Tailored Creative Applications of Concatenative Synthesis in Electronic Dance Music}},
year = {2016}
}
@article{taube2004notes,
author = {Taube, Heinrich},
publisher = {Taylor {\&} Francis Group},
title = {{Notes from the metalevel: introduction to algorithmic music composition}},
year = {2004}
}
@article{Biles2001,
author = {Biles, Ja},
file = {:Users/carthach/Documents/Mendeley Desktop/Biles - 2001 - Autonomous GenJam eliminating the fitness bottleneck by eliminating fitness.pdf:pdf},
issn = {0955-6036},
journal = {Proceedings of the 2001 Genetic and Evolutionary Computation Conference},
title = {{Autonomous GenJam: eliminating the fitness bottleneck by eliminating fitness}},
url = {http://www.ist.rit.edu/{~}jab/GECCO01/},
year = {2001}
}
@inproceedings{Duignan2005,
abstract = {Sequencing tools play a central role in our ability to create computer music. Despite their importance, there has been little structured analysis of how the characteristics of sequencers impact our ability to use them effectively. This paper addresses this through a new taxonomy for classifying sequencing tools. This taxonomy can be used to help us better understand the strengths and weaknesses of current sequencer tools, and suggest novel possibilities for future sequencers.},
author = {Duignan, Matthew and Noble, James and Biddle, Robert},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/cr1260.pdf:pdf},
keywords = {Computer Music,Sequencer,Taxonomy,Visual Progra},
title = {{A taxonomy of sequencer user-interfaces}},
url = {http://www.music.mcgill.ca/{~}ich/research/misc/papers/cr1260.pdf},
year = {2005}
}
@incollection{Bartok1993,
abstract = {None of Bartok's examples in his commentary mention 7-rhythms. Of his 320 collected examples, several are in irregular metres, but most are in 4/4 or 2/4, with a few 3/4, the occasional 6/8, and a few 5/4s. Only example 34 (as far as I could tell on a quick scan) is in 7/8 (see attached file and sibelius file). The lyrics are innocent enough - about a girl's dress, although one version leads to her fiance killing someone. There are 23 verses!},
author = {Bart{\'{o}}k, B{\'{e}}la},
booktitle = {B{\'{e}}la Bart{\'{o}}k Essays},
doi = {10.2307/848835},
pages = {3--4},
title = {{Hungarian Folk Music}},
year = {1993}
}
@article{Lacker2013,
author = {Lacker, Ben},
file = {:Users/carthach/Documents/Mendeley Desktop/Lacker - 2013 - Jazz Drum Machine A Novel Interface for Data-Driven Remixing of Music.pdf:pdf},
journal = {Ninth Artificial Intelligence and Interactive Digital {\ldots}},
keywords = {AAAI Technical Report WS-13-22},
pages = {64--66},
title = {{Jazz Drum Machine: A Novel Interface for Data-Driven Remixing of Music}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewPDFInterstitial/7459/7677},
year = {2013}
}
@article{Diakopoulos2009,
abstract = {The performance of electronica by Disc Jockys (DJs) presents a unique opportunity to develop interactions between performer and music. Through recent research in the MIR field, new tools for expanding DJ performance are emerging. The use of spectral, loudness, and temporal descriptors for the classification of electronica is explored. Our research also introduces the use of a multitouch interface to drive a performance-oriented DJ application utilizing the feature set. Furthermore, we present that a multi-touch surface provides an extensible and collaborative interface for browsing and manipulating MIRrelated data in real time.},
author = {Diakopoulos, D and Vallis, O and Hochenbaum, J and Murphy, J and Kapur, a},
journal = {10th International Society for Music Information Retrieval Conference},
keywords = {classification,dj,electronic dance music,electronica,genre,multi touch,user interfaces},
number = {Ismir},
pages = {465--469},
title = {{21st Century Electronica: MIR Techniques for Classification and Performance}},
year = {2009}
}
@article{Whitman2001b,
author = {Whitman, Brian and Flake, Gary and Lawrence, Steve},
file = {:Users/carthach/Documents/Mendeley Desktop/Whitman, Flake, Lawrence - 2001 - Artist detection in music with minnowmatch.pdf:pdf},
isbn = {0780371968},
journal = {Neural Networks for Signal {\ldots}},
keywords = {audio databases,information retrieval,machine learning,machine listening,music retrieval,networks,neural,support vector machines},
number = {C},
pages = {559--568},
title = {{Artist detection in music with minnowmatch}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=943160},
volume = {00},
year = {2001}
}
@phdthesis{Donahue2013,
author = {Donahue, Chris},
file = {:Users/carthach/Documents/Mendeley Desktop/Donahue - 2013 - Applications of genetic programming to digital audio synthesis.pdf:pdf},
school = {The University of Texas at Austin},
title = {{Applications of genetic programming to digital audio synthesis}},
year = {2013}
}
@book{bernstein,
abstract = {The varied forms of Leonard Bernstein's musical creativity have been recognized and enjoyed by millions. These lectures, Mr. Bernstein's most recent venture in musical explication, will make fascinating reading as well. Virgil Thomson says of the lectures: "Nobody anywhere presents this material so warmly, so sincerely, so skillfully. As musical mind-openers they are first class; as pedagogy they are matchless".Mr. Bernstein considers music ranging from Hindu ragas through Mozart and Ravel, to Copland, suggesting a worldwide, innate musical grammar. Folk music, pop songs, symphonies, modal, tonal, atonal, well-tempered and ill-tempered works all find a place in these discussions. Each, Mr. Bernstein suggests, has roots in a universal language central to all artistic creation. Using certain linguistic analogies, he explores the ways in which this language developed and can be understood as an aesthetic surface. Drawing on his insights as a master composer and conductor, Mr. Bernstein also explores what music means below the surface: the symbols and metaphors which exist in every musical piece, of whatever sort. And, finally, Mr. Bernstein analyzes twentieth century crises in the music of Schoenberg and Stravinsky, finding even here a transformation of all that has gone before, as part of the poetry of expression, through its roots in the earth of human experience.These talks, written and delivered when Leonard Bernstein was Charles Eliot Norton Professor of Poetry at Harvard University, are the newest of the author's literary achievements. In addition to a distinguished career as conductor, pianist, and composer, Mr. Bernstein is the recipient of many television Emmys for the scripts of his Young People's Concerts, Omnibus programs, and others, and is the author of The Infinite Variety of Music and The Joy of Music, for which he received the Christopher Award.},
address = {Cambridge},
author = {Bernstein, Leonard},
booktitle = {Harvard University Press},
doi = {10.2307/3395135},
isbn = {0674920015},
issn = {00274321},
number = {6},
pages = {19},
title = {{The Unanswered Question: Six Talks at Harvard}},
volume = {63},
year = {1977}
}
@phdthesis{DErrico2011,
author = {D'Errico, Michael A.},
file = {:Users/carthach/Documents/Mendeley Desktop/MikeDErricoMAthesis.pdf:pdf},
pages = {86},
school = {Tufts University},
title = {{Behind the Beat : Technical and Practical Aspects of Instrumental Hip-Hop Composition}},
url = {http://www.cs.tufts.edu/{~}jacob/250hcm/MikeDErricoMAthesis.pdf},
year = {2011}
}
@article{Roma2010,
author = {Roma, Gerard and Herrera, P},
file = {:Users/carthach/Documents/Mendeley Desktop/Roma, Herrera - 2010 - Graph grammar representation for collaborative sample-based music creation.pdf:pdf},
isbn = {9781450300469},
journal = {Proceedings of the 5th Audio Mostly Conference},
keywords = {collaborative composition,graph grammars,network music},
title = {{Graph grammar representation for collaborative sample-based music creation}},
url = {http://dl.acm.org/citation.cfm?id=1859816},
year = {2010}
}
@book{Collins2011a,
author = {Collins, Nick and D'Escrivan, Julio},
file = {:Users/carthach/Documents/Mendeley Desktop/The Cambridge Companion to Elec - Nick Collins.pdf:pdf},
isbn = {978-1-139-00176-2},
keywords = {Electronic music,History and critic},
publisher = {Cambridge University Press},
title = {{The Cambridge companion to electronic music}},
url = {http://ezproxy.lib.ed.ac.uk/login?url=http://dx.doi.org/10.1017/CCOL9780521868617},
year = {2017}
}
@article{Bird2016,
author = {Bird, Steven},
doi = {10.3115/1225403.1225421},
file = {:Users/carthach/Documents/Mendeley Desktop/Bird - 2016 - NLTK The natural language toolkit NLTK The Natural Language Toolkit.pdf:pdf},
journal = {Proceedings of the COLING/ACL on Interactive presentation sessions},
number = {March},
pages = {69--72},
title = {{NLTK : The natural language toolkit NLTK : The Natural Language Toolkit}},
year = {2016}
}
@inproceedings{Pachet2002,
abstract = {We propose a system, the Continuator, that bridges the gap between two classes of traditionally incompatible musical systems: 1) interactive musical systems, limited in their ability to generate stylistically consistent material, and 2) music imitation systems, which are fundamentally not inter- active. Our purpose is to allow musicians to extend their technical ability with stylistically consistent, automatically learnt material. This goal requires the ability for the system to build operational representations of musical styles in a real time context. Our approach is based on a Markov model of musical styles augmented to account for musical issues such as management of rhythm, beat, harmony, and imprecision. The resulting system is able to learn and generate music in any style, either in standalone mode, as continuations of musician's input, or as interactive improvisation back up. Lastly, the very design of the system makes possible new modes of musical collaborative playing. We describe the architecture, implementation issues and experimentations conducted with the system in several real world contexts.},
author = {Pachet, Fran{\c{c}}ois},
booktitle = {Proceedings of the International Computer Music Conference},
doi = {10.1076/jnmr.32.3.333.16861},
file = {:Users/carthach/Documents/Mendeley Desktop/pachet-02f.pdf:pdf},
isbn = {0929-8215},
issn = {0929-8215},
pages = {333--341},
title = {{The Continuator: Musical Interaction With Style}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1076/jnmr.32.3.333.16861{\&}magic=crossref{\%}7C{\%}7CD404A21C5BB053405B1A640AFFD44AE3},
year = {2002}
}
@article{Yen1971,
abstract = {This paper presents an algorithm for finding the K loopless paths that have the shortest lengths from one node to another node in a network. The significance of the new algorithm is that its computational upper bound increases only linearly with the value of K. Consequently, in general, the new algorithm is extremely efficient as com- pared with the algorithms proposed by Bock, Kantner, and Haynes [2], Pollack [7], [81, Clarke, Krikorian, and Rausan [3], Sakarovitch [9] and others. This paper first reviews the algorithms presently available for finding the K shortest loopless paths in terms of the computational effort and memory addresses they require. This is followed by the presentation of the new algorithm and its justification. Finally, the efficiency of the new algorithm is examined and compared with that of other algorithms.},
author = {Yen, J. Y.},
doi = {10.1287/mnsc.17.11.712},
isbn = {00251909},
issn = {0025-1909},
journal = {Management Science},
number = {11},
pages = {712--716},
title = {{Finding the K Shortest Loopless Paths in a Network}},
volume = {17},
year = {1971}
}
@article{Alonso2007,
abstract = {This article describes a method to estimate and track the tempo of musical recordings which was submitted to the MIREX 2006 evaluation contest where it was ranked third out of seven submissions. The algorithm that we present is composed of three stages: first a front-end analyses the audio signal in order to extract a representation of the musically relevant events, the so-called "detection function". Then, the periodicity of these events is estimated in contiguous and overlapping excerpts of the detection function signal. Finally, the periodicities are tracked through time and the most energetic are selected as tempi. [ABSTRACT FROM AUTHOR]},
author = {Alonso, Miguel and Richard, Ga{\"{e}}l and David, Bertrand},
doi = {10.1080/09298210701653260},
file = {:Users/carthach/Documents/Mendeley Desktop/Alonso, Richard, David - 2007 - Tempo estimation for audio recordings.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
number = {1},
pages = {17--25},
title = {{Tempo estimation for audio recordings}},
volume = {36},
year = {2007}
}
@inproceedings{Schl2007,
address = {Hawaii, USA},
author = {Schl{\"{u}}ter, R and Bezrukov, I and Wagner, H and Ney, H},
booktitle = {Acoustics, Speech and Signal Processing, ICASSP 2007},
file = {:Users/carthach/Documents/Mendeley Desktop/04218184.pdf:pdf},
isbn = {1424407281},
pages = {4--7},
title = {{Gammatone Features and Feature Combination For Large Vocabulary Speech Recognition}},
year = {2007}
}
@inproceedings{Kobayashi,
author = {Kobayashi, Ryoho},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Kobayashi - 2003 - Sound Clustering Synthesis Using Spectral Data.pdf:pdf},
number = {1},
pages = {1--3},
title = {{Sound Clustering Synthesis Using Spectral Data}},
volume = {0},
year = {2003}
}
@article{fitch2013rhythmic,
author = {Fitch, W Tecumseh},
journal = {Frontiers in systems neuroscience},
publisher = {Frontiers Media SA},
title = {{Rhythmic cognition in humans and animals: distinguishing meter and pulse perception}},
volume = {7},
year = {2013}
}
@phdthesis{Martin2017,
author = {Mart{\'{i}}n, Sergio Oramas},
file = {:Users/carthach/Documents/Mendeley Desktop/PhDThesis{\_}SergioOramas{\_}online{\_}B5.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Knowledge Extraction and Representation Learning for Music Recommendation and Classification}},
year = {2017}
}
@article{Latartara2010,
abstract = {Laptop composition – the creation and performance of music primarily using laptop computers – emerged as an important musical activity in the last decade of the twentieth century. While much has been written about the cultural and conceptual significance of this new music, less has been published regarding the sonic structure of specific works. This article explores the musical structure and design of compositions by three laptop composers at the turn of the millennium: ‘Untitled {\#}2' by Oval (Markus Popp), ‘Cow Cow' by Merzbow (Masami Akita), and ‘Powerbookfiend' by Kid606 (Miguel De Pedro). Each piece is analysed using spectrographic images, representations of musical sound that allow for the precise measurement of frequency and intensity. Repetition and noise are revealed as musical characteristics common to all three pieces, defining both smaller-scale patterns and large-scale designs. Using the conceptual vocabulary of Paul Virilio and Gilles Deleuze, repetition and noise are framed in relation to a ‘machine aesthetic' and ‘difference and repetition'.},
author = {Latartara, John},
doi = {10.1017/S1478572211000065},
file = {:Users/carthach/Documents/Mendeley Desktop/laptop{\_}composition{\_}at{\_}the{\_}turn{\_}of{\_}the{\_}millennium{\_}repetition{\_}and{\_}noise{\_}in{\_}the{\_}music{\_}of{\_}oval{\_}merzbow{\_}and{\_}kid606.pdf:pdf},
isbn = {1478-5722},
issn = {1478-5722},
journal = {Twentieth-Century Music},
number = {01},
pages = {91--115},
title = {{Laptop Composition at the Turn of the Millennium: Repetition and Noise in the Music of Oval, Merzbow, and Kid606}},
url = {http://www.journals.cambridge.org/abstract{\_}S1478572211000065},
volume = {7},
year = {2010}
}
@article{Hainsworth2003,
abstract = {This paper presents work on changepoint detection in musi- cal audio signals, focusing on the case where there are note changes with low associated energy variation. Several meth- ods are described and results of the best are presented},
author = {Hainsworth, Stephen and Macleod, Malcolm},
file = {:Users/carthach/Documents/Mendeley Desktop/Hainsworth, Macleod - 2003 - Onset detection in musical audio signals.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {163--166},
title = {{Onset detection in musical audio signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.7782{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@article{Neupert2013,
abstract = {We present an innovative remix-instrument that allows to create music from a collection of audio-visual media frag- ments. A three-dimensional scatter plot derived from feature- analysis becomes a Theremin-inspired instrument that en- ables exploration, intuitive navigation and embodied perfor- mance of the media fragments on a granular level. As exam- ple, we are using the audiovisual recording of an instrumental performance as source for interactive remix. Our system provides an alternative interface to the mu- sical instrument's audiovisual corpus: as the instrument's sound and behavior can be accessed in ways which are not possible on the original instrument, the resulting non-linear playback of the grains generates an instant remix in a cut-up aesthetic. The presented system is a novel strategy to access and re-use media fragments in the domain of musical inter- faces.},
author = {Neupert, Max and Gossmann, Joachim},
doi = {10.1109/ICMEW.2013.6618343},
file = {:Users/carthach/Documents/Mendeley Desktop/06618343.pdf:pdf},
isbn = {9781479916047},
journal = {Electronic Proceedings of the 2013 IEEE International Conference on Multimedia and Expo Workshops, ICMEW 2013},
keywords = {Audio-visual,Cut-Up,Embodiment,Instrument,Remix},
title = {{A remix instrument based on fragment feature-analysis}},
year = {2013}
}
@article{Nuanain2017b,
abstract = {In this article, we summarize recent research examining concatenative synthesis and its application and relevance in the composition and production of styles of electronic dance music. We introduce the conceptual underpinnings of concatenative synthesis and describe key works and systematic approaches in the literature. Our system, RhythmCAT, is proposed as a user-friendly system for generating rhythmic loops that model the timbre and rhythm of an initial target loop. The architecture of the system is explained, and an extensive evaluation of the system's performance and user response is discussed based on our results.},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Herrera, Perfecto and Jord{\`{a}}, Sergi},
doi = {10.1162/COMJ},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Herrera, Jord{\`{a}} - 2017 - Rhythmic Concatenative Synthesis for Electronic Music Techniques, Implementation, and Evaluation.pdf:pdf},
isbn = {8187672641},
issn = {1531-5169},
journal = {Computer Music Journal},
keywords = {kyma,systems},
number = {2},
pages = {21--37},
title = {{Rhythmic Concatenative Synthesis for Electronic Music: Techniques, Implementation, and Evaluation}},
volume = {41},
year = {2017}
}
@article{grey1978perceptual,
author = {Grey, John M and Gordon, John W},
journal = {The Journal of the Acoustical Society of America},
number = {5},
pages = {1493--1500},
publisher = {ASA},
title = {{Perceptual effects of spectral modifications on musical timbres}},
volume = {63},
year = {1978}
}
@article{Bonada2016,
author = {Bonada, Jordi and Lachlan, Robert and Blaauw, Merlijn},
file = {:Users/carthach/Documents/Mendeley Desktop/Bonada, Lachlan, Blaauw - 2016 - Bird Song Synthesis Based on Hidden Markov Models Music Technology Group , Universitat Pompeu Fabra , S.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {2582--2586},
title = {{Bird Song Synthesis Based on Hidden Markov Models Music Technology Group , Universitat Pompeu Fabra , Spain}},
year = {2016}
}
@article{Stevens1937,
author = {Stevens, Stanley Smith and Volkmann, John and Newman, Edwin B},
journal = {The Journal of the Acoustical Society of America},
number = {3},
pages = {185--190},
publisher = {ASA},
title = {{A scale for the measurement of the psychological magnitude pitch}},
volume = {8},
year = {1937}
}
@inproceedings{Eigenfeldt2016,
author = {Eigenfeldt, Arne and Bown, Oliver and Brown, Andrew R and Gifford, Toby},
booktitle = {International Conference on Computational Creativity},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt et al. - 2016 - Flexible Generation of Musical Form Beyond Mere Generation.pdf:pdf},
title = {{Flexible Generation of Musical Form : Beyond Mere Generation}},
year = {2016}
}
@inproceedings{Hsu2009,
author = {Hsu, William and Sosnick, Marc},
booktitle = {Proceedings of New Interfaces for Musical Expression},
file = {:Users/carthach/Documents/Mendeley Desktop/Hsu, Sosnick - 2009 - Evaluating interactive music systems An HCI approach.pdf:pdf},
keywords = {[Electronic Manuscript],evaluation tests,human computer,interaction,interactive music systems},
pages = {25--28},
title = {{Evaluating interactive music systems: An HCI approach}},
url = {http://userwww.sfsu.edu/whsu/IMSHCI/NIME{\_}Initial{\_}Submission.pdf},
year = {2009}
}
@article{Gartland2003,
author = {Gartland-Jones, Andrew and Copley, Peter},
journal = {Contemporary Music Review},
number = {3},
pages = {43--55},
publisher = {Taylor {\&} Francis},
title = {{The suitability of genetic algorithms for musical composition}},
volume = {22},
year = {2003}
}
@article{Chang2008,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:Users/carthach/Documents/Mendeley Desktop/guide.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Bello2006,
abstract = {This paper addresses the issue of drum sound classification in the context of automatic rhythm modification of drum loops. The proposed method segments the signal using an onset detection algorithm, characterises segmented sounds using a spectral feature set, and classifies them using k-means clustering. We propose a simple taxonomy for the grouping of different instrumental sounds under a few utilitarian labels. Results demonstrate the adequacy of our proposed taxonomy while showing that our classification approach outperforms commonly-used supervised learning techniques},
author = {Bello, J.R. and Ravelli, E. and Sandler, M.B.},
doi = {10.1109/ICASSP.2006.1661255},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello, Ravelli, Sandler - 2006 - Drum Sound Analysis for the Manipulation of Rhythm in Drum Loops.pdf:pdf},
isbn = {1-4244-0469-X},
issn = {1520-6149},
journal = {2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings},
pages = {8--11},
title = {{Drum Sound Analysis for the Manipulation of Rhythm in Drum Loops}},
volume = {5},
year = {2006}
}
@article{Bock2012b,
abstract = {In this paper, we evaluate various onset detection algo- rithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27,774 an- notated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detec- tion method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with audio signals of various volume levels.},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Schedl, Markus},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck, Krebs, Schedl - 2012 - Evaluating the online capabilities of onset detection methods.pdf:pdf},
journal = {13th International Society for Music Information Retrieval Conference},
pages = {49--54},
title = {{Evaluating the online capabilities of onset detection methods}},
url = {http://ismir2012.ismir.net/event/papers/049-ismir-2012.pdf},
year = {2012}
}
@article{Valero2012,
abstract = {In the context of non-speech audio recognition and classification for multimedia applications, it becomes essential to have a set of features able to accurately represent and discriminate among audio signals. Mel frequency cepstral coefficients (MFCC) have become a de facto standard for audio parameterization. Taking as a basis the MFCC computation scheme, the Gammatone cepstral coefficients (GTCCs) are a biologically inspired modification employing Gammatone filters with equivalent rectangular bandwidth bands. In this letter, the GTCCs, which have been previously employed in the field of speech research, are adapted for non-speech audio classification purposes. Their performance is evaluated on two audio corpora of 4 h each (general sounds and audio scenes), following two cross-validation schemes and four machine learning methods. According to the results, classification accuracies are significantly higher when employing GTCC rather than other state-of-the-art audio features. As a detailed analysis shows, with a similar computational cost, the GTCC are more effective than MFCC in representing the spectral characteristics of non-speech audio signals, especially at low frequencies.},
author = {Valero, Xavier and Alias, Francesc},
doi = {10.1109/TMM.2012.2199972},
file = {:Users/carthach/Documents/Mendeley Desktop/06202347.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Audio classification,Gammatone cepstral coefficients,audio scene recognition,environmental sound,feature extraction},
number = {6},
pages = {1684--1689},
title = {{Gammatone cepstral coefficients: Biologically inspired features for non-speech audio classification}},
volume = {14},
year = {2012}
}
@article{bellman1958routing,
author = {Bellman, Richard},
journal = {Quarterly of applied mathematics},
number = {1},
pages = {87--90},
title = {{On a routing problem}},
volume = {16},
year = {1958}
}
@article{Bullock2007,
abstract = {The libxtract library consists of a collection of over forty functions that can be used for the extraction of low level audio features. In this paper I will describe the develop- ment and usage of the library as well as the rationale for its design. Its use in the composition and performance of music involving live electronics also will be discussed. A number of use case scenarios will be presented, including the use of individual features and the use of the library to create a 'feature vector', which may be used in conjunc- tion with a classification algorithm, to extract higher level features. 1.},
author = {Bullock, Jamie},
file = {:Users/carthach/Documents/Mendeley Desktop/Bullock - 2007 - LibXtract A Lightweight Library for Audio Feature Extraction.pdf:pdf},
journal = {Proc. International Computer Music Conference},
pages = {3--6},
title = {{LibXtract: A Lightweight Library for Audio Feature Extraction}},
url = {http://www.scopus.com/inward/record.url?partnerID=HzOxMe3b{\&}scp=84924989679{\&}origin=inward{\%}5Cnpapers3://publication/uuid/a52b5c02-03a5-45d2-a682-7432b9ce2208},
year = {2007}
}
@phdthesis{Collins2006a,
author = {Collins, Nick},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2006 - Towards autonomous agents for live computer music Realtime machine listening and interactive music systems.pdf:pdf},
number = {October 2003},
school = {University of Cambridge},
title = {{Towards autonomous agents for live computer music: Realtime machine listening and interactive music systems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Towards+Autonomous+Agents+for+Live+Computer+Music+:+Realtime+Machine+Listening+and+Interactive+Music+Systems{\#}0},
year = {2006}
}
@inproceedings{Grill,
author = {Grill, Thomas},
booktitle = {2nd International Linux Audio Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Grill - 2004 - C layer for Pure Data {\&} MaxMSP externals.pdf:pdf},
title = {{C++ layer for Pure Data {\&} Max/MSP externals}},
year = {2004}
}
@article{Herrera2000,
author = {Herrera, Perfecto and Amatriain, Xavier},
journal = {{\ldots} symposium on music {\ldots}},
keywords = {classification,multimedia content,music content processing,segmentation,timbre models},
title = {{Towards instrument segmentation for music content description: a critical review of instrument classification techniques}},
url = {http://users.cis.fiu.edu/{~}lli003/Music/cla/15.pdf},
year = {2000}
}
@article{Thomson2004,
abstract = {Microsound is an emerging approach to music composition and analysis which places emphasis on extremely brief time-scales, usually a tenth of a second or less, as well as an integration of this micro-time level with the time-levels of sound gestures, sections, movements and whole pieces. This paper summarises some of the technical issues involved in microsonic analysis/composition, traces a history of microsonic techniques in contemporary music, and examines some of its aesthetic implications in a social context.},
author = {Thomson, Phil},
doi = {10.1017/S1355771804000299},
file = {:Users/carthach/Documents/Mendeley Desktop/atomsanderrors.pdf:pdf},
isbn = {1355-7718},
issn = {1355-7718},
journal = {Organised Sound},
number = {02},
pages = {1--37},
title = {{Atoms and errors: towards a history and aesthetics of microsound}},
url = {http://www.journals.cambridge.org/abstract{\_}S1355771804000299},
volume = {9},
year = {2004}
}
@article{Grandvalet2007,
author = {Grandvalet, Yves and Eck, Douglas},
file = {:Users/carthach/Documents/Mendeley Desktop/Grandvalet, Eck - 2007 - A Generative Model for Rhythms.pdf:pdf},
journal = {Neural Information Processing Systems, Workshop on Brain, Music and Cognition},
pages = {1--8},
title = {{A Generative Model for Rhythms}},
year = {2007}
}
@article{Tzanetakis2002,
abstract = {The majority of existing work in music information retrieval for audio signals has followed the content-based query-by-example paradigm. In this paradigm a musical piece is used as a query and the result is a list of other musical pieces ranked by their content similarity. In this paper we describe algorithms and graphical user interfaces that enable novel alternative ways for querying and browsing large audio collections. Computer audition algorithms are used to extract content information from audio signals. This automatically extracted information is used to configure the graphical user interfaces and to genereate new query audio signals for browsing and retrieval.},
author = {Tzanetakis, G and Ermolinskyi, A and Cook, P},
file = {:Users/carthach/Documents/Mendeley Desktop/c276bbfa6246f52fd3a89d1487b0d31862c4.pdf:pdf},
journal = {ICMC Proceedings},
pages = {1--7},
title = {{Beyond the Query-By-Example Paradigm: New Query Interfaces for Music Information Retrieval}},
year = {2002}
}
@article{Sioros2011,
author = {Sioros, George and Guedes, Carlos},
file = {:Users/carthach/Documents/Mendeley Desktop/Sioros, Guedes - 2011 - COMPLEXITY DRIVEN RECOMBINATION OF MIDI LOOPS.pdf:pdf},
number = {Ismir},
pages = {381--386},
title = {{COMPLEXITY DRIVEN RECOMBINATION OF MIDI LOOPS}},
year = {2011}
}
@article{Wiggins2012a,
abstract = {This chapter is about computational modelling of the process of musical composition, based on a cognitive model of human behaviour. The idea is to try to study not only the requirements for a computer system which is capable of musical composition, but also to relate it to human behaviour during the same process, so that it may, perhaps, work in the same way as a human composer, but also so that it may, more likely, help us understand how human composers work. Pearce et al. (2002) give a fuller discussion of the motivations behind this endeavour. $\backslash$r$\backslash$n$\backslash$r$\backslash$nWe take a purist approach to our modelling: we are aiming, ultimately, at a computer system which we can claim to be creative. Therefore, we must address in advance the criticism that usually arises in these circumstances: “a computer can't be creative because it can only do what it has explicitly been programmed to do”. This argument does not hold, because, with the advent of machine learning, it is no longer true that a computer is limited to what its programmer explicitly tells it, especially in an unsupervised learning task like composition (as compared with the usually-supervised task of learning, say, the piano). Thus, a creative system based on machine learning can, in principle, be given credit for creative output, much as Wolfgang Amadeus Mozart is deemed the creator of the Magic Flute, and not Leopold Mozart,Wolfgang's father, teacher and de facto agent.},
author = {Wiggins, Geraint A. and Pearce, Marcus T. and M{\"{u}}llensiefen, Daniel},
doi = {10.1093/oxfordhb/9780199792030.013.0019},
file = {:Users/carthach/Documents/Mendeley Desktop/Computational{\_}Modeling{\_}of{\_}Music{\_}Cognition{\_}and{\_}Musi.pdf:pdf},
isbn = {9780199940233},
journal = {The Oxford Handbook of Computer Music},
keywords = {Computational modeling,Human behavior,Musical composition,Musical phrase segmentation,Tonal melodies},
number = {January},
title = {{Computational Modeling of Music Cognition and Musical Creativity}},
year = {2012}
}
@misc{DE,
title = {{DEAF CD 2009}},
url = {http://deafireland.com/2009/?page{\_}id=16},
urldate = {2014-01-01},
year = {2009}
}
@article{Song2013,
author = {Song, Chunyang and Simpson, Andrew J R and Harte, Christopher A and Pearce, Marcus T and Sandler, Mark B},
journal = {PloS one},
number = {9},
pages = {e74692},
publisher = {Public Library of Science},
title = {{Syncopation and the Score}},
volume = {8},
year = {2013}
}
@book{lerdahljackendoff,
author = {Lerdahl, Fred and Jackendoff, Ray},
publisher = {MIT press},
title = {{A generative theory of tonal music}},
year = {1985}
}
@article{Cooprider2007,
author = {Cooprider, Nathan D and Burton, Robert P},
doi = {10.1117/12.703359},
file = {:Users/carthach/Documents/Mendeley Desktop/star{\_}coordinates.pdf:pdf},
isbn = {0819466085},
issn = {0277786X},
journal = {Electronic Imaging 2007},
keywords = {cluster discovery,multi-variate data visualization},
number = {801},
pages = {64950Q--64950Q},
title = {{Extension of Star Coordinates into three dimensions}},
volume = {6495},
year = {2007}
}
@article{Zhu2006,
author = {Zhu, Huming and Jiao, Licheng and Pan, Jin},
file = {:Users/carthach/Documents/Mendeley Desktop/Zhu, Jiao, Pan - 2006 - Multi-population Genetic Algorithm for Feature.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {480--487},
title = {{Multi-population Genetic Algorithm for Feature}},
volume = {4222/2006},
year = {2006}
}
@inproceedings{Cano2004,
abstract = {Sound engineers need to access vast collections of sound effects for their film and video productions. Sound effects providers rely on text-retrieval techniques to offer their collections. Currently, annotation of audio content is done manually, which is an arduous task. Automatic annotation methods, normally fine-tuned to reduced domains such as musical instruments or reduced sound effects taxonomies, are not mature enough for labeling with great detail any possible sound. A general sound recognition tool would require: first, a taxonomy that represents the world and, second, thousands of classifiers, each specialized in distinguishing little details. We report experimental results on a general sound annotator. To tackle the taxonomy definition problem we use WordNet, a semantic network that organizes real world knowledge. In order to overcome the need of a huge number of classifiers to distinguish many different sound classes, we use a nearest-neighbor classifier with a database of isolated sounds unambiguously linked to WordNet concepts. A 30{\%} concept prediction is achieved on a database of over 50,000 sounds and over 1600 concepts ER -},
author = {Cano, Pedro and Koppenberger, Markus},
booktitle = {Proceedings of the 2004 14th IEEE Signal Processing Society Workshop Machine Learning for Signal Processing, 2004.},
doi = {10.1109/MLSP.2004.1422998},
file = {:Users/carthach/Documents/Mendeley Desktop/01422998.pdf:pdf},
isbn = {0-7803-8608-4},
issn = {1551-2541},
pages = {391--400},
title = {{Automatic sound annotation}},
url = {http://ieeexplore.ieee.org/document/1422998/},
year = {2004}
}
@misc{IntelResearch,
author = {{Intel Research}},
title = {{Internet Killed the Video Star: Mobile Apps Redefine the Album Experience}},
url = {http://iq.intel.com/internet-killed-the-video-star-mobile-apps-redefine-the-album-experience/}
}
@book{Nyman1999,
author = {Nyman, Michael},
edition = {2nd},
file = {:Users/carthach/Documents/Mendeley Desktop/Unknown - Unknown - Nyman{\_}Michael{\_}Experimental{\_}Music{\_}Cage{\_}and{\_}Beyond{\_}2nd{\_}ed.pdf.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Experimental Music: Cage and Beyond (Music in the Twentieth Century)}},
year = {1999}
}
@article{Fathima2013,
author = {Fathima, Rahana and Raseena, P E},
file = {:Users/carthach/Documents/Mendeley Desktop/63{\_}rahana{\_}fathima.pdf:pdf},
journal = {International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering},
keywords = {Feature extraction,Feature matching,Gammatone Ce},
number = {10},
pages = {795--798},
title = {{Gammatone Cepstral Coefficient for Speaker Identification}},
volume = {4},
year = {2013}
}
@phdthesis{Tamagawa1988,
abstract = {The status of the Frech composer Claude Debussy (1862-1918) as one of the great musical revolutionaries of the early twentieth century is undisputed. One aspect of his originality often mentioned but seldom discussed in detail is his interest in Far Eastern music, particularly that of the Javanese gamelan, the ensemble of percussion instruments first encountered by Debussy at the Paris Universal Exposition of 1889.   Beginning with a recounting of Debussy's musical background and influences prior to that year, the author then examines the composer's writings about the music of Java as well as commentaries and transcriptions by other musicians of the time interested in the Exposition gamelan. From this evidence a list of musical criteria indicating possible Javanese influence in Debussy's music is formulated. Applying these criteria to numerous piano, vocal, chamber and orchestral works produced after 1889 reveals, first, that the gamelan was one of the important catalysts for the flowering of Debussy's mature style in the 1890's; second, that the music of Java left its mark on the composer's work in a much broader, more profound way than is generally supposed.},
author = {Tamagawa, Kiyoshi},
booktitle = {ProQuest Dissertations and Theses},
keywords = {0413:Music,Communication and the arts,Music},
pages = {166--166 p.},
title = {{Echoes from the East: The Javanese gamelan and its influence on the music of Claude Debussy}},
url = {http://search.proquest.com/docview/303578419?accountid=13771},
year = {1988}
}
@article{Faraldo2017a,
author = {Faraldo, {\'{A}}ngel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
file = {:Users/carthach/Documents/Mendeley Desktop/faraldo{\_}paper.pdf:pdf},
journal = {Proceedings of International Conference on Technologies for Music Notation and Representation},
title = {{The House Harmonic Filler: Interactive Exploration of Chord Sequences by Means of an Intuitive Representation}},
year = {2017}
}
@article{Wang2003a,
author = {Wang, A},
journal = {ISMIR},
title = {{An Industrial Strength Audio Search Algorithm.}},
url = {http://www.ee.columbia.edu/{~}dpwe/papers/Wang03-shazam.pdf},
year = {2003}
}
@article{deltorn2017deep,
author = {Deltorn, Jean-Marc},
journal = {Frontiers in Digital Humanities},
pages = {3},
publisher = {Frontiers},
title = {{Deep creations: Intellectual Property and the Automata}},
volume = {4},
year = {2017}
}
@article{Dzhambazov2016,
abstract = {Lyrics-to-audio alignment aims to automatically match given lyrics and musical audio. In this work we extend a state of the art approach for lyrics-to-audio alignment with information about note onsets. In particular, we consider the fact that transition to next lyrics syllable usually im-plies transition to a new musical note. To this end we for-mulate rules that guide the transition between consecutive phonemes when a note onset is present. These rules are in-corporated into the transition matrix of a variable-time hid-den Markov model (VTHMM) phonetic recognizer based on MFCCs. An estimated melodic contour is input to an automatic note transcription algorithm, from which the note onsets are derived. The proposed approach is evalu-ated on 12 a cappella audio recordings of Turkish Makam music using a phrase-level accuracy measure. Evaluation of the alignment is also presented on a polyphonic version of the dataset in order to assess how degradation in the ex-tracted onsets affects performance. Results show that the proposed model outperforms a baseline approach unaware of onset transition rules. To the best of our knowledge, this is the one of the first approaches tackling lyrics tracking, which combines timbral features with a melodic feature in the alignment process itself.},
author = {Dzhambazov, Georgi and Srinivasamurthy, Ajay},
file = {:Users/carthach/Documents/Mendeley Desktop/Dzhambazov, Srinivasamurthy - 2016 - on the Use of Note Onsets for Improved Lyrics-To-Audio Alignment in Turkish Makam Music.pdf:pdf},
journal = {Proc. 17th International Society for Music Information Retrieval Conference},
pages = {716--722},
title = {{on the Use of Note Onsets for Improved Lyrics-To-Audio Alignment in Turkish Makam Music}},
year = {2016}
}
@article{Battenberg2012,
author = {Battenberg, Eric and Huang, Victor and Wessel, David},
file = {:Users/carthach/Documents/Mendeley Desktop/Battenberg, Huang, Wessel - 2012 - Live Drum Separation Using Probabilistic Spectral Clustering Based on the Itakura-Saito Divergence.pdf:pdf},
journal = {parlab.eecs.berkeley.edu},
pages = {1--10},
title = {{Live Drum Separation Using Probabilistic Spectral Clustering Based on the Itakura-Saito Divergence}},
url = {http://parlab.eecs.berkeley.edu/sites/all/parlab/files/Live Drum Separation Using Probabilistic Spectral Clustering Based on the Itakura-Saito Divergence.pdf},
year = {2012}
}
@inproceedings{Mikula2008,
author = {Mikula, Luka and Sontacchi, Alois and H{\"{o}}ldrich, Robert},
booktitle = {Tonmeistertagung - VDT International Convention},
file = {:Users/carthach/Documents/Mendeley Desktop/Mikula, Sontacchi, H{\"{o}}ldrich - 2008 - Sound Art - Synthesis Based on Rhythm and Feature Extraction.pdf:pdf},
number = {November},
title = {{Sound Art - Synthesis Based on Rhythm and Feature Extraction}},
year = {2008}
}
@misc{Krumhansl1989,
author = {Krumhansl, Carol L},
booktitle = {Structure and perception of electroacoustic sound and music},
file = {:Users/carthach/Documents/Mendeley Desktop/Why{\_}Is{\_}Musical{\_}Timbre{\_}so hard{\_}to{\_}understand.pdf:pdf},
pages = {43--53},
title = {{Why is musical timbre so hard to understand ?}},
volume = {9},
year = {1989}
}
@book{cope2009hidden,
author = {Cope, David},
publisher = {AR Editions, Inc.},
title = {{Hidden structure: music analysis using computers}},
year = {2009}
}
@misc{Traunmuller1997,
author = {Traunm{\"{u}}ller, Hartmut},
title = {{Auditory scales of frequency representation}},
url = {http://www2.ling.su.se/staff/hartmut/bark.htm},
urldate = {2015-07-01},
year = {1997}
}
@article{Bock2015,
abstract = {In this paper we present a new tempo estimation algorithm which uses a bank of resonating comb filters to determine the dominant periodicity of a musical excerpt. Unlike ex-isting (comb filter based) approaches, we do not use hand-crafted features derived from the audio signal, but rather let a recurrent neural network learn an intermediate beat-level representation of the signal and use this information as in-put to the comb filter bank. While most approaches apply complex post-processing to the output of the comb filter bank like tracking multiple time scales, processing differ-ent accent bands, modelling metrical relations, categoris-ing the excerpts into slow / fast or any other advanced pro-cessing, we achieve state-of-the-art performance on nine of ten datasets by simply reporting the highest resonator's histogram peak.},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Widmer, Gerhard},
file = {:Users/carthach/Documents/Mendeley Desktop/Boeck{\_}etal{\_}ISMIR{\_}2015.pdf:pdf},
journal = {Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)},
pages = {625--631},
title = {{Accurate Tempo Estimation based on Recurrent Neural Networks and Resonating Comb Filters}},
year = {2015}
}
@inproceedings{Schwarz2011,
address = {Padova, Italy},
author = {Schwarz, Diemo},
booktitle = {Sound and Music Computing Conference (SMC)},
file = {:Users/carthach/Documents/Mendeley Desktop/smc2011{\_}submission{\_}127.pdf:pdf},
title = {{Distance Mapping for Corpus-Based Concatenative Synthesis}},
year = {2011}
}
@book{winkler2001composing,
author = {Winkler, Todd},
publisher = {MIT press},
title = {{Composing interactive music: techniques and ideas using Max}},
year = {2001}
}
@book{Puckette2006,
author = {Puckette, Miller},
doi = {10.1186/1471-2105-11-50},
file = {:Users/carthach/Documents/Mendeley Desktop/book.pdf:pdf},
isbn = {9789812700773},
issn = {14712105},
pages = {1--337},
pmid = {20100357},
publisher = {World Scientific Publishing Co Inc},
title = {{The Theory and Technique of Electronic Music}},
url = {http://www.amazon.com/Theory-Technique-Electronic-Music/dp/9812700773},
volume = {11},
year = {2006}
}
@misc{Lyons2015,
author = {Lyons, James},
file = {:Users/carthach/Documents/Mendeley Desktop/fb466970fd0780f6d767b773c1726b04e60f56a7.html:html},
title = {{Mel Frequency Cepstral Coefficient (MFCC) tutorial [Practical Cryptography]}},
url = {http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/},
urldate = {2017-11-14},
year = {2015}
}
@article{Smith2000,
abstract = {This article discusses the compositional strategies of turntablist DJs working within the hip-hop genre, focusing on processes developed by these musicians such as mixing, scratching and beat juggling, all of which are carried out on turntables. Since the development of the gramophone at the turn of the century, the turntable has become an instrument of creation as well as reproduction, resulting in the ground-breaking compositional strategies of hip-hop music. Hip-hop DJs create original music from a range of existing musical texts and in doing so, raise questions concerning originality and authorship as well as questioning the division between composer and performer. Hip-hop music is regarded by authors such as Poschardt (1998) as one of the final avant-gardes of the twentieth century; this article will explore and discuss the ground-breaking nature of the genre with reference to new compositional strategies and the turntable technology with which they are carried out.},
author = {Smith, Sophy},
doi = {10.1017/S135577180000203X},
file = {:Users/carthach/Documents/Mendeley Desktop/compositional-strategies-of-the-hip-hop-turntablist.pdf:pdf},
issn = {1469-8153},
journal = {Organised Sound},
keywords = {scratching},
number = {2},
pages = {75--79},
publisher = {Biblioteca de la Universitat Pompeu Fabra},
title = {{Compositional strategies of the hip-hop turntablist}},
volume = {5},
year = {2000}
}
@article{VanDerMaaten2008,
abstract = {KNAW Narcis. Back to search results. Publication - - (2008). Pagina-navigatie: Main. Title, - - . Published in, Journal of Machine Learning Research, Vol. 9, No. nov, p.2579-2605.},
author = {{Van Der Maaten}, L J P and Hinton, G E},
doi = {10.1007/s10479-011-0841-3},
file = {:Users/carthach/Documents/Mendeley Desktop/Van Der Maaten, Hinton - 2008 - Visualizing high-dimensional data using t-sne.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
pmid = {20652508},
title = {{Visualizing high-dimensional data using t-sne}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=7911431479148734548related:VOiAgwMNy20J},
volume = {9},
year = {2008}
}
@article{Davies2007,
abstract = {We present a simple and efficient method for beat tracking of musical audio. With the aim of replicating the human ability of tapping in time to music, we formulate our approach using a two state model. The first state performs tempo induction and tracks tempo changes, while the second maintains contextual continuity within a single tempo hypothesis. Beat times are recovered by passing the output of an onset detection function through adaptively weighted comb filterbank matrices to separately identify the beat period and alignment. We evaluate our beat tracker both in terms of the accuracy of estimated beat locations and computational complexity. In a direct comparison with existing algorithms, we demonstrate equivalent performance at significantly reduced computational cost},
author = {Davies, Matthew E P and Plumbley, Mark D.},
doi = {10.1109/TASL.2006.885257},
file = {:Users/carthach/Documents/Mendeley Desktop/Davies, Plumbley - 2007 - Context-dependent beat tracking of musical audio.pdf:pdf},
isbn = {0000000000000},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Beat tracking,Musical meter,Onset detection,Rhythm analysis},
number = {3},
pages = {1009--1020},
title = {{Context-dependent beat tracking of musical audio}},
volume = {15},
year = {2007}
}
@article{Brossier2004,
abstract = {We present a new system for the estimation of note at- tributes from a live monophonic music source, within a short time delay and without any previous knowledge of the signal. The labelling is based on the temporal segmen- tation and the successive estimation of the fundamental frequency of the current note object. The setup, imple- mented around a small C library, is directed at the robust note segmentation of a variety of audio signals. A system for evaluation of performances is also presented. The fur- ther extension to polyphonic signals is considered, as well as design concerns such as portability and integration in other software environments.},
author = {Brossier, Paul M and Bello, Juan P and Plumbley, Mark D},
file = {:Users/carthach/Documents/Mendeley Desktop/Brossier, Bello, Plumbley - 2004 - Fast labelling of notes in music signals.pdf:pdf},
isbn = {84-88042-44-2},
journal = {Proceedings of the International Symposium on Music Information Retrieval (ISMIR)},
pages = {331----336},
title = {{Fast labelling of notes in music signals}},
url = {http://aubio.org/articles/brossier04fastnotes.pdf},
year = {2004}
}
@inproceedings{Dupont2013,
author = {Dupont, Stephane and Ravet, Thierry and Picard-Limpens, Cecile and Frisson, Christian},
booktitle = {Proceedings - IEEE International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2013.6607550},
file = {:Users/carthach/Documents/Mendeley Desktop/22a8bd269c0f40f085c30228690050d94875.pdf:pdf},
isbn = {9781479900152},
issn = {19457871},
keywords = {Dimensionality reduction,audio and music analysis,multimedia information retrieval},
title = {{Nonlinear dimensionality reduction approaches applied to music and textural sounds}},
year = {2013}
}
@article{Gomez2005,
author = {G{\'{o}}mez, Francisco and Melvin, A and Rappaport, David and Toussaint, Godfried T.},
file = {:Users/carthach/Documents/Mendeley Desktop/G{\'{o}}mez, Melvin - 2005 - Mathematical measures of syncopation.pdf:pdf},
journal = {BRIDGES: Mathematical Connections in Art, Music and Science},
title = {{Mathematical measures of syncopation}},
url = {http://archive.bridgesmathart.org/2005/bridges2005-73.html},
year = {2005}
}
@article{Schwarz2006,
author = {Schwarz, Diemo and Beller, G and Verbrugghe, B and Britton, S},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz et al. - 2006 - Real-Time Corpus-Based Concatenative Synthesis with CataRT.pdf:pdf},
journal = {Proceedings of the 9th International Conference on Digital Audio Effects},
pages = {18--21},
title = {{Real-Time Corpus-Based Concatenative Synthesis with CataRT}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.9605{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Toussaint2004,
author = {Toussaint, GT},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint - 2004 - A Comparison of Rhythmic Similarity Measures.pdf:pdf},
journal = {ISMIR},
pages = {3--6},
title = {{A Comparison of Rhythmic Similarity Measures.}},
url = {http://www.cs.mcgill.ca/research/techreports/reports/2004/SOCS-TR-2004.6.pdf},
year = {2004}
}
@article{Davies2014b,
author = {Davies, Matthew E P and Hamel, Philippe and Yoshii, Kazuyoshi and Goto, Masataka},
journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
number = {12},
pages = {1726--1737},
publisher = {IEEE Press},
title = {{AutoMashUpper: Automatic creation of multi-song music mashups}},
volume = {22},
year = {2014}
}
@article{Dixon2006a,
author = {Dixon, S},
file = {:Users/carthach/Documents/Mendeley Desktop/Dixon - 2006 - Onset detection revisited.pdf:pdf},
journal = {Proceedings of the 9th International Conference on Digital Audio Effects (DAFx-06)},
pages = {133--137},
title = {{Onset detection revisited}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.414.8898{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@article{Jorda2005a,
abstract = {This is a dissertation about performing music with computers, and about constructing the tools that will facilitate playing and improvising with these computers. The primary aim of this research is to construct a theoretical framework that could serve in evaluating the potential, the possibilities and the diversity of new digital musical instruments, with the hope that these ideas may inspire and assist the construction of new and powerful instruments with which perform and listen to wonderful new and previously unheard music. Computer-based interactive music systems date back to the late 1960s, initially involving computer-controlled analog synthesizers in concerts or installations. The use of real-time algorithmic composition spread in the 1970s with the work of composers and performers such as David Behrman, Joel Chadabe, Salvatore Martirano, Gordon Mumma or Laurie Spiegel. However the most rapid period of growth probably occurred during the mid 1980s with the MIDI standardization and, subsequently, with the advent of data-flow graphical programming languages such as Max, which made the design and implementation of custom interactive systems simpler than ever before. In spite of this, nearly four decades after the works of these pioneers, the design of computer-based music instruments, and computer music performance and improvisation in general, still seem immature multidisciplinary areas in which knowledge does not behave in incremental and accumulative ways, resulting in the permanent textquoterightreinvention of the wheeltextquoteright. New digital instrument design is a broad field, encompassing highly technological areas (e.g. electronics and sensor technology, sound synthesis and processing techniques, software engineering, etc.), and disciplines related to the study of human behavior (e.g. psychology, physiology, ergonomics and human-computer interaction components, etc.). Much of this focused research attempts to solve independent parts of the problem an approach essential to achieve any progress in this field. However, as this dissertation will show, it is also clearly insufficient. I believe an approach dedicated to the integrated understanding of the whole is the key to achieving fruitful results. Integral studies and approaches, which consider not only ergonomic or technological but also psychological, philosophical, conceptual, musicological, historical and above all, musical issues, even if non-systematic by definition, are necessary for genuine progress. Putting forward the idea that a digital instrument is a conceptual whole, independent of its potential components and features (e.g. the ways it is controlled or its sonic or musical output tendencies), we will investigate the essence and the potential highlights of new digital instruments, the new musical models and the new music making paradigms they can convey. This dissertation begins with the assumption that better new musical instruments based on computers can only be conceived by exploring three parallel paths identifying the quintessence of new digital instruments; what they can bring of really original to the act of music performance; how can they redefine it; identifying the drawbacks or obsolescences of traditional instruments; what limitations or problems could be eliminated, improved or solved; without forgetting the essential generic assets of traditional instruments; those qualities that should never be forgotten nor discarded. The identification of these points is the primary aim of this thesis. There is a complex interconnected relationship between the tasks of imagining, designing and crafting musical computers, and performing and improvising with them. This relationship can only be understood as a permanent work in progress. This thesis comes from my own experience of fifteen years as a luthier-improviser. Therefore the dissertation is both theoretical (or conceptual) and experimental in approach, although the experiments it documents span years, even decades. To better organize this, the thesis is divided in three parts. Part I progressively enlightens the aforementioned three fundamental exploration paths. This is achieved by introducing the new possibilities offered by digital instruments, in addition to providing a thorough overview of current know-how and of the technical and conceptual frameworks in which new instrument designers and researchers are currently working on. Several taxonomies that will help us in developing a more synthetic and clear overview of the whole subject, are also presented. This first part concludes in chapter seven, presenting the first fundamental contribution of this dissertation; a theoretical framework for the evaluation of the expressive possibilities new digital musical instruments can offer to their performers. Part II describes in depth seven musical instruments, the implementations of my journeys into Digital Lutherie, developed during the previous decade. Since all seven are conceptually very different, each of them serves to illustrate several paradigms introduced in Part I. Presented in chronological order, these music instrument also help to clarify and understand of the path that has led me to the conception of the framework previously introduced. Part III incorporates the teachings and conclusions resulting from this evolutionary journey, and present the final milestone of this dissertation the presentation of possible solutions to better accomplish the goals presented at the end of the part I. Finally this dissertation concludes with what could be considered textquoterightmy digital lutherie decaloguetextquoteright which synthesizes most of the ideas introduced in the thesis. As a postlude, I offer the reacTable to be presented as future work. The reacTable is a digital instrument which constitutes the first one conceived from scratch, that takes into account all the concepts introduced in this thesis, the culmination thus far of my journey into Digital Lutherie},
author = {Jord{\`{a}}, Sergi},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}} - 2005 - Digital Lutherie Crafting musical computers for new musics' performance and improvisation.pdf:pdf},
journal = {Departament de Tecnologia},
number = {3},
pages = {531},
title = {{Digital Lutherie: Crafting musical computers for new musics' performance and improvisation}},
url = {http://dialnet.unirioja.es/servlet/tesis?codigo=19509},
volume = {26},
year = {2005}
}
@misc{Schwarz2005,
author = {Schwarz, Diemo},
booktitle = {IRCAM},
title = {{Corpus-Based Sound Synthesis Survey}},
urldate = {2017-02-01},
year = {2005}
}
@article{Gillet2004a,
abstract = { Recent efforts in audio indexing and retrieval in music databases mostly focus on melody. If this is appropriate for polyphonic music signals, specific approaches are needed for systems dealing with percussive audio signals such as those produced by drums, tabla or djembe. Most studies of drum signal transcription focus on sounds taken in isolation. In this paper, we propose several methods for drum loop transcription where the drums signals dataset reflects the variability encountered in modern audio recordings (real and natural drum kits, audio effects, simultaneous instruments, etc.). The approaches described are based on hidden Markov models (HMM) and support vector machines (SVM). Promising results are obtained with a 83.9{\%} correct recognition rate for a simplified taxonomy.},
author = {Gillet, O. and Richard, G.},
doi = {10.1109/ICASSP.2004.1326815},
file = {:Users/carthach/Documents/Mendeley Desktop/Gillet, Richard - 2004 - Automatic transcription of drum loops.pdf:pdf},
isbn = {0-7803-8484-9},
issn = {1520-6149},
journal = {2004 IEEE International Conference on Acoustics, Speech, and Signal Processing},
pages = {2--5},
pmid = {1326815},
title = {{Automatic transcription of drum loops}},
volume = {4},
year = {2004}
}
@article{Chai2006a,
author = {Chai, Wei},
file = {:Users/carthach/Documents/Mendeley Desktop/Chai - 2006 - Semantic Segmentation and Summarization of Music - Methods based on tonality and recurrent structure.pdf:pdf},
journal = {Signal Processing Magazine, IEEE},
number = {March},
pages = {124--132},
title = {{Semantic Segmentation and Summarization of Music - Methods based on tonality and recurrent structure}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1598088},
year = {2006}
}
@inproceedings{Whitman2001,
abstract = {In this paper we demonstrate the artist detection component of Minnowmatch, a machine listening and music retrieval engine. Minnowmatch (Mima) automatically determines various meta-data and makes classifications concerning a piece of audio using neural networks and support vector machines. The technologies developed in Minnowmatch may be used to create audio information retrieval systems, copyright protection devices, and recommendation agents. This paper concentrates on the artist or source detection component of Mima, which we show to classify a one-in-n artist space correctly 91{\%} over a small song-set and 70{\%} over a larger song set. We show that scaling problems using only neural networks for classification can be addressed with a pre-classification step of multiple support vector machines},
author = {Whitman, B. and Flake, G. and Lawrence, S.},
booktitle = {Neural Networks for Signal Processing XI: Proceedings of the 2001 IEEE Signal Processing Society Workshop (IEEE Cat. No.01TH8584)},
doi = {10.1109/NNSP.2001.943160},
file = {:Users/carthach/Documents/Mendeley Desktop/Whitman, Flake, Lawrence - 2001 - Artist detection in music with minnowmatch.pdf:pdf},
isbn = {0-7803-7196-8},
issn = {1089-3555},
keywords = {Audio databases,Copyright protection,Engines,Mima,Minnowmatch,Multiple signal classification,Music information retrieval,National electric code,Neural networks,Space technology,Support vector machine classification,Support vector machines,artist detection,audio coding,audio information retrieval systems,classifications,copyright protection devices,information retrieval,learning (artificial intelligence),learning automata,machine listening,machine listening and music retrieval engine,meta-data,music,music retrieval,neural nets,one-in-n artist space,recommendation agents},
pages = {559--568},
publisher = {IEEE},
shorttitle = {Neural Networks for Signal Processing XI, 2001. Pr},
title = {{Artist detection in music with Minnowmatch}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=943160},
year = {2001}
}
@inproceedings{Patterson1987,
address = {Malvern, UK},
author = {Patterson, Roy and Nimmo-Smith, Ian and Holdsworth, John and Rice, Peter},
booktitle = {a meeting of the IOC Speech Group on Auditory Modelling at RSRE},
file = {:Users/carthach/Documents/Mendeley Desktop/An{\_}efficient{\_}auditory{\_}filterbank{\_}based{\_}o.pdf:pdf},
number = {December},
pages = {14--15},
title = {{An Efficient Auditory Filterbank Based On The Gammatone Function}},
year = {1987}
}
@book{Lochhead2002,
abstract = {These essays on postmodernism and music include topics such as the importance of technology and marketing in postmodern music, the appropriation and reworking of Western music by non-Western bands, and issues of music and race.},
author = {Lochhead, Judith Irene and Auner, Joseph Henry},
booktitle = {Studies in contemporary music and culture v. 4},
isbn = {0815338198},
keywords = {Music Philosophy and aesthetics.,Postmodernism.},
pages = {xi, 372 p.},
pmid = {12986612},
title = {{Postmodern music/postmodern thought}},
url = {http://www.loc.gov/catdir/toc/fy034/2002514829.html{\%}5Cnhttp://www.loc.gov/catdir/enhancements/fy0652/2002514829-d.html},
year = {2002}
}
@article{Collins2007a,
abstract = {1In 1956, John Cage predicted that “in the future, records will be made from records” (Duffel, 202). Certainly, musical creativity has always involved a certain amount of appropriation and adaptation of previous works. For example, Vivaldi appropriated and adapted the “Cum sancto spiritu” fugue of Ruggieri‟s Gloria (Burnett, 4; Forbes, 261). If stuck for a guitar solo on stage, Keith Richards admits that he‟ll adapt Buddy Holly for his own purposes (Street, 135). Similarly, Nirvana adapted the opening riff from Killing Jokes‟ “Eighties” for their song “Come as You Are”. Musical “quotation” is actively encouraged in jazz, and contemporary hip-hop would not exist if the genre‟s pioneers and progenitors had not plundered and adapted existing recorded music. Sampling technologies, however, have taken musical adaptation a step further and realised Cage‟s prediction. Hardware and software samplers have developed to the stage where any piece of audio can be appropriated and adapted to suit the creative impulses of the sampling musician (or samplist). The practice of sampling challenges established notions of creativity, with whole albums created with no original musical input as most would understand it—literally “records made from records.”},
author = {Collins, Steve},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2007 - Amen to that.pdf:pdf},
journal = {M/C Journal},
keywords = {Art,Australia,borrowing,digital media,musical borrowing,quotation,sampling,substantialit,t music,y copyrigh},
number = {2},
title = {{Amen to that}},
volume = {10},
year = {2007}
}
@book{Schedl2014a,
author = {Schedl, Markus and Gomez, Emilia and Urbano, Julian},
booktitle = {Foundations and Trends in Information Retrieval},
doi = {10.1561/1500000045},
file = {:Users/carthach/Documents/Mendeley Desktop/Schedl, Gomez, Urbano - 2014 - Music Information Retrieval Recent Developments and Applications.pdf:pdf},
isbn = {9781601988065},
issn = {1554-0669},
number = {2-3},
pages = {127--261},
title = {{Music Information Retrieval: Recent Developments and Applications}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-information-retrieval/INR-045},
volume = {8},
year = {2014}
}
@article{Ozerov2012,
author = {Ozerov, Alexey and Vincent, Emmanuel and Bimbot, Fr{\'{e}}d{\'{e}}ric},
file = {:Users/carthach/Documents/Mendeley Desktop/Ozerov, Vincent, Bimbot - 2012 - A general flexible framework for the handling of prior information in audio source separation.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {4},
pages = {1118--1133},
title = {{A general flexible framework for the handling of prior information in audio source separation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6047568},
volume = {20},
year = {2012}
}
@article{Pope2004,
author = {Pope, Stephen Travis and Holm, Frode and Kouznetsov, Alexandre},
file = {:Users/carthach/Documents/Mendeley Desktop/Pope, Holm, Kouznetsov - 2004 - Feature Extraction and Database Design for Music Software Dimensions of MDB Applications.pdf:pdf},
pages = {1--8},
title = {{Feature Extraction and Database Design for Music Software * Dimensions of MDB Applications}},
year = {2004}
}
@article{Bischoff2012,
author = {Bischoff, John and Gold, Rich and Horton, Jim and Bischoff, John and Gold, Rich and Horton, Jim},
file = {:Users/carthach/Documents/Mendeley Desktop/music{\_}for{\_}an{\_}interactive{\_}network-bischoff-gold-horton.pdf:pdf},
number = {3},
pages = {24--29},
title = {{for of Network Microcomputers}},
volume = {2},
year = {2012}
}
@article{Brown2010,
abstract = {BACKGROUND: Traditional algorithms for hidden Markov model decoding seek to maximize either the probability of a state path or the number of positions of a sequence assigned to the correct state. These algorithms provide only a single answer and in practice do not produce good results.$\backslash$n$\backslash$nRESULTS: We explore an alternative approach, where we efficiently compute the k paths of highest probability to explain a sequence and then either use those paths to explore alternative explanations for a sequence or to combine them into a single explanation. Our procedure uses an online pruning technique to reduce usage of primary memory.$\backslash$n$\backslash$nCONCLUSION: Out algorithm uses much less memory than naive approach. For membrane proteins, even simple path combination algorithms give good explanations, and if we look at the paths we are combining, we can give a sense of confidence in the explanation as well. For proteins with two topologies, the k best paths can give insight into both correct explanations of a sequence, a feature lacking from traditional algorithms in this domain.},
author = {Brown, Daniel G and Golod, Daniil},
doi = {10.1186/1471-2105-11-S1-S28},
file = {:Users/carthach/Documents/Mendeley Desktop/Brown, Golod - 2010 - Decoding HMMs using the k best paths algorithms and applications.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Databases, Protein,Markov Chains,Proteins,Proteins: chemistry},
pages = {S28},
pmid = {20122200},
title = {{Decoding HMMs using the k best paths: algorithms and applications.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3009499{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11 Suppl 1},
year = {2010}
}
@article{Pachet2017,
abstract = {Recently, machine-learning techniques have been success-fully used for the generation of complex artifacts such as music or text. However, these techniques are still unable to capture and generate artifacts that are convincingly struc-tured. In particular, musical sequences do not exhibit pat-tern structure, as typically found in human composed mu-sic. We present an approach to generate structured se-quences, based on a mechanism for sampling efficiently variations of musical sequences. Given an input sequence and a statistical model, this mechanism uses belief propa-gation to sample a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism uses local fields to bias the generation. We show experimentally that sampled sequences are in-deed closely correlated to the standard musical similarity function defined by Mongeau and Sankoff. We then show how this mechanism can be used to implement composi-tion strategies that enforce arbitrary structure on a musical lead sheet generation problem. We illustrate our approach with a convincingly structured generated lead sheet in the style of the Beatles.},
author = {Pachet, Fran{\c{c}}ois and Papadopoulos, Alexandre and Roy, Pierre},
file = {:Users/carthach/Documents/Mendeley Desktop/50{\_}Paper.pdf:pdf},
journal = {Proceedings of the 18th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Sampling Variations of Sequences for Structured Music Generation}},
url = {https://ismir2017.smcnus.org/wp-content/uploads/2017/10/50{\_}Paper.pdf},
year = {2017}
}
@techreport{ford1956network,
author = {{Ford Jr}, Lester R},
institution = {RAND CORP SANTA MONICA CA},
title = {{Network flow theory}},
year = {1956}
}
@book{Collins2010,
abstract = {An up-to-date, core undergraduate text, Introduction to Computer Music deals with both the practical use of technology in music and the key principles underpinning the discipline. It targets both musicians exploring computers, and technologists engaging with music, and does so in the confidence that both groups can learn tremendously from the cross-disciplinary encounter. It is designed to approach computer music as its own subject and strongly bridge the arts to computing divide, benefiting and reconciling both musicians and computer scientists. You will need little or no prior experience of computer programming itself, and may not have an extensive background in mathematics or music, but this highly engaging textbook will help you master many disciplines at once, with a focus on both fascinating theories and exciting practical applications.},
author = {Collins, Nick},
doi = {Book Review},
file = {:Users/carthach/Documents/Mendeley Desktop/Introduction to Computer Music.pdf:pdf},
isbn = {0470714557},
issn = {00274321},
publisher = {John Wiley {\&} Sons},
title = {{Introduction to computer music}},
url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-EHEP000962.html},
year = {2010}
}
@article{Ellis2007a,
abstract = {Beat tracking i.e. deriving from a music audio signal a sequence of beat instants that might correspond to when a human listener would tap his foot involves satisfying two con- straints: On the one hand, the selected instants should generally correspond to moments in the audio where a beat is indicated, for instance by the onset of a note played by one of the instru- ments. On the other hand, the set of beats should reflect a locally-constant inter-beat-interval, since it is this regular spacing between beat times that defines musical rhythm. These dual constraints map neatly onto the two constraints optimized in dynamic programming, the local match, and the transition cost. We describe a beat tracking system which first estimates a global tempo, uses this tempo to construct a transition cost function, then uses dynamic programming to find the best-scoring set of beat times that reflect the tempo as well as corresponding to moments of high onset strength in a function derived from the audio. This very simple and computationally efficient procedure is shown to perform well on the MIREX-06 beat track- ing training data, achieving an average beat accuracy of just under 60{\%} on the development data. We also examine the impact of the assumption of a fixed target tempo, and show that the system is typically able to track tempo changes in a range of 10{\%} of the target tempo.},
author = {Ellis, Daniel P. W.},
doi = {10.1080/09298210701653344},
file = {:Users/carthach/Documents/Mendeley Desktop/Ellis07-beattrack.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1},
pages = {51--60},
title = {{Beat Tracking by Dynamic Programming}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09298210701653344},
volume = {36},
year = {2007}
}
@article{Perry2017,
abstract = {A command-line tool and Python framework is proposed for the exploration of a new form of audio synthesis known as ‘concatenative synthesis', a form of synthesis that uses perceptual audio analyses to arrange small segments of audio based on their characteristics. The tool is designed to synthesise representations of an input target sound using a source database of sounds. This involves the segmentation and ana- lysis of both the input sound and database, the matching of input segments to their closest segment from the database, and the re-synthesis of the closest matches to pro- duce the final result. The project aims to provide a tool capable of generating high-quality sonic represent- ations of an input, to present a variety of examples that demonstrated the breadth of possibilities that this style of synthesis has to offer and to provide a robust framework on which concatenative synthesis projects can be developed easily. The purpose of this project was primarily to highlight the potential for further development in the area of concatenative synthesis, and to provide a simple and intuitive tool that could be used by composers for sound design and experimentation. The breadth of possibilities for creating new sounds offered by this method of synthesis makes it ideal for digital sound design and electroacoustic composition. Results demonstrate the wide variety of sounds that can be produced using this method of synthesis. A number of technical issues are outlined that impeded the overall quality of results and efficiency of the software. However, the project clearly demonstrates the strong},
author = {Perry, Sam},
doi = {10.5920/fields.2017.12},
file = {:Users/carthach/Documents/Mendeley Desktop/Perry.pdf:pdf},
journal = {Fields: journal of Huddersfield student research},
keywords = {audio analysis,audio descriptor,command line tool,concatenative synthesis,python,python framework,python sound},
number = {1},
pages = {1--10},
title = {{Descriptor driven concatenative synthesis tool for Python}},
url = {http://eprints.hud.ac.uk/31352/},
volume = {3},
year = {2017}
}
@article{Savage2015,
abstract = {Music has been called “the universal language of mankind.” Al- though contemporary theories of music evolution often invoke various musical universals, the existence of such universals has been disputed for decades and has never been empirically demon- strated. Here we combine a music-classification scheme with sta- tistical analyses, including phylogenetic comparative methods, to examine a well-sampled global set of 304 music recordings. Our analyses reveal no absolute universals but strong support for many statistical universals that are consistent across all nine geo- graphic regions sampled. These universals include 18 musical fea- tures that are common individually as well as a network of 10 features that are commonly associated with one another. They span not only features related to pitch and rhythm that are often cited as putative universals but also rarely cited domains including per- formance style and social context. These cross-cultural structural reg- ularities of human music may relate to roles in facilitating group coordination and cohesion, as exemplified by the universal tendency to sing, play percussion instruments, and dance to simple, repetitive music in groups. Our findings highlight the need for scientists study- ing music evolution to expand the range of musical cultures and musical features under consideration. The statistical universals we identified represent important candidates for future investigation.},
author = {Savage, Patrick E. and Brown, Steven and Sakai, Emi and Currie, Thomas E.},
doi = {10.1073/pnas.1414495112},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {29},
pages = {8987--8992},
pmid = {26124105},
title = {{Statistical universals reveal the structures and functions of human music}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1414495112},
volume = {112},
year = {2015}
}
@article{Liu2010a,
abstract = {Instrumental music is often classified or retrieved in terms of instruments played in it. With a large database consists of Chinese traditional music and western classical music, this paper extracted several features to automatically classify Chinese and western instruments by SVM classifier, and analyzed the classification results.},
author = {Liu, Jing and Xie, Lingyun},
doi = {10.1109/ICICTA.2010.64},
file = {:Users/carthach/Documents/Mendeley Desktop/05523032.pdf:pdf},
isbn = {9780769540771},
journal = {2010 International Conference on Intelligent Computation Technology and Automation, ICICTA 2010},
keywords = {Classification,Instrumental music,SVM},
pages = {669--673},
title = {{SVM-based automatic classification of musical instruments}},
volume = {3},
year = {2010}
}
@article{Bown2017,
author = {Bown, Oliver},
doi = {10.1145/3029374},
file = {:Users/carthach/Documents/Mendeley Desktop/Bown - 2017 - Special Issue on Musical Metacreation, Part II.pdf:pdf},
issn = {15443574},
journal = {Computers in Entertainment},
number = {3},
pages = {1--3},
title = {{Special Issue on Musical Metacreation, Part II}},
url = {http://dl.acm.org/citation.cfm?doid=3023312.3029374},
volume = {14},
year = {2017}
}
@book{Burns1994,
author = {Burns and Helen, Kristine},
publisher = {Ball State University},
title = {{The history and development of algorithms in music composition, 1957–1993}},
url = {https://dl.acm.org/citation.cfm?id=192467},
year = {1994}
}
@misc{Hindmarch2001,
author = {Hindmarch, Carl},
publisher = {BBC},
title = {{Pump Up the Volume}},
year = {2001}
}
@article{Bookstein,
author = {Bookstein, Abraham and Klein, Shmuel Tomi and Raita, Timo},
file = {:Users/carthach/Documents/Mendeley Desktop/Bookstein, Klein, Raita - Unknown - Fuzzy Hamming Distance ( Extended Abstract ).pdf:pdf},
pages = {86--97},
title = {{Fuzzy Hamming Distance : ( Extended Abstract )}}
}
@article{Brossier2004a,
abstract = {Segmenting note objects in a real time context is useful for live performances, audio broadcasting, or object-based cod- ing. This temporal segmentation relies upon the correct de- tection of onsets and offsets of musical notes, an area of much research over recent years. However the low-latency require- ments of real-time systems impose new, tight constraints on this process. In this paper, we present a system for the seg- mentation of note objects with very short delays, using recent developments in onset detection, specially modified to work in a real-time context. A portable and open C implementation is presented.},
author = {Brossier, Paul and Bello, Juan Pablo and Plumbley, Mark D},
file = {:Users/carthach/Documents/Mendeley Desktop/brossier04realtimesegmentation.pdf:pdf},
journal = {Proceedings of the 2004 International Computer Music Conference},
title = {{Real-time temporal segmentation of note objects in music signals}},
url = {http://www-student.elec.qmul.ac.uk/people/juan/Documents/Brossier-ICMC-2004.pdf},
year = {2004}
}
@article{Aucouturier2005,
author = {Aucouturier, Jean-Julien and Pachet, Fran{\c{c}}ois},
file = {:Users/carthach/Documents/Mendeley Desktop/Aucouturier, Pachet - 2005 - Ringomatic A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors.pdf:pdf},
isbn = {0-9551179-0-9},
journal = {Proceedings of the International Conference on Music Information Retrieval},
keywords = {concatenative synthesis,constraint,drumtrack,interaction,metadata,satisfaction},
pages = {412--419},
title = {{Ringomatic: A Real-Time Interactive Drummer Using Constraint-Satisfaction and Drum Sound Descriptors}},
year = {2005}
}
@article{Janssen2015,
author = {Janssen, Berit and van Kranenburg, Peter and Volk, Anja},
file = {:Users/carthach/Documents/Mendeley Desktop/Janssen, van Kranenburg, Volk - 2015 - A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments.pdf:pdf},
journal = {Proceedings of the 16th International Society for Music Information Retrieval Conference},
pages = {659--665},
title = {{A Comparison of Symbolic Similarity Measures for Finding Occurrences of Melodic Segments}},
url = {https://drive.google.com/open?id=1xhyKUR8EaXbVSHRGMfnKZ39wxUWGyUKVE352-6fmBXE{\%}5Cnhttps://github.com/BeritJanssen/MelodicOccurrences},
year = {2015}
}
@inproceedings{Biles1994,
abstract = {This paper describes GenJam, a genetic algorithm-based model of a novice jazz musician learning to improvise. GenJam maintains hierarchically related populations of melodic ideas that are mapped to specific notes through scales suggested by the chord progression being played. As GenJam plays its solos over the accompaniment of a standard rhythm section, a human mentor gives real-time feedback, which is used to derive fitness values for the individual measures and phrases. GenJam then applies various genetic operators to the populations to breed improved generations of ideas},
author = {Biles, John A},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Biles - 1994 - GenJam A Genetic Algorithm for Generating Jazz Solos.pdf:pdf},
issn = {1026-1087},
keywords = {BSc Thesis,Folder - IAT 811: Metacreation,artificial intelligence,automated composition,emotions,generative music,genetic algorithm,genetic algorithms,jazz,music,research},
mendeley-tags = {BSc Thesis,Folder - IAT 811: Metacreation,artificial intelligence,automated composition,emotions,generative music,genetic algorithm,genetic algorithms,jazz,music,research},
pages = {131 -- 137},
title = {{GenJam : A Genetic Algorithm for Generating Jazz Solos}},
year = {1994}
}
@article{Schwarz2008b,
abstract = {Corpus-based concatenative synthesis plays grains from a large corpus of segmented and descriptor-analysed sounds according to proximity to a target position in the descriptor space. This can be seen as a content-based extension to granular synthesis providing direct access to specific sound characteristics. The interactive concatenative sound synthesis system CATART that realises real-time corpusbased concatenative synthesis is implemented as a collection of Max/MSP patches using the FTM library. CATART allows to explore the corpus interactively or via a written target score, to resynthesise an audio file or live input with the source sounds. We will show musical applications of pieces that explore the new concepts made possible by corpus-based concatenative synthesis.},
author = {Schwarz, Diemo and Cahen, Roland and Britton, Sam},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz, Cahen, Britton - 2008 - Principles and Applications of Interactive Corpus-Based Concatenative Synthesis.pdf:pdf},
journal = {Journ{\'{e}}es d'Informatique Musicale},
title = {{Principles and Applications of Interactive Corpus-Based Concatenative Synthesis}},
year = {2008}
}
@article{Meroo-Peuela2017,
author = {Mero{\~{n}}o-Pe{\~{n}}uela, Albert and Meerwaldt, Rick and Schlobach, Stefan},
file = {:Users/carthach/Documents/Mendeley Desktop/paper510.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {MIDI,MIDI linked data,Mashups,SPARQL},
pages = {1--4},
title = {{SPARQL-DJ: The MIDI linked data mashup mixer for your next semantic party}},
volume = {1963},
year = {2017}
}
@article{Harvey2000,
author = {Harvey, Jonathan},
journal = {Contemporary music review},
number = {3},
pages = {11--14},
publisher = {Taylor {\&} Francis},
title = {{Spectralism}},
volume = {19},
year = {2000}
}
@article{Bates2009,
abstract = {The results of a large number of listening tests and simulations were analysed to determine the fundamental capabilities of different spatialization techniques under the less than ideal conditions typically encountered during a performance. This analysis focussed on multichannel stereophony, Ambisonics, and Wavefield Synthesis. Other methods which are orientated toward a single listener are not addressed in this thesis. The results indicated that each spatialization scheme has particular strengths and weaknesses, and that the optimum technique in any situation is dependent on the particular spatial effect required. It was found that stereophonic techniques based on amplitude panning provided the most accurate localization but suffered from a lack of spaciousness and envelopment. Ambisonics provided an improved sense of envelopment but poor localization accuracy, particularly with first order Ambisonics systems. Consequently it would appear that stereophony is preferable when the directionality and focus of the virtual source is paramount, while Ambisonics is preferable if a more diffuse enveloping sound field is required. Ambisonics},
author = {Bates, Enda},
file = {:Users/carthach/Documents/Mendeley Desktop/Enda Bates - The Composition and Performance of Spatial Music.pdf:pdf},
journal = {PhD Thesis},
number = {August},
title = {{The Composition and Performance of Spatial Music}},
year = {2009}
}
@article{Peeters2004b,
abstract = {In this report, we review the set of audio descriptors which has been developed and used in the framework of the CUIDADO I.S.T. project at Ircam.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0611061v2},
author = {Peeters, Geoffroy},
doi = {10.1234/12345678},
eprint = {0611061v2},
file = {:Users/carthach/Documents/Mendeley Desktop/Peeters - 2004 - A large set of audio features for sound description (similarity and classification) in the CUIDADO project.pdf:pdf},
isbn = {9780335226375},
issn = {nul},
journal = {CUIDADO IST Project Report},
number = {0},
pages = {1--25},
pmid = {20314319},
primaryClass = {arXiv:quant-ph},
title = {{A large set of audio features for sound description (similarity and classification) in the CUIDADO project}},
url = {http://www.citeulike.org/group/1854/article/1562527},
volume = {54},
year = {2004}
}
@incollection{Greenberg1971,
author = {Greenberg, Clement},
booktitle = {Art and Culture: Critical Essays},
publisher = {Beacon Press},
title = {{Collage}},
year = {1971}
}
@book{Hackeling2014,
abstract = {If you are a software developer who wants to learn how machine learning models work and how to apply them effectively, this book is for you. Familiarity with machine learning fundamentals and Python will be helpful, but is not essential.},
author = {Hackeling, Gavin},
file = {:Users/carthach/Documents/Mendeley Desktop/Mastering Machine Learning with scikit-learn.pdf:pdf},
isbn = {1783988371},
pages = {238},
publisher = {Packt Publishing Ltd},
title = {{Mastering Machine Learning with scikit-learn}},
url = {http://books.google.com/books?id=fZQeBQAAQBAJ{\&}pgis=1},
year = {2014}
}
@article{Shan2009,
author = {Shan, MK and Chiu, SC},
doi = {10.1007/s11042-009-0303-y},
file = {:Users/carthach/Documents/Mendeley Desktop/Shan, Chiu - 2010 - Algorithmic compositions based on discovered musical patterns.pdf:pdf},
journal = {Multimedia Tools and Applications},
keywords = {algorithmic composition,data mining,music style,repeating patterns},
title = {{Algorithmic compositions based on discovered musical patterns}},
url = {http://link.springer.com/article/10.1007/s11042-009-0303-y},
year = {2010}
}
@article{Serra1997,
author = {Serra, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Serra - 1997 - Musical sound modeling with sinusoids plus noise.pdf:pdf},
journal = {Musical signal processing},
pages = {1--25},
title = {{Musical sound modeling with sinusoids plus noise}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=RJ9lAgAAQBAJ{\&}oi=fnd{\&}pg=PT14{\&}dq=Musical+Sound+Modeling+with+Sinusoids+plus+Noise{\&}ots=ZeWz1mGX3K{\&}sig=EBJaVNkS9HI1jz9zebRhYhe4DVc},
year = {1997}
}
@article{Kim-Boyle2006,
abstract = {The author describes applications of Craig Reynolds's boids algorithm for sound spatialization. A MaxMSP/Jitter patch is presented where the movement of individual boids in two dimensional space is rendered in OpenGL and is used to map the spatial trajectories of granular voices in a granular sampling patch and also the spatial location of a sound's spectral components. Musical possibilities of the technique are discussed and some performance considerations are also outlined.},
author = {Kim-Boyle, D},
file = {:Users/carthach/Documents/Mendeley Desktop/spectral-and-granular-spatialization-with-boids.pdf:pdf},
journal = {Proceedings of the 2006 International Computer Music Conference},
number = {Reynolds 1987},
pages = {139--142},
title = {{Spectral and Granular Spatialization with Boids}},
url = {http://en.scientificcommons.org/48705774},
year = {2006}
}
@article{Miron2013,
abstract = {This paper presents a drum transcription algorithm adjusted to the constraints of real-time audio. We introduce an instance filtering (IF) method using sub-band onset detection, which improves the performance of a system having at its core a feature-based K-nearest neighbor classifier (KNN). The ar- chitecture proposed allows for adapting different parts of the algorithm for either bass drum, snare drum or hi-hat cymbals. The open-source system is implemented in the graphic programming languages Pure Data (PD) and Max MSP, and aims to work with a large variety of drum sets. We evaluated its performance on a database of audio samples generated from a well known collection of midi drum loops randomly matched with a diverse collection of drum sets. Both of the evaluation stages, testing and validation, showa significant improvement in the performance when using the instance filtering algorithm.},
author = {Miron, Marius and Davies, Matthew E P and Gouyon, Fabien},
doi = {10.1109/ICASSP.2013.6637641},
file = {:Users/carthach/Documents/Mendeley Desktop/Miron, Davies, Gouyon - 2013 - An open-source drum transcription system for Pure Data and Max MSP.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {drum transcription,feature-based classification,machine learning,real-time audio,signal processing},
pages = {221--225},
title = {{An open-source drum transcription system for Pure Data and Max MSP}},
year = {2013}
}
@article{Antunes2014,
author = {Antunes, RF and Leymarie, FF and Latham, W},
file = {:Users/carthach/Documents/Mendeley Desktop/Antunes, Leymarie, Latham - 2014 - Two Decades of Evolutionary Art Using Computational Ecosystems and Its Potential for Virtual Worlds.pdf:pdf},
journal = {Journal of Virtual Worlds Research},
title = {{Two Decades of Evolutionary Art Using Computational Ecosystems and Its Potential for Virtual Worlds}},
url = {https://journals.tdl.org/jvwr/index.php/jvwr/article/view/7051},
year = {2014}
}
@article{Thul2008,
author = {Thul, Eric},
file = {:Users/carthach/Documents/Mendeley Desktop/Thul - 2008 - Measuring the Complexity of Musical Rhythm.pdf:pdf},
number = {June},
title = {{Measuring the Complexity of Musical Rhythm}},
url = {http://www-cgrl.cs.mcgill.ca/{~}godfried/teaching/mir-reading-assignments/Eric-Thul-Thesis.pdf},
year = {2008}
}
@article{Lantz2013,
abstract = {Abstract: Likert-type data are often assumed to be equidistant by applied researchers so that they can use parametric methods to analyse the data. Since the equidistance assumption rarely is tested, the validity of parametric analyses of Likert-type data is often unclear. This paper consists of two parts where we deal with this validity problem in two different respects. In the first part, we use an experimental design to show that the perceived distance between scale points on a regular five-point Likert-type scale depends on how the verbal anchors are used. Anchors only at the end points create a relatively larger perceived distance between points near the ends of the scale than in the middle (end-of-scale effect), while anchors at all points create a larger perceived distance between points in the middle of the scale (middle-of-scale effect). Hence, Likert-type scales are generally not perceived as equidistant by subjects. In the second part of the paper, we use Monte Carlo simulations to explore how parametric methods commonly used to compare means between several groups perform in terms of actual significance and power when data are assumed to be equidistant even though they are not. The results show that the preferred statistical method to analyse Likert-type data depends on the nature of their non- equidistance as well as their skewness. Under middle-of-scale effect, the omnibus one-way ANOVA works best when data are relatively symmetric. However, the Kruskal-Wallis test works better when data are skewed except when sample sizes are unequal, in which case the Brown-Forsythe test is better. Under end-of-scale effect, on the other hand, the Kruskal- Wallis test should be preferred in most cases when data are at most moderately skewed. When data are heavily skewed, ANOVA works best unless when sample sizes are unequal, in which case the Brown-Forsythe test should be preferred.},
archivePrefix = {arXiv},
arxivId = {Virginia University, Morgan town, West Virginia},
author = {Lantz, Bjorn},
doi = {10.1111/j.1365-2929.2004.02012.x},
eprint = {Virginia University, Morgan town, West Virginia},
file = {:Users/carthach/Documents/Mendeley Desktop/1LikertScales.pdf:pdf},
isbn = {0308-0110 (Print)},
issn = {14777029},
journal = {Electronic Journal of Business Research Methods},
keywords = {ANOVA,Brown-forsythe test,Equidistance,Kruskal-wallis test,Likert-type scale,Monte carlo simulation,Welch test},
number = {1},
pages = {16--28},
pmid = {15566531},
title = {{Equidistance of Likert-Type Scales and Validation of Inferential Methods Using Experiments and Simulations}},
volume = {11},
year = {2013}
}
@article{Holzapfel2009a,
author = {Holzapfel, A and Stylianou, Y},
file = {:Users/carthach/Documents/Mendeley Desktop/Holzapfel, Stylianou - 2009 - A scale transform based method for rhythmic similarity of music.pdf:pdf},
isbn = {9781424423545},
journal = {Acoustics, Speech and Signal {\ldots}},
pages = {317--320},
title = {{A scale transform based method for rhythmic similarity of music}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4959584},
year = {2009}
}
@article{Gabrielsson1973a,
author = {Gabrielsson, Alf},
journal = {Scandinavian Journal of Psychology},
number = {1},
pages = {138--160},
publisher = {Wiley Online Library},
title = {{Similarity ratings and dimension analyses of auditory rhythm patterns. 1}},
volume = {14},
year = {1973}
}
@article{Boden2009,
author = {Boden, Margaret A},
journal = {AI Magazine},
number = {3},
pages = {23},
title = {{Computer models of creativity}},
volume = {30},
year = {2009}
}
@phdthesis{Jade2016,
author = {Jade, Chaithanya},
file = {:Users/carthach/Documents/Mendeley Desktop/Chaithanya-Jade-Master-thesis-2016.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Rhythm alternation using interval sets}},
year = {2016}
}
@article{Schwarz2008a,
abstract = {We propose an extension to real-time corpus-based concatenative synthesis that predicts the best sound unit to follow an arbitrary sequence of units depending on context. This novel method is well suited to interactive applications because it does not need a preliminary analysis or training phase. We experiment different modes of interaction and present results with a quantitative evaluation of the influence of the new method on corpora of drum loops, voice, and environmental sounds.},
author = {Schwarz, Diemo and Cadars, Sylvain and Schnell, Norbert},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz, Cadars, Schnell - 2008 - What Next Continuation in Real-Time Corpus-Based Concatenative Synthesis.pdf:pdf},
journal = {International Computer Music Conference 2008},
title = {{What Next? Continuation in Real-Time Corpus-Based Concatenative Synthesis}},
year = {2008}
}
@article{Pollard1982,
abstract = {A tristimulus method is described that shows graphically the time-dependent$\backslash$nbehaviour of musical transients. Following analysis of the sound$\backslash$nby a sampled filter method, the data are converted into 1/3 octave$\backslash$nfilter band loudness values at 5 ms intervals. The loudness values$\backslash$nare then converted into three coordinates, based on the loudness$\backslash$nof (i) the fundamental, (ii) a group containing partials 2-4, and$\backslash$n(iii) a group containing partials 5-n, where n is the highest significant$\backslash$npartial. This procedure allows a graph to be drawn that shows in$\backslash$na simple manner the time-dependent behaviour of the starting transient$\backslash$nin relation to the 'steady state'. Examples show the tonal evolution$\backslash$nof a number of musical sounds},
author = {Pollard, H F and Jansson, E V},
issn = {00017884},
journal = {Acustica},
pages = {162--171},
title = {{A tristimulus method for the specification of musical timbre}},
volume = {51},
year = {1982}
}
@inproceedings{Wessel1976,
author = {Wessel, David L},
booktitle = {International Computer Music Conference, Massachusetts Institute of Technology},
title = {{Perceptually based controls for additive synthesis}},
year = {1976}
}
@article{Bruderer2006,
author = {Bruderer, MJ and McKinney, MF and Kohlrausch, Armin},
file = {:Users/carthach/Documents/Mendeley Desktop/Bruderer, McKinney, Kohlrausch - 2006 - Structural boundary perception in popular music.pdf:pdf},
journal = {ISMIR},
keywords = {music cognition,music percep-,music segmentation,music structure,tion},
title = {{Structural boundary perception in popular music.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.1661{\&}rep=rep1{\&}type=pdf},
volume = {4},
year = {2006}
}
@book{Benward2008,
author = {Benward, Bruce and Saker, Marilyn},
edition = {Eighth},
file = {:Users/carthach/Documents/Mendeley Desktop/Music Theory and Practice Textbook.pdf:pdf},
isbn = {978-0-07-310187-3},
pages = {147--148},
publisher = {McGraw-Hill},
title = {{Music: In Theory and Practise}},
url = {http://www.gcisd-k12.org/cms/lib4/TX01000829/Centricity/Domain/2825/Music Theory and Practice Textbook.pdf},
year = {2008}
}
@inproceedings{Stoll2013,
author = {Stoll, Thomas},
booktitle = {InternationalWorkshop on Musical Metacreation (MuMe)},
file = {:Users/carthach/Documents/Mendeley Desktop/Stoll - 2013 - CorpusDB Software for Analysis, Storage, and Manipulation of Sound Corpora.pdf:pdf},
keywords = {AAAI Technical Report WS-13-22},
number = {Figure 1},
pages = {108--113},
title = {{CorpusDB: Software for Analysis, Storage, and Manipulation of Sound Corpora}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/viewFile/7457/7684},
year = {2013}
}
@article{Arewa1979,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Arewa, Olufunmilayo B.},
doi = {10.3366/ajicl.2011.0005},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/OlufunmilayoBArewaFromJCB.pdf:pdf},
isbn = {0674539265},
issn = {02729490},
journal = {Hofstra Law Review},
number = {2},
pages = {243--258},
pmid = {21675331},
title = {{From JC Bach to hip hop: Musical borrowing, copyright and cultural context}},
volume = {7},
year = {1979}
}
@book{arom2004african,
author = {Arom, Simha},
publisher = {Cambridge university press},
title = {{African polyphony and polyrhythm: musical structure and methodology}},
year = {2004}
}
@article{Stevens1955,
abstract = {This paper reviews the available evidence (published and unpublished) on the relation between loudness and stimulus intensity. The e. vidence suggests that for the typical listener the loudness L of a 1000-cycle tone can be approximated by a power function of intensity I, of which the exponent is log • 02. The equation is: L--kI {\o}.a. Intensity here is assumed to be proportional to the square of the sound pressure. In terms of sones, where 1 sone is the loudness produced by a tone at 40 db above the standard reference level, the equation for loudness L as a function of the number of decibels N becomes' logL=0.03N--1.2. Otherwise said, a loudness ratio of 2:1 is produced by a pair of stimuli that differ by 10 db, and this re-lation appears to hold over the entire range of audible intensities. At low levels of intensity, the loudness of white noise grows more rapidly than the loudness of a 1000-cycle tone, but above the level of approximately 50 db the two loudnesses remain more nearly proportional. The suggestion is made that for all levels greater than 50 db the loudness of continuous noises may be calculated from the equation: logL=O.O3N-t--S, where {\$} is a spectrum parameter to be determined empirically. HE purpose of this review is to examine the available data on the measurement of subjective loudness. It is hoped that by assembling the relevant information in one place we may be able to reach a reasonable conclusion concerning the relation between loudness and intensity. The various results obtained by workers in this field make it plain that the scale relating loudness to intensity is not something that can be determined with high precision, but these efforts also make it plain that people are able to make quantitative estimates of loudness and that it is not unreasonable to},
author = {Stevens, S S},
doi = {10.1121/1.1912650},
file = {:Users/carthach/Documents/Mendeley Desktop/StevensJASA1955.pdf:pdf},
issn = {00014966},
journal = {Journal of the Acoustical Society of America},
number = {5},
pages = {815},
pmid = {20733330},
title = {{The Measurement of Loudness}},
volume = {27},
year = {1955}
}
@phdthesis{Srinivasamurthy2017,
author = {Srinivasamurthy, Ajay},
file = {:Users/carthach/Documents/Mendeley Desktop/Srinivasamurthy - 2017 - A Data-driven Bayesian Approach to Automatic RhythmAnalysis of Indian Art Music.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{A Data-driven Bayesian Approach to Automatic RhythmAnalysis of Indian Art Music}},
year = {2017}
}
@article{Ariza2009,
abstract = {Procedural or algorithmic approaches to generating music have been explored in the medium of software for over fifty years. Occasionally, researchers have attempted to evaluate the success of these generative music systems by measuring the perceived quality or style conformity of isolated musical outputs. These tests are often conducted in the form of comparisons between computer-aided output and non-computer-aided output. The model of the Turing Test (TT), Alan Turing's proposed “Imitation Game” (Turing 1950), has been submitted and employed as a framework for these comparisons. In this context, it is assumed that if machine output sounds like, or is preferred to, human output, the machine has succeeded. The nature of this success is rarely questioned, and is often interpreted as evidence of a successful generative music system. Such listener surveys, within necessary statistical and psychological constraints, may be pooled to gauge common responses to and interpretations of music—yet these surveys are not TTs. This article argues that Turing's well-known proposal cannot be applied to executing and evaluating listener surveys. Whereas pre-computer generative music systems have been employed for centuries, the idea of testing the output of such systems appears to only have emerged since computer implementation. One of the earliest tests is reported in Hiller (1970, p. 92): describing the research of Havass (1964), Hiller reports that, at a conference in 1964, Havass conducted an experiment to determine if listeners could distinguish computer-generated and traditional melodies. Generative techniques derived from the fields of artificial intelligence (AI; for example, neural nets and various learning algorithms) and artificial life (e.g., genetic algorithms and cellular automata) may be associated with such tests due to explicit reference to biological systems. Yet, since only the output of the system is tested (that is, system and interface design are ignored), any generative technique can be employed. These tests may be associated with the broader historical context of human-versus-machine tests, as demonstrated in the American folk-tale of John Henry versus the steam hammer (Nelson 2006) or the more recent competition of Garry Kasparov versus Deep Blue (Hsu 2002). Some tests attempt to avoid measures of subjective quality by measuring perceived conformity to known musical artifacts. These musical artifacts are often used to create the music being tested: they are the source of important generative parameters, data, or models. The design goals of a system provide context for these types of tests. Pearce, Meredith, and Wiggins (2002, p. 120) define four motivations for the development of generative music systems: (1) composer-designed tools for personal use, (2) tools designed for general compositional use, (3) “theories of a musical style . . . implemented as computer programs,” and (4) “cognitive theories of the processes supporting compositional expertise . . . implemented as computer programs.” Such motivational distinctions may be irrelevant if the system is used outside of the context of its creation; for this reason, system-use cases, rather than developer motivations, might offer alternative distinctions. The categories proposed by Pearce, Meredith, and Wiggins can be used to generalize about two larger use cases: systems used as creative tools for making original music (motivations 1 and 2, above), and systems that are designed to computationally model theories of musical style or cognition (motivations 3 and 4). These two larger categories will be referred to as “creative tools” and “computational models.” Although design motivation is not included in the seven descriptors of computer-aided algorithmic systems proposed in Ariza (2005), the “idiom affinity” descriptor is closely related: systems with singular idiom affinities are often computational models. Explicitly testing the output of generative music systems is uncommon. As George Papadopoulos and Geraint Wiggins (1999, p. 113) observe, research in generative music systems demonstrates a “lack of experimental methodology.” Furthermore, “there is usually no evaluation of the output by real experts.” Similarly, Pearce, Meredith, and Wiggins (2002, p. 120), presumably describing all types of generative music systems, state that “researchers often fail to adopt suitable methodologies for the development and evaluation of composition programs and this, in turn, has compromised the practical or theoretical value of their research.” In the case of creative tools, the lack of empirical output evaluation is not a shortcoming: creative...},
author = {Ariza, Christopher},
doi = {10.1162/comj.2009.33.2.48},
file = {:Users/carthach/Documents/Mendeley Desktop/Ariza - 2009 - The Interrogator as Critic The Turing Test and the Evaluation of Generative Music Systems.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {2},
pages = {48--70},
title = {{The Interrogator as Critic : The Turing Test and the Evaluation of Generative Music Systems}},
url = {http://muse.jhu.edu/journals/cmj/summary/v033/33.2.ariza.html},
volume = {33},
year = {2009}
}
@phdthesis{AlvaroSarasuaBerodia2017,
author = {{{\'{A}}lvaro Saras{\'{u}}a Berodia}},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{A}}lvaro Saras{\'{u}}a Berodia - 2017 - Musical Interaction Based on the Conductor Metaphor.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Musical Interaction Based on the Conductor Metaphor}},
year = {2017}
}
@inproceedings{schwarz2005,
author = {Schwarz, Diemo},
booktitle = {Proceedings of the International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz - 2005 - Current Research In Concatenative Sound Synthesis.pdf:pdf},
pages = {9--12},
title = {{Current Research In Concatenative Sound Synthesis}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:CURRENT+RESEARCH+IN+CONCATENATIVE+SOUND+SYNTHESIS{\#}0},
year = {2005}
}
@phdthesis{Tardieu2008,
author = {Tardieu, Damien},
file = {:Users/carthach/Documents/Mendeley Desktop/Tardieu-Phd-Thesis.pdf:pdf},
school = {Universit{\'{e}} Pierre Et Marie Curie},
title = {{Mod{\`{e}}les d'instruments pour l'aide {\`{a}} l' orchestration}},
year = {2008}
}
@article{Sheh2003,
abstract = {Automatic extraction of content description from commercial audio recordings has a number of impor- tant applications, from indexing and retrieval through to novel musicological analyses based on very large corpora of recorded performances. Chord sequences are a description that captures much of the charac- ter of a piece in acompact form and using a mod- est lexicon. Chords also have the attractive property that a piece of music can (mostly) be segmented into time intervals that consistof a single chord, much as recorded speech can (mostly)besegmented into time intervals that correspond to specific words. In this work, we build a system for automatic chord tran- scription using speech recognition tools. For features we use pitch class profile vectors to emphasize the tonal content of the signal, and we show that these features far outperform cepstral coefficients for our task. Sequence recognition is accomplishedwith hid- den Markov models (HMMs) directly analogous to subword models in a speech recognizer, and trained by the same Expectation-Maximization (EM) algo- rithm. Crucially, this allows us to use as input only the chord sequences for our training examples, with- out requiring the precise timings of the chord changes which are determined automatically during train- ing. Our results on a small set of 20 early Beatles songs show frame-level accuracy of around 75{\%} on a forced-alignment task.},
author = {Sheh, Alexander and Ellis, Daniel P W},
file = {:Users/carthach/Documents/Mendeley Desktop/Sheh, Ellis - 2003 - Chord segmentation and recognition using EM-trained hidden markov models.pdf:pdf},
isbn = {2974619401},
journal = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
keywords = {as ensemble performances,audio,beat detection,chords,em,etc,for complex signals,hmm,in,in difficulty,music,or more complex analyses,pitch transcription,such,such as,the task rapidly increases,unfortunately},
pages = {185--191},
title = {{Chord segmentation and recognition using EM-trained hidden markov models}},
url = {http://jscholarship.library.jhu.edu:8080/handle/1774.2/26},
year = {2003}
}
@book{Russ2004,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Russ, Martin},
doi = {10.1017/CBO9781107415324.004},
edition = {2nd},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Sound Synthesis And Sampling - 2nd Edition by Martin Russ.pdf:pdf},
isbn = {978-0-240-52105-3},
issn = {1098-6596},
pmid = {25246403},
publisher = {Taylor {\&} Francis},
title = {{Sound Synthesis and Sampling}},
url = {http://www-student.elec.qmul.ac.uk/people/juan/Documents/Brossier-ICMC-2004.pdf},
year = {2004}
}
@article{Kiefer2008a,
abstract = {There is small but useful body of research concerning the evaluation of musical interfaces with HCI techniques. In this paper, we present a case study in implementing these techniques; we describe a usability experiment which eval- uated the Nintendo Wiimote as a musical controller, and reflect on the effectiveness of our choice of HCI methodolo- gies in this context. The study offered some valuable results, but our picture of theWiimote was incomplete as we lacked data concerning the participants' instantaneous musical ex- perience. Recent trends in HCI are leading researchers to tackle this problem of evaluating user experience; we review some of their work and suggest that with some adaptation it could provide useful new tools and methodologies for com- puter musicians.},
author = {Kiefer, Chris and Collins, Nick and Fitzpatrick, Geraldine},
file = {:Users/carthach/Documents/Mendeley Desktop/Kiefer, Collins, Fitzpatrick - 2008 - HCI Methodology For Evaluating Musical Controllers A Case Study.pdf:pdf},
journal = {Proceedings of the 2008 International Conference on New Interfaces for Musical Expression (NIME-08)},
keywords = {evaluating musical interac-,hci methodology,nime08,wiimote},
pages = {87--90},
title = {{HCI Methodology For Evaluating Musical Controllers: A Case Study}},
url = {http://www.nime.org/proceedings/2008/nime2008{\_}087.pdf{\%}5Cnhttp://sro.sussex.ac.uk/37012/},
year = {2008}
}
@article{Liu2010,
author = {Liu, Xiao Fan and Tse, Chi K. and Small, Michael},
doi = {10.1016/j.physa.2009.08.035},
file = {:Users/carthach/Documents/Mendeley Desktop/Liu, Tse, Small - 2010 - Complex network structure of musical compositions Algorithmic generation of appealing music.pdf:pdf},
issn = {03784371},
journal = {Physica A: Statistical Mechanics and its Applications},
month = {jan},
number = {1},
pages = {126--132},
publisher = {Elsevier B.V.},
title = {{Complex network structure of musical compositions: Algorithmic generation of appealing music}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0378437109006827},
volume = {389},
year = {2010}
}
@article{Vitos2014,
author = {Vitos, Botond},
journal = {Dancecult: Journal of Electronic Dance Music Culture},
number = {1},
title = {{Along the Lines of the Roland TB-303: Three Perversions of Acid Techno}},
volume = {6},
year = {2014}
}
@article{Huddleston2007,
author = {Huddleston, John and Zhang, Jianna},
file = {:Users/carthach/Documents/Mendeley Desktop/Huddleston, Zhang - 2007 - Evolutionary rhythm composition with trajectory-based fitness evaluation.pdf:pdf},
journal = {PROCEEDINGS OF THE NATIONAL {\ldots}},
keywords = {Student Abstracts,Technical Papers},
pages = {1999--2000},
title = {{Evolutionary rhythm composition with trajectory-based fitness evaluation}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-309.pdf},
year = {2007}
}
@article{Hamel2010,
abstract = {Feature extraction is a crucial part of many MIR tasks. In this work, we present a system that can automatically ex- tract relevant features from audio for a given task. The fea- ture extraction system consists of a Deep Belief Network (DBN) on Discrete Fourier Transforms (DFTs) of the au- dio. We then use the activations of the trained network as inputs for a non-linear Support Vector Machine (SVM) classifier. In particular, we learned the features to solve the task of genre recognition. The learned features per- form significantly better than MFCCs. Moreover, we ob- tain a classification accuracy of 84.3{\%} on the Tzanetakis dataset, which compares favorably against state-of-the-art genre classifiers using frame-based features. We also ap- plied these same features to the task of auto-tagging. The autotaggers trained with our features performed better than those that were trained with timbral and temporal features.},
author = {Hamel, Philippe and Eck, Douglas},
file = {:Users/carthach/Documents/Mendeley Desktop/Hamel, Eck - 2010 - Learning Features from Music Audio with Deep Belief Networks.pdf:pdf},
isbn = {9789039353813},
journal = {International Society for Music Information Retrieval Conference (ISMIR)},
number = {Ismir},
pages = {339--344},
title = {{Learning Features from Music Audio with Deep Belief Networks}},
year = {2010}
}
@misc{CollinsConcat,
author = {Collins, Nick},
title = {{Concat}},
url = {http://www.sussex.ac.uk/Users/nc81/iphone.html}
}
@article{Cano2005,
abstract = {Sound engineers need to access vast collections of sound effects for their film and video productions. Sound effects providers rely on text-retrieval techniques to give access to their collections. Currently, audio content is annotated manually, which is an arduous task. Automatic annotation methods, normally fine-tuned to reduced domains such as musical instruments or limited sound effects taxonomies, are not mature enough for labeling with great detail any possible sound. A general sound recognition tool would require first, a taxonomy that represents the world and, second, thousands of classifiers, each specialized in distinguishing little details. We report experimental results on a general sound annotator. To tackle the taxonomy definition problem we use WordNet, a semantic network that organizes real world knowledge. In order to overcome the need of a huge number of classifiers to distinguish many different sound classes, we use a nearest-neighbor classifier with a database of isolated sounds unambiguously linked to WordNet concepts. A 30{\%} concept prediction is achieved on a database of over 50,000 sounds and over 1600 concepts.},
author = {Cano, Pedro and Koppenberger, Markus and {Le Groux}, Sylvain and Ricard, Julien and Wack, Nicolas and Herrera, Perfecto},
doi = {10.1007/s10844-005-0318-4},
file = {:Users/carthach/Documents/Mendeley Desktop/10.1007{\%}2Fs10844-005-0318-4.pdf:pdf},
isbn = {0925-9902},
issn = {09259902},
journal = {Journal of Intelligent Information Systems},
keywords = {Audio identification,Everyday sound,Knowledge management,Nearest-neighbor,WordNet},
number = {2-3},
pages = {99--111},
title = {{Nearest-neighbor automatic sound annotation with a WordNet taxonomy}},
volume = {24},
year = {2005}
}
@article{Barreiro2010,
author = {Barreiro, Daniel L},
journal = {Organised Sound},
number = {3},
pages = {290--296},
publisher = {Cambridge University Press},
title = {{Considerations on the handling of space in multichannel electroacoustic works}},
volume = {15},
year = {2010}
}
@article{Sturm2006,
abstract = {doi: 10.1162/comj.2006.30.4.46},
author = {Sturm, Bob L.},
doi = {10.1162/comj.2006.30.4.46},
file = {:Users/carthach/Documents/Mendeley Desktop/Sturm - 2006 - Adaptive Concatenative Sound Synthesis and Its Application to Micromontage Composition.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {4},
pages = {46--66},
title = {{Adaptive Concatenative Sound Synthesis and Its Application to Micromontage Composition}},
volume = {30},
year = {2006}
}
@misc{Lee2006,
abstract = {In this paper, we propose a novel method for obtaining la- beled training data to estimate the parameters in a super- vised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to esti- mate model parameters. Experimental results show higher performance in frame-level chord recognition than the pre- vious approaches.},
author = {Lee, Kyogu and Slaney, Malcolm},
booktitle = {International Society for Music Information Retrieval},
doi = {10.1145/1178723.1178726},
file = {:Users/carthach/Documents/Mendeley Desktop/Lee, Slaney - 2006 - Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.pdf:pdf},
isbn = {1595935010},
issn = {10413200},
keywords = {chord recognition,hidden markov model},
pages = {2--6},
title = {{Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.9311{\&}rep=rep1{\&}type=pdf{\%}5Cnhttps://ccrma.stanford.edu/{~}kglee/pubs/klee-ismir06.pdf},
year = {2006}
}
@book{lysloff2003music,
author = {Lysloff, Ren{\'{e}} T A and {Gay Jr}, Leslie C},
publisher = {Wesleyan University Press},
title = {{Music and technoculture}},
year = {2003}
}
@article{Bartsch2005,
author = {Bartsch, M.a. and Wakefield, G.H.},
doi = {10.1109/TMM.2004.840597},
file = {:Users/carthach/Documents/Mendeley Desktop/Bartsch, Wakefield - 2005 - Audio thumbnailing of popular music using chroma-based representations.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
month = {feb},
number = {1},
pages = {96--104},
title = {{Audio thumbnailing of popular music using chroma-based representations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1386245},
volume = {7},
year = {2005}
}
@article{Jones,
author = {Jones, Daniel and Brown, Andrew R},
file = {:Users/carthach/Documents/Mendeley Desktop/Jones, Brown - Unknown - The Extended Composer Creative reflection and extension with generative tools.pdf:pdf},
pages = {1--31},
title = {{The Extended Composer Creative reflection and extension with generative tools}}
}
@article{Alwakeel2009,
author = {Alwakeel, Ramzy},
doi = {10.12801/1947-5403.2009.01.01.01},
file = {:Users/carthach/Documents/Mendeley Desktop/Alwakeel - 2009 - IDM as a Minor Literature The Treatment of Cultural and Musical Norms by Intelligent Dance Music.pdf:pdf},
issn = {19475403},
journal = {Dancecult: Journal of Electronic Dance Music Culture},
keywords = {193,1999,aphex twin,autechre,can you really kiss,deleuze,genre,idm,minority,morphology,simon reynolds,subjectivity,the sky with your,tongue in cheek,warp},
number = {1},
pages = {1--21},
title = {{IDM as a "Minor" Literature: The Treatment of Cultural and Musical Norms by "Intelligent Dance Music"}},
volume = {1},
year = {2009}
}
@article{Barbosa2015,
abstract = {Evaluation has been suggested to be one of the main trends in current NIME research. However, the meaning of the term for the community may not be as clear as it seems. In order to explore this issue, we have analyzed all papers and posters published in the proceedings of the NIME conference from 2012 to 2014. For each publication that explicitly mentioned the term "evaluation", we looked for: a) What targets and stakeholders were considered? b) What goals were set? c) What criteria were used? d) What methods were used? e) How long did the evaluation last? Results show different understandings of evaluation, with little consistency regarding the usage of the word. Surprisingly in some cases, not even basic information such as goal, criteria and methods were provided. In this paper, we attempt to provide an idea of what "evaluation" means for the NIME community, pushing the discussion towards how could we make a better use of evaluation on NIME design and what criteria should be used regarding each goal.},
author = {Barbosa, Jeronimo and Malloch, Joseph and Wanderley, Marcelo M. and Huot, St{\'{e}}phane},
file = {:Users/carthach/Documents/Mendeley Desktop/Barbosa et al. - 2015 - What does Evaluation mean for the NIME community.pdf:pdf},
journal = {NIME 2015 - 15th International Conference on New Interfaces for Musical Expression},
keywords = {Digital Musical Instruments,Evaluation,Metareview,Methodology},
pages = {6},
title = {{What does " Evaluation " mean for the NIME community?}},
url = {https://hal.inria.fr/hal-01158080/},
year = {2015}
}
@inproceedings{Cope1987,
author = {Cope, David},
booktitle = {ICMC Proceedings},
file = {:Users/carthach/Documents/Mendeley Desktop/Cope - 1987 - Experiments in Music Intelligence (EMI).pdf:pdf},
pages = {174--181},
title = {{Experiments in Music Intelligence (EMI)}},
year = {1987}
}
@article{Bradski2000,
abstract = {OpenCV is an open-source, computer-vision library for extracting and processing meaningful data from images. Additional resources include opencv.txt (listings).},
author = {Bradski, G},
doi = {10.1111/0023-8333.50.s1.10},
isbn = {3540306188},
issn = {1044-789X},
journal = {Dr Dobbs Journal of Software Tools},
pages = {120--125},
pmid = {22118455},
title = {{The OpenCV Library}},
url = {http://opencv.willowgarage.com},
volume = {25},
year = {2000}
}
@article{Lewis2007,
abstract = {In this paper we present methodology of categorization of musical instruments sounds, aiming at the continuing goal of codifying the classiffication of these sounds for automating indexing and retrieval purposes. The proposed categorization is based on numerical parameters. The motivation for this paper is based upon the fallibility of Hornbostel and Sachs generic classiffication scheme, most commonly used for categorization of musical instruments. In eliminating the discrepancies of Hornbostel and Sachs' classiffication of musical sounds we present a procedure that draws categorization from numerical attributes, describing both time domain and spectrum of sound, rather than using classiffication based directly on Hornbostel and Sachs scheme. As a result we propose a categorization system based upon the empirical musical parameters and then incorporating the resultant structure for classiffication rules.},
author = {Lewis, Rory A and Wieczorkowska, Alicja},
doi = {10.1007/978-1-84628-992-7_12},
file = {:Users/carthach/Documents/Mendeley Desktop/Categorization{\_}of{\_}musical{\_}instrument{\_}sou.pdf:pdf},
isbn = {978-1-84628-990-3},
journal = {Iccs 2007},
pages = {87},
title = {{Categorization of Musical Instrument Sounds Based on Numerical Parameters}},
url = {http://adsabs.harvard.edu/cgi-bin/nph-data{\_}query?bibcode=2007iccs.conf...87L{\&}link{\_}type=ABSTRACT},
year = {2007}
}
@article{Gustafson1988,
author = {Gustafson, K},
file = {:Users/carthach/Documents/Mendeley Desktop/Gustafson - 1988 - The graphical representation of rhythm.pdf:pdf},
journal = {PROPH) Progress Reports from Oxford Phonetics},
title = {{The graphical representation of rhythm}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+graphical+representation+of+rhythm{\#}0},
year = {1988}
}
@article{Grzywczak2014,
abstract = {Applications of Convolutional Neural Networks (CNNs) to various problems have been the subject of a number of recent studies ranging from image classification and object detection to scene parsing, segmentation 3D volumetric images and action recognition in videos. CNNs are able to learn input data representation, instead of using fixed engineered features. In this study, the image model trained on CNN were applied to a Music Information Retrieval (MIR), in particular to musical genre recognition. Themodel was trained on ILSVRC-2012 (more than 1million natural images) to perform image classification and was reused to perform genre classification using spectrograms images. Harmonic/percussive separation was applied, because it is characteristic formusical genre. At final stage, the evaluation of various strategies of merging Support Vector Machines (SVMs) was performed on well known in MIR community - GTZAN dataset. Even though, the model was trained on natural images, the results achieved in this study were close to the state-of-the-art.},
author = {Grzywczak, Daniel and Gwardys, Grzegorz},
doi = {10.2478/eletel-2014-0042},
issn = {2300-1933},
journal = {Active Media Technology},
keywords = {classification,convolutional neural networks,deep learning,genre,music information retrieval,transfer learning},
number = {4},
pages = {187--199},
title = {{Audio Features in Music Information Retrieval}},
volume = {60},
year = {2014}
}
@article{Post2011,
author = {Post, Olaf and Toussaint, Godfried},
file = {:Users/carthach/Documents/Mendeley Desktop/Post, Toussaint - 2011 - The Edit Distance as a Measure of Perceived Rhythmic Similarity.pdf:pdf},
journal = {Empirical Musicology Review},
keywords = {alterations,edit distance,levenshtein distance,mantel test,metrical hierarchies,music information,music perception,music theory,musical rhythm,phylogenetic trees,retrieval,rhythmic},
number = {3},
pages = {164--179},
title = {{The Edit Distance as a Measure of Perceived Rhythmic Similarity}},
url = {https://kb.osu.edu/dspace/handle/1811/52811},
volume = {6},
year = {2011}
}
@article{Atkinson2007,
abstract = {Sound provides an often-ignored element of our conceptualisation of the urban fabric. The power of music, sound and noise to denote place and demarcate space is used here to develop the idea of a sonic ecology. The paper attempts to map the relative order of this unseen city and to theorise its spatial and temporal patterning. The sonic ecology, a relatively persistent and chronologically ordered quality to sound in urban space, is used as a means of examining the distribution of sound and to weigh the broader social impact of these qualities. The ambient soundscape of the street is made up of a shifting aural terrain, a resonant metropolitan fabric, which may exclude or subtly guide us in our experience of the city, thus highlighting an invisible yet highly affecting and socially relevant area of urban enquiry.},
author = {Atkinson, Rowland},
doi = {10.1080/00420980701471901},
file = {:Users/carthach/Documents/Mendeley Desktop/Ecology{\_}of{\_}sound.pdf:pdf},
isbn = {1360-063X, 0042-0980},
issn = {00420980},
journal = {Urban Studies},
number = {10},
pages = {1905--1917},
title = {{Ecology of sound: The sonic order of urban space}},
volume = {44},
year = {2007}
}
@article{Cardoso2009,
abstract = {In this article, we survey the history of studies of computational creativity, following the development of the International Conference on Computational Creativity from its beginnings, a decade ago, in two parallel workshop series. We give a brief outline of key issues and a summary of the various different approaches taken by participants in the research field. The outlook is optimistic: a lot has been achieved in 10 years.},
author = {Cardoso, Am{\'{i}}lcar and Veale, Tony and Wiggins, Geraint},
doi = {10.1609/aimag.v30i3.2252},
file = {:Users/carthach/Documents/Mendeley Desktop/2252-3074-1-PB.pdf:pdf},
isbn = {0738-4602},
issn = {0738-4602},
journal = {AI Magazine},
number = {3},
pages = {15--22},
title = {{Converging on the Divergent: The History (and Future) of the International Joint Workshops in Computational Creativity}},
volume = {30},
year = {2009}
}
@article{Collins2012a,
abstract = {Audio content analysis can assist investigation of musical inﬂuence, given a corpus of date-annotated works. We study a number of techniques which illuminate musicological questions on genre and creative inﬂuence. By applying machine learning tests and statistical analysis to a database of early EDM tracks, we examine how distinct putatively different musical genres really are, the retrospectively la- belled Detroit techno and Chicago house being the core case study. Further, by building predictive models based on works from earlier years, both by a priori assumed genre groups and by individual tracks, we examine questions of inﬂuence, and whether Detroit techno really is a sort of electronic future funk, and Chicago house an electronic ex- tension of disco. We discuss the implications and prospects for modeling musical inﬂuence.},
author = {Collins, Nick},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2012 - Influence in early electronic dance music an audio content analysis investigation.pdf:pdf},
isbn = {9789727521449},
journal = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR 2012)},
number = {Ismir},
pages = {1--6},
title = {{Influence in early electronic dance music: an audio content analysis investigation}},
year = {2012}
}
@article{Maestre2006,
abstract = {We present here a concatenative sample-based saxophone synthesizer using an induced performancemodel intended for expressive synthesis. The system consists on three main parts. The first part provides the analysis of saxophone expressive performance recordings and the extraction of descriptors related to different temporal levels. With the obtained descriptors and the analyzed samples, we construct an annotated sample database extracted directly from the performances. For the second part, we use the annotations to induce a performance model capable of predicting some features related to expressivity. In the third part, the predictions of the performance model are used to retrieve the most suitable note samples for each situation, and transform and concatenate them following the input score and the induced model.},
author = {Maestre, Esteban and Hazan, Amaury and Ramirez, Rafael and Perez, Alfonso},
file = {:Users/carthach/Documents/Mendeley Desktop/USING{\_}CONCATENATIVE{\_}SYNTHESIS{\_}FOR{\_}EXPRES.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {163--166},
title = {{Using concatenative synthesis for expressive performance in jazz saxophone}},
volume = {2006},
year = {2006}
}
@article{Goßmann2014,
abstract = {We present an instrument for audio-visual performance that allows to recombine sounds from a collection of sampled media through concatenative synthesis. A three-dimensional distribution derived from feature-analysis becomes accessible through a theremin-inspired interface, allowing the player to shift from exploration and intuitive navigation toward embodied performance on a granular level. In our example we illustrate this concept by using the audiovisual recording of an instrumental performance as a source. Our system provides an alternative interface to the musical instrument's audiovisual corpus: as the instrument's sound and behavior is accessed in ways that are not possible on the instrument itself, the resulting non-linear playback of the grains generates an instant remix in a cut-up aesthetic. The presented instrument is a human-computer interface that employs the structural outcome of machine analysis accessing audiovisual corpora in the context of a musical performance.},
author = {Go{\ss}mann, Joachim and Neupert, Max},
file = {:Users/carthach/Documents/Mendeley Desktop/nime2014{\_}296.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {audio-visual,cloud,concatenative syn-,contact-free control,cut-up,embodiment,glitch,instrument,leap motion,point,real-time,scatter plot,thesis,video},
pages = {151--154},
title = {{Musical Interface to Audiovisual Corpora of Arbitrary Instruments}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}296.pdf},
year = {2014}
}
@article{McDonald2013,
author = {McDonald, I I and Austin, C},
file = {:Users/carthach/Documents/Mendeley Desktop/McDonald, Austin - 2013 - The Hero of Copyright Reform Exploring Non-Cochlear Impacts of Girl Talk's Plunderphonics.pdf:pdf},
journal = {Kaleidoscope: A Graduate Journal of {\ldots}},
keywords = {a computer,and i traveled across,collage,concert,copyright,girl talk,guy in sweatpants with,if we,in october of 2009,just a skinny white,lines to see a,microphone,no instruments would be,no singer at the,on stage,plunderphonics,sampling,state,two carpools of friends},
pages = {53--68},
title = {{The Hero of Copyright Reform: Exploring Non-Cochlear Impacts of Girl Talk's Plunderphonics}},
url = {http://opensiuc.lib.siu.edu/kaleidoscope/vol12/iss1/5/},
volume = {12},
year = {2013}
}
@book{theberge1997any,
author = {Th{\'{e}}berge, Paul},
publisher = {Wesleyan University Press},
title = {{Any sound you can imagine: Making music/consuming technology}},
year = {1997}
}
@article{Pachet2008,
author = {Pachet, Fran{\c{c}}ois},
file = {:Users/carthach/Documents/Mendeley Desktop/pachet-10b.pdf:pdf},
journal = {Computers in Entertainment (CIE)},
number = {3},
pages = {1--20},
title = {{The Future of Content is in Ourselves}},
volume = {6},
year = {2008}
}
@article{Ghedini2015,
abstract = {This chapter introduces the vision and the technical challenges of the Flow Machines project. Flow Machines aim at fostering creativity in artistic domains such as music and literature. We first observe that typically, great artists do not output just single artefacts but develop novel, individual styles. Style mirrors an individual's uniqueness; style makes an artist's work recognised and recognisable. Artists develop their own style after prolonged periods of imitation and exploration of the style of others. We envision style exploration as the application of existing styles, considered as texture, to arbitrary constraints, considered as structure. The goal of Flow Machines is to assist this process by allowing users to explicitly manipulate styles as computational objects. During interactions with Flow Machines, the user can create artefacts (melodies, texts, orchestrations) by combining styles with arbitrary constraints. Style exploration under user-defined constraints raises complex sequence generation issues that were addressed and solved for the most part during the first half of the project. We illustrate the potential of these techniques for style exploration with three examples.},
author = {Ghedini, Fiammetta and Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1007/978-981-287-618-8_18},
file = {:Users/carthach/Documents/Mendeley Desktop/ghedini-15b.pdf:pdf},
isbn = {9789812876188},
journal = {Multidisciplinary Contributions to the Science of Creative Thinking},
pages = {325--343},
title = {{Creating music and texts with flow machines}},
year = {2015}
}
@article{Kestler2008,
abstract = {BACKGROUND: Microarray experiments generate vast amounts of data. The functional context of differentially expressed genes can be assessed by querying the Gene Ontology (GO) database via GoMiner. Directed acyclic graph representations, which are used to depict GO categories enriched with differentially expressed genes, are difficult to interpret and, depending on the particular analysis, may not be well suited for formulating new hypotheses. Additional graphical methods are therefore needed to augment the GO graphical representation. RESULTS: We present an alternative visualization approach, area-proportional Euler diagrams, showing set relationships with semi-quantitative size information in a single diagram to support biological hypothesis formulation. The cardinalities of sets and intersection sets are represented by area-proportional Euler diagrams and their corresponding graphical (circular or polygonal) intersection areas. Optimally proportional representations are obtained using swarm and evolutionary optimization algorithms. CONCLUSION: VennMaster's area-proportional Euler diagrams effectively structure and visualize the results of a GO analysis by indicating to what extent flagged genes are shared by different categories. In addition to reducing the complexity of the output, the visualizations facilitate generation of novel hypotheses from the analysis of seemingly unrelated categories that share differentially expressed genes.},
author = {Kestler, Hans A and M{\"{u}}ller, Andr{\'{e}} and Kraus, Johann M and Buchholz, Malte and Gress, Thomas M and Liu, Hongfang and Kane, David W and Zeeberg, Barry R and Weinstein, John N},
doi = {10.1186/1471-2105-9-67},
file = {:Users/carthach/Documents/Mendeley Desktop/Kestler et al. - 2008 - VennMaster area-proportional Euler diagrams for functional GO analysis of microarrays.pdf:pdf},
isbn = {1471-2105},
issn = {1471-2105},
journal = {BMC bioinformatics},
pages = {67},
pmid = {18230172},
title = {{VennMaster: area-proportional Euler diagrams for functional GO analysis of microarrays.}},
volume = {9},
year = {2008}
}
@article{Carpentier2010,
abstract = {The article presents information on Orchid{\'{e}}e orchestration system that has the capability to interface with automated music environments via a client/service design. Interfaces mentioned included OpenMusic, Max/MSP and MATLAB, all of which used feedback loops and preferences-inference logic to facilitate linkage of symbols and sounds. The author provides graphical illustrations and detailed descriptions of the framework of the system, along with specifications, search processes and graphical user interfaces (GUI).},
author = {Carpentier, Gr{\'{e}}goire and Bresson, Jean},
doi = {10.1162/comj.2010.34.1.10},
file = {:Users/carthach/Documents/Mendeley Desktop/orchidee{\_}Carpentier{\_}Bresson{\_}2010.pdf:pdf},
isbn = {01489267},
issn = {0148-9267},
journal = {Computer Music Journal},
keywords = {COMPUTER interfaces,COMPUTER music,GRAPHICAL user interfaces (Computer systems),INTERFERENCE (Sound),MUSIC},
number = {1},
pages = {10--27},
pmid = {48428087},
title = {{Interacting with symbol, sound, and feature spaces in Orchid{\'{e}}e, a computer-aided orchestration environment}},
url = {http://gateway.library.qut.edu.au/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=afh{\&}AN=48428087{\&}site=ehost-live{\&}scope=site},
volume = {34},
year = {2010}
}
@article{Wang2006,
abstract = {In this paper, we present a new environmental sound classification architecture. The proposed sound classifier is performed in frame level and fuses the support vector machine (SVM) and the k nearest neighbor rule (KNN). In feature selection, three MPEG-7 audio low-level descriptors, spectrum centroid, spectrum spread, and spectrum flatness are used as the sound features to exploit their ability in sound classification. Experiments carried out on a 12-class sound database can achieve an 85.1 {\%} accuracy rate. The performance comparison between the HMM sound classifier using audio spectrum projection features demonstrates the superiority of the proposed scheme.},
author = {Wang, Jia-Ching and Wang, Jhing-Fa and He, Kuok Wai and Hsu, Cheng-Shu},
doi = {10.1109/IJCNN.2006.246644},
file = {:Users/carthach/Documents/Mendeley Desktop/01716317.pdf:pdf},
isbn = {0-7803-9490-9},
issn = {10987576},
journal = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
pages = {1731--1735},
title = {{Environmental Sound Classification using Hybrid SVM/KNN Classifier and MPEG-7 Audio Low-Level Descriptor}},
year = {2006}
}
@inproceedings{Smith2015,
address = {Maynooth, Ireland},
author = {Smith, Jordan B L and Percival, Graham and Kato, Jun and Goto, Masataka and Fukayama, Satoru},
booktitle = {Sound and Music Computing Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/SMC2015smith.pdf:pdf},
isbn = {9780992746629},
title = {{CrossSong Puzzle: Generating and Unscrambling Music Mashups with Real-time Interactivity}},
year = {2015}
}
@article{Che2009,
author = {Che, Deborah},
journal = {Sound, Society and the Geography of Popular Music},
pages = {261--280},
publisher = {Ashgate Burlington, VT},
title = {{Techno: music and entrepreneurship in post-fordist Detroit}},
year = {2009}
}
@misc{RobertM.KellerandDavidMorrisonandStephenJoa,
author = {{Robert M. Keller and David Morrison and Stephen Jo}},
keywords = {Robert M. Keller and David Morrison and Stephen Jo},
title = {{A Computational Framework Enhancing Jazz Creativity}},
url = {http://www.cs.hmc.edu/{~}keller/jazz/improvisor/jazzCreativity.pdf}
}
@article{Jensen2004,
author = {Jensen, Kristoffer and Andersen, TH},
file = {:Users/carthach/Documents/Mendeley Desktop/Jensen, Andersen - 2004 - Real-Time Beat EstimationUsing Feature Extraction.pdf:pdf},
journal = {Computer Music Modeling and Retrieval},
title = {{Real-Time Beat EstimationUsing Feature Extraction}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-39900-1{\_}2},
year = {2004}
}
@article{Todd1999,
author = {Todd, Peter M and Werner, Gregory M},
journal = {Musical networks: parallel distributed perception and performace},
pages = {313--340},
title = {{Frankensteinian methods for evolutionary music}},
year = {1999}
}
@article{Lakkavalli2010,
abstract = {A new method based on unit continuity metric (UCM) is proposed for optimal unit selection in text-to-speech (TTS) synthesis. UCM employs two features, namely, pitch continuity metric and spectral continuity metric. The methods have been implemented and tested on our test bed called MILE-TTS and it is available as web demo. After verification by a self selection test, the algorithms are evaluated on 8 paragraphs each for Kannada and Tamil by native users of the languages. Mean-opinion-score (MOS) shows that naturalness and comprehension are better with UCM based algorithm than the non-UCM based ones. The naturalness of the TTS output is further enhanced by a new rule based algorithm for pause prediction for Tamil language. The pauses between the words are predicted based on parts-of-speech information obtained from the input text. {\textcopyright} 2010 IEEE.},
author = {Lakkavalli, Vikram Ramesh and Arulmozhi, P. and Ramakrishnan, A. G.},
doi = {10.1109/SPCOM.2010.5560495},
file = {:Users/carthach/Documents/Mendeley Desktop/Lakkavalli, Arulmozhi, Ramakrishnan - 2010 - Continuity metric for unit selection based text-to-speech synthesis.pdf:pdf},
isbn = {9781424471362},
journal = {2010 International Conference on Signal Processing and Communications, SPCOM 2010},
keywords = {Kannada,MFCC,MILE-TTS,Part-of-speech,Pause model,Pitch continuity metric,Spectral continuity metric,Tamil,Unit continuity metric,Unit selection},
title = {{Continuity metric for unit selection based text-to-speech synthesis}},
year = {2010}
}
@article{Cohen-vrignaud2014,
author = {Cohen-vrignaud, Gerard},
doi = {10.1525/rep.2008.104.1.92.This},
file = {:Users/carthach/Documents/Mendeley Desktop/Cohen-vrignaud - 2014 - University of California Press.pdf:pdf},
isbn = {0520057295},
journal = {Nineteenth-Century Literature},
keywords = {meter,polyphony,rhythm,syncopation},
number = {2},
pages = {1--32},
title = {{University of California Press}},
volume = {68},
year = {2014}
}
@article{Muscutt2007,
abstract = {David Cope (see Figure 1) is a well-known composer, programmer, author and, since 1977, Professor of Music at the University of California, Santa Cruz. His five books (Cope 1991, 1996, 2000, 2001, and 2005), four recordings (Cope 1994, 1997, 1999, 2003), and many published articles on his program Experiments in Musical Intelligence, affectionately known as Emmy, describe 25 years of research into musical style simulation and related topics including musical recombinance, pattern matching, allusions, structure, and form detection. Table 1 presents a listing of Mr. Copes compositions (spanning fifty years as of 2006) and a general description of Emmys prodigious output. As a writer and colleague of Mr. Cope at UCSC, I have been tracking the evolution of Emmy since its inception in the mid 1980s. This interview, concentrating on Mr. Copes views on algorithms and music, is a distillation of approximately six hours of recorded conversation as well as occasional notes and correspondence.},
author = {Muscutt, Keith},
doi = {10.1162/comj.2007.31.3.10},
file = {:Users/carthach/Documents/Mendeley Desktop/comj.2007.31.3.10.pdf:pdf},
isbn = {0148-9267, 0148-9267},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {3},
pages = {10--22},
title = {{Composing with Algorithms: An Interview with David Cope}},
url = {http://www.mitpressjournals.org/doi/10.1162/comj.2007.31.3.10},
volume = {31},
year = {2007}
}
@phdthesis{J.Butler2003,
author = {{J. Butler}, Mark},
file = {:Users/carthach/Documents/Mendeley Desktop/J. Butler - 2003 - Unlocking the Groove Rhythm, Meter, and Musical Design in Electronic Dance Music.pdf:pdf},
school = {Indiana University},
title = {{Unlocking the Groove: Rhythm, Meter, and Musical Design in Electronic Dance Music}},
year = {2003}
}
@article{Korzeniowski2014,
abstract = {We present a probabilistic way to extract beat positions from the output (activations) of the neural network that is at the heart of an existing beat tracker. The method can serve as a replacement for the greedy search the beat tracker cur-rently uses for this purpose. Our experiments show im-provement upon the current method for a variety of data sets and quality measures, as well as better results com-pared to other state-of-the-art algorithms.},
author = {Korzeniowski, Filip and Sebastian, B and Widmer, Gerhard},
file = {:Users/carthach/Documents/Mendeley Desktop/Korzeniowski{\_}ISMIR{\_}2014.pdf:pdf},
journal = {Proceedings of the 15th International Society for Music Information Retrieval Conference (ISMIR 2014)},
number = {Ismir},
pages = {513--518},
title = {{Probabilistic Extraction of Beat positions from a Beat Activation Function}},
year = {2014}
}
@article{Paiement2007,
abstract = {In order to cope for the difficult problem of long term dependencies in sequential data in general, and in musical data in particular, a generative model for distance patterns especially designed for music is introduced. A specific implementation of the model when considering Hamming distances over rhythms is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy over two different music databases.},
author = {Paiement, Jean-Francois and Grandvalet, Yves and Bengio, Samy and Eck, Douglas},
doi = {10.1145/1390156.1390249},
file = {:Users/carthach/Documents/Mendeley Desktop/Paiement et al. - 2007 - A generative model for rhythms.pdf:pdf},
isbn = {9781605582054},
journal = {NIPS'2007 Music Brain Cognition Workshop},
keywords = {learning,statistics {\&} optimisation},
pages = {1--8},
title = {{A generative model for rhythms}},
url = {http://eprints.pascal-network.org/archive/00003429/},
year = {2007}
}
@article{Scobie2014,
author = {Scobie, Niel},
file = {:Users/carthach/Documents/Mendeley Desktop/Scobie - 2014 - “We Wanted Our Coffee Black” Public Enemy, Improvisation, and Noise.pdf:pdf},
journal = {Critical Studies in Improvisation / {\'{E}}tudes critiques en improvisation},
number = {1},
pages = {1--9},
title = {{“We Wanted Our Coffee Black”: Public Enemy, Improvisation, and Noise}},
volume = {10},
year = {2014}
}
@article{Cardle2003,
author = {Cardle, Mark and Brooks, Stephen and Robinson, Peter},
file = {:Users/carthach/Documents/Mendeley Desktop/Cardle, Brooks, Robinson - 2003 - Audio and User Directed Sound Synthesis.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference (ICMC)},
title = {{Audio and User Directed Sound Synthesis}},
year = {2003}
}
@article{Maganti2010,
abstract = {In this paper, an auditory based modulation spectral feature is presented to improve automatic speech recognition performance in presence of room reverberation. The solution is based on extracting features from auditory processing characteristics, specifically gammatone filtering based long-term modulation spectral features to reduce sensitivity to environmental noise and further preserve the important speech intelligibility information in the speech signal essential for ASR. Experiments are performed on Aurora-5 meeting recorder digit task recorded with four different microphones in hands-free mode at a real meeting room. For comparison purposes the recognition results obtained using standard ETSI basic and advanced front-ends and conventional features with standard feature compensation are tested. The experimental results reveal that the proposed features provide reliable and considerable improvements with respect to the state of the art feature extraction techniques. {\textcopyright} 2010 ISCA.},
author = {Maganti, Harikrishna and Matassoni, Marco},
file = {:Users/carthach/Documents/Mendeley Desktop/63540d89917431b97b89a87dada0853c2ed9.pdf:pdf},
journal = {Spectrum},
keywords = {[Electronic Manuscript]},
number = {September},
pages = {570--573},
title = {{An Auditory Based Modulation Spectral Feature for Reverberant Speech Recognition}},
year = {2010}
}
@article{Mcfee2015,
abstract = {—This document describes version 0.4.0 of librosa: a Python pack-age for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the field of music information retrieval. In this document, a brief overview of the library's functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
author = {Mcfee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P W and Mcvicar, Matt and Battenberg, Eric and Nieto, Oriol},
file = {:Users/carthach/Documents/Mendeley Desktop/brian{\_}mcfee.pdf:pdf},
journal = {PROC. OF THE 14th PYTHON IN SCIENCE CONF},
keywords = {Index Terms—audio,music,signal processing},
number = {Scipy},
pages = {1--7},
title = {{librosa: Audio and Music Signal Analysis in Python}},
year = {2015}
}
@article{Burton1999b,
abstract = {Biologically inspired computational methods have recently$\backslash$nattracted much research interest in the field of computer music. This$\backslash$ngroup of methods is a subset of computational intelligence, and is$\backslash$nrepresented by neural networks (NNs), genetic algorithms (GAs) and$\backslash$ngenetic programming (GP). The article reviews how GA and GP have been$\backslash$napplied to musical tasks.},
author = {Burton, Ar and Vladimirova, T},
doi = {10.1162/014892699560001},
file = {:Users/carthach/Documents/Mendeley Desktop/Burton, Vladimirova - 1999 - Generation of musical sequences with genetic techniques.pdf:pdf},
isbn = {014892699560001},
issn = {0148-9267},
journal = {Computer Music Journal},
keywords = {Arts {\&} Humanities,Computer Science,Interdisciplinary Applications,Music,Science {\&} Technology,Technology},
title = {{Generation of musical sequences with genetic techniques}},
url = {http://hdl.handle.net/2381/18454},
year = {1999}
}
@article{Hesmondhalgh2017,
author = {Hesmondhalgh, David},
file = {:Users/carthach/Documents/Mendeley Desktop/Hesmondhalgh - 2017 - The British Dance Music Industry A Case Study of Independent Cultural Production.pdf:pdf},
journal = {The British Journal of Sociology},
number = {2},
pages = {234--251},
title = {{The British Dance Music Industry: A Case Study of Independent Cultural Production}},
volume = {49},
year = {2017}
}
@article{Miranda1995,
abstract = {Chaosynth is a new sound synthesis system being developed by the author and others working at Edinburgh University. Chaosynth functions by generating a large amount of short sonic events, or particles, in order to form larger, complex sound events. This synthesis technique is inspired by granular synthesis. Most granular synthesis techniques, however, use stochastic methods to control the formation of sound events, while Chaosynth uses a cellular automaton. Following an introduction to the basics of granular synthesis, the author explains how Chaosynth's technique works. He then introduces the basics of cellular automata and presents ChaOs, the cellular automaton used in Chaosynth. The article concludes with some final remarks and suggestions for further work.},
author = {Miranda, Eduardo Reck},
doi = {10.2307/1576193},
file = {:Users/carthach/Documents/Mendeley Desktop/1576193.pdf:pdf},
isbn = {0024094X},
issn = {0024-094X},
journal = {Leonardo},
number = {4},
pages = {297--300},
title = {{Granular Synthesis of Sounds by Means of a Cellular Automaton}},
url = {http://www.jstor.org/stable/1576193{\%}5Cnhttp://www.jstor.org/stable/pdfplus/1576193.pdf?acceptTC=true},
volume = {28},
year = {1995}
}
@inproceedings{Herrera2003,
author = {Herrera, Perfecto and Dehamel, Amaury and Gouyon, Fabien},
booktitle = {Audio Engineering Society 114th Convention},
file = {:Users/carthach/Documents/Mendeley Desktop/Herrera, Dehamel, Gouyon - 2003 - Automatic labeling of unpitched percussion sounds.pdf:pdf},
title = {{Automatic labeling of unpitched percussion sounds}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=12599},
year = {2003}
}
@article{Hoffman2009,
author = {Hoffman, Matthew and Cook, Perry R. and Blei, David M.},
file = {:Users/carthach/Documents/Mendeley Desktop/2009{\_}icmc{\_}mcmcviamcmc.pdf:pdf},
isbn = {9780971319271},
journal = {Proceedings of the 2009 International Computer Music Conference (ICMC)},
pages = {16--21},
title = {{Bayesian Spectral Matching: Turning Your MC Into MC Hammer via MCMC Sampling}},
year = {2009}
}
@article{Collins2007,
author = {Collins, Nick},
file = {:Users/carthach/Documents/Mendeley Desktop/audiovisual-concatenative-synthesis.pdf:pdf},
journal = {Proceedings of the International Computer Conference},
pages = {389--392},
title = {{Audiovisual concatenative synthesis}},
url = {http://composerprogrammer.com/research/avconcat.pdf},
year = {2007}
}
@book{Butler2006,
abstract = {Unlocking the Groove is a groundbreaking, award-winning, music-driven analysis of electronic dance music (EDM). Author Mark Butler interweaves traditional and non-traditional musical analysis with consideration of the genre's history and social significance, deconstructing several typical examples of electronic dance music and focusing on the interaction of beat and rhythmic structure in creating an overall musical design. Interviews with DJs, listeners, and producers flesh out the book, providing insight into the perceptions and performance world of EDM, and making a vivid case for the musical artistry of EDM disc jockeys. The CD included with the book illustrates the analysis with multiple musical examples, both in excerpts and full songs. Butler's work propels the study of popular music in exciting new directions, and will impact the range from popular music studies, music theory, ethnomusicology, and musicology [Publisher description]. Getting into the groove : approaching the study of electronic dance music. Introduction ; The history and creation of electronic dance music ; Conceptualizing rhythm and meter in electronic dance music -- Electronic dance music and interpretive multiplicity. Ambiguity ; Metrical dissonance -- Electronic dance music and the epic. Multimeasure patterning ; Form from the record to the set.},
author = {Butler, Mark J. (Mark Jonathan)},
isbn = {9780253346629},
pages = {346},
publisher = {Indiana University Press},
title = {{Unlocking the groove : rhythm, meter, and musical design in electronic dance music}},
year = {2006}
}
@book{Gerhard2003,
abstract = {Pitch extraction (also called fundamental frequency estimation) has been a popular topic in many fields of research since the age of computers. Yet in the course of some 50 years of study, current techniques are still not to a desired level of accuracy and robustness. When presented with a single clean pitched signal, most techniques do well, but when the signal is noisy, or when there are multiple pitch streams, many current pitch algorithms still fail to perform well. This report presents a discussion of the history of pitch detection techniques, as well as a survey of the current state of the art in pitch detection technology.},
author = {Gerhard, David},
booktitle = {Regina: Department of Computer Science},
file = {:Users/carthach/Documents/Mendeley Desktop/pitch.pdf:pdf},
isbn = {0773104550},
issn = {0828-3494},
pages = {0--22},
title = {{Pitch extraction and fundamental frequency: History and current techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.834{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@article{Gabrielsson1973b,
author = {Gabrielsson, Alf},
journal = {Scandinavian Journal of Psychology},
number = {1},
pages = {161--176},
title = {{Similarity ratings and dimension analyses of auditory rhythm patterns. 2}},
volume = {14},
year = {1973}
}
@article{Television2000,
author = {Television, Color},
file = {:Users/carthach/Documents/Mendeley Desktop/Television - 2000 - Do Not Open.pdf:pdf},
title = {{Do Not Open}},
year = {2000}
}
@article{Magnusson2014,
abstract = {After an eventful decade of live-coding activities, this article seeks to explore the practice with the aim of situating it in the history of contemporary arts and music. The article introduces several key points of investigation in live-coding research and discusses some examples of how live-coding practitioners engage with these points in their system design and performances. In light of the extremely diverse manifestations of live-coding activities, the problem of defining the practice is discussed, and the question is raised whether live coding is actually necessary as an independent category.},
author = {Jathal, Kunal},
doi = {10.1162/COMJ},
file = {:Users/carthach/Documents/Mendeley Desktop/07938136.pdf:pdf},
isbn = {8187672641},
issn = {1531-5169},
journal = {Computer Music Journal},
number = {41.2},
pages = {8--16},
title = {{Real-Time Timbre Classification for Tabletop Hand Drumming}},
year = {2017}
}
@article{Povel1985a,
author = {Povel, DJ and Essens, P},
file = {:Users/carthach/Documents/Mendeley Desktop/Povel, Essens - 1985 - Perception of Temporal Patterns.pdf:pdf},
journal = {Music Perception},
title = {{Perception of Temporal Patterns}},
url = {http://www.jstor.org/stable/10.2307/40285311},
year = {1985}
}
@book{Cox2004,
author = {Cox, Christoph and Warner, Daniel},
publisher = {A{\&}C Black},
title = {{Audio culture: Readings in modern music}},
year = {2004}
}
@article{Sturm2016,
abstract = {Building systems that possess the sensitivity and intelligence to identify and describe high-level attributes in music audio signals continues to be an elusive goal, but one that surely has broad and deep implications for a wide variety of applications. Hundreds of papers have so far been published toward this goal, and great progress appears to have been made. Some systems produce remarkable accuracies at recognising high-level semantic concepts, such as music style, genre and mood. However, it might be that these numbers do not mean what they seem. In this paper, we take a state-of-the-art music content analysis system and investigate what causes it to achieve exceptionally high performance in a benchmark music audio dataset. We dissect the system to understand its operation, determine its sensitivities and limitations, and predict the kinds of knowledge it could and could not possess about music. We perform a series of experiments to illuminate what the system has actually learned to do, and to what extent it is performing the intended music listening task. Our results demonstrate how the initial manifestation of music intelligence in this state-of-the-art can be deceptive. Our work provides constructive directions toward developing music content analysis systems that can address the music information and creation needs of real-world users.},
archivePrefix = {arXiv},
arxivId = {1606.03044},
author = {Sturm, Bob L.},
doi = {10.1145/0000000.0000000},
eprint = {1606.03044},
file = {:Users/carthach/Documents/Mendeley Desktop/Sturm - 2016 - The Horse'' Inside Seeking Causes Behind the Behaviours of Music Content Analysis Systems.pdf:pdf},
issn = {15443981},
journal = {Computers in Entertainment},
number = {2},
pages = {1--32},
title = {{The "Horse'' Inside: Seeking Causes Behind the Behaviours of Music Content Analysis Systems}},
url = {http://arxiv.org/abs/1606.03044},
volume = {14},
year = {2016}
}
@article{Pachet2004,
author = {Pachet, F.},
file = {:Users/carthach/Documents/Mendeley Desktop/67e13c2e6ab4e77e89fed663817e9c30d3f5.pdf:pdf},
journal = {A learning zone of one's own: sharing representations and flow in collaborative learning environments},
title = {{On the design of a musical flow machine}},
url = {http://books.google.ca/books?hl=en{\&}lr={\&}id=izzUZvMZ-WsC{\&}oi=fnd{\&}pg=PA113{\&}dq=music+{\%}22F+Pachet{\%}22{\&}ots=iaHdIBVBRi{\&}sig=e13juqKxKAnNUQoWtkIi-UPEWMo},
year = {2004}
}
@article{Biles2003,
author = {Biles, John A},
journal = {Leonardo},
number = {1},
pages = {43--45},
publisher = {MIT Press},
title = {{GenJam in perspective: a tentative taxonomy for GA music and art systems}},
volume = {36},
year = {2003}
}
@inproceedings{Panteli2014,
abstract = {A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop', a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between seg- ments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most no- tably: attack phase of onsets, periodicity of rhythmic el- ements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, af- ter which the similarity between segments can be calcu- lated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the over- all performance of the model with perceptual ratings of rhythm similarity.},
author = {Panteli, Maria and Bogaards, N and Honingh, A},
booktitle = {International Society for Music Information Retrieval Conference (ISMIR 2014)},
file = {:Users/carthach/Documents/Mendeley Desktop/35f74718b81d8d0281280fb061ee73437e39.pdf:pdf},
isbn = {9781632662842},
title = {{Modeling Rhythm Similarity for Electronic Dance Music}},
url = {https://staff.fnwi.uva.nl/a.k.honingh/publicaties/Pantelietal{\_}ISMIRSubmission{\_}July18th.pdf},
year = {2014}
}
@book{Mitchell1997,
author = {Mitchell, Tom},
file = {:Users/carthach/Documents/Mendeley Desktop/Mitchell - 1997 - Machine Learning.pdf:pdf;:Users/carthach/Documents/Mendeley Desktop/Machine Learning - Tom Mitchell.pdf:pdf},
publisher = {McGraw Hill},
title = {{Machine Learning}},
year = {1997}
}
@inproceedings{Miron2017,
author = {Miron, Marius and {Janer Mestres}, Jordi and {G{\'{o}}mez Guti{\'{e}}rrez}, Emilia},
booktitle = {Proceedings of the 14th Sound and Music Computing Conference},
organization = {Espoo, Finland},
title = {{Generating data to train convolutional neural networks for classical music source separation}},
year = {2017}
}
@article{Rodgers2003,
abstract = {Through examples from underground genres and interviews with electronic musicians, the sampler's relationship to other traditions of instrument playing is examined. Also discussed are the ways in which electronic musicians use the mechanisms of digital instruments—argued to be incorrectly conceived of as automated—to achieve nuanced musical expression and cultural commentary. The political implications of presenting sampled and processed sounds in a reconfigured compositional environment are explored. A reprint is cited as RILM [ref]2012-11972[/ref].},
author = {Rodgers, Tara},
doi = {10.1017/S1355771803000293},
file = {:Users/carthach/Documents/Mendeley Desktop/div-class-title-on-the-process-and-aesthetics-of-sampling-in-electronic-music-production-div.pdf:pdf},
isbn = {1355771803},
issn = {1355-7718},
journal = {Organised Sound},
number = {03},
pages = {313--320},
title = {{On the process and aesthetics of sampling in electronic music production}},
url = {http://www.journals.cambridge.org/abstract{\_}S1355771803000293},
volume = {8},
year = {2003}
}
@article{Knesebeck2010,
author = {von dem Knesebeck, A and Z{\"{o}}lzer, U},
file = {:Users/carthach/Documents/Mendeley Desktop/Knesebeck, Z{\"{o}}lzer - 2010 - COMPARISON OF PITCH TRACKERS FOR REAL-TIME GUITAR EFFECTS.pdf:pdf},
journal = {{\ldots} Int. Conf. on Digital Audio Effects ( {\ldots}},
pages = {1--4},
title = {{COMPARISON OF PITCH TRACKERS FOR REAL-TIME GUITAR EFFECTS}},
url = {http://dafx10.iem.at/proceedings/papers/VonDemKnesebeckZoelzer{\_}DAFx10{\_}P102.pdf},
year = {2010}
}
@article{bischoff1978music,
author = {Bischoff, John and Gold, Rich and Horton, Jim},
journal = {Computer Music Journal},
pages = {24--29},
publisher = {JSTOR},
title = {{Music for an interactive network of microcomputers}},
year = {1978}
}
@misc{Holden2015,
author = {Holden, James},
booktitle = {Ableton},
title = {{James Holden on Human Timing}},
url = {https://www.ableton.com/en/blog/james-holden-human-timing/},
urldate = {2015-09-28},
year = {2015}
}
@article{Dixon2004,
abstract = {A central problem in music information retrieval is finding suitable representations which enable efficient and accurate computation of musical similarity and identity. Low level audio features are ideal for calculating identity, but are of limited use for similarity measures, as many aspects of music can only be captured by considering high level features. We present a new method of characterising music by typical bar-length rhythmic patterns which are automatically extracted from the audio signal, and demonstrate the usefulness of this representation by its application in a genre classification task. Recent work has shown the importance of tempo and periodicity features for genre recognition, and we extend this research by employing the extracted temporal patterns as features. Standard classification algorithms are utilised to discriminate 8 classes of Standard and Latin ballroom dance music (698 pieces). Although pattern extraction is error-prone, and patterns are not always unique to a genre, classification by rhythmic pattern alone achieves up to 50{\%} correctness (baseline 16{\%}), and by combining with other features, a classification rate of 96{\%} is obtained.},
author = {Dixon, Simon and Gouyon, Fabien and Widmer, Gerhard},
file = {:Users/carthach/Documents/Mendeley Desktop/Dixon, Gouyon, Widmer - 2004 - Towards characterisation of music via rhythmic patterns.pdf:pdf},
isbn = {8488042442},
journal = {Proceedings of the 5th International Conference on Music Information Retrieval (ISMIR 2004)},
number = {April},
pages = {509--516},
title = {{Towards characterisation of music via rhythmic patterns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.6686{\&}rep=rep1{\&}type=pdf},
volume = {5},
year = {2004}
}
@article{Fernandez2013,
abstract = {Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.},
archivePrefix = {arXiv},
arxivId = {1402.0585},
author = {Fern{\'{a}}ndez, Jose David and Vico, Francisco},
doi = {10.1613/jair.3908},
eprint = {1402.0585},
file = {:Users/carthach/Documents/Mendeley Desktop/Fern{\'{a}}ndez, Vico - 2013 - Ai methods in algorithmic composition A comprehensive survey.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
pages = {513--582},
title = {{Ai methods in algorithmic composition: A comprehensive survey}},
volume = {48},
year = {2013}
}
@article{Levy2008,
author = {Levy, Mark and Sandler, Mark},
file = {:Users/carthach/Documents/Mendeley Desktop/Levy, Sandler - 2008 - Structural segmentation of musical audio by constrained clustering.pdf:pdf},
journal = {Audio, Speech, and Language Processing, {\ldots}},
number = {2},
pages = {318--326},
title = {{Structural segmentation of musical audio by constrained clustering}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4432648},
volume = {16},
year = {2008}
}
@article{Tanghe,
author = {Tanghe, Koen and Degroeve, Sven and Baets, Bernard De},
file = {:Users/carthach/Documents/Mendeley Desktop/Tanghe, Degroeve, Baets - 2005 - An algorithm for detecting and labeling drum events in polyphonic music.pdf:pdf},
journal = {{\ldots} of the 1st Annual Music {\ldots}},
keywords = {2 algorithm description,audio-to-midi,drum detection,feature ex-,onset detection,support vector machines,traction},
title = {{An algorithm for detecting and labeling drum events in polyphonic music}},
url = {http://www.music-ir.org/mirex/abstracts/2005/tanghe.pdf},
year = {2005}
}
@article{Herrera2002,
author = {Herrera, Perfecto and Yeterian, Alexandre and Gouyon, Fabien},
file = {:Users/carthach/Documents/Mendeley Desktop/Herrera, Yeterian, Gouyon - 2002 - Automatic classification of drum sounds a comparison of feature selection methods and classification.pdf:pdf},
journal = {International Conference on Music and Artificial Intelligence (ICMAI)},
title = {{Automatic classification of drum sounds: a comparison of feature selection methods and classification techniques}},
url = {http://link.springer.com/chapter/10.1007/3-540-45722-4{\_}8},
year = {2002}
}
@article{Collins2016,
author = {Collins, Nick},
doi = {10.1145/2967510},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2016 - Towards Machine Musicians Who Have Listened to More Music Than Us Audio Database-Led Algorithmic Criticism for Automat.pdf:pdf},
issn = {15443574},
journal = {Computers in Entertainment},
number = {3},
title = {{Towards Machine Musicians Who Have Listened to More Music Than Us : Audio Database-Led Algorithmic Criticism for Automatic r r}},
volume = {14},
year = {2016}
}
@phdthesis{Sewell2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Sewell, Amanda},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Sewell - 2013 - A Typology of Sampling in Hip-Hop.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
school = {Indiana University},
title = {{A Typology of Sampling in Hip-Hop}},
year = {2013}
}
@article{Varese1966,
abstract = {Through an historical analysis, the author presents the background to electronic music's birth and the important steps taken from 1948 to the 1960s. He details for each trend the main composers, works and recordings as well as technical discoveries. The book is completed by useful appendices: a history of the Cologne School, some thoughts on computers and music, the relationships between composer and computer music and a glossary of electronic terms. Table of contents: The breakup of the harmonic era The first revolution: 1900 The nineteenth-century romantic upheaval The American experimental tradition Revolution II: the 1950s The search for a new music The historical background Edgard Var{\`{e}}se and ``The liberation of sound'' Chronology of electronic music, from the invention of phonograph (1877-1896) to the first ``school'' of electronic music, musique concr{\`{e}}te (1948) The first ``schools'' of electronic music in the 1950s The breakthrough of the tape recorder Musique concr{\`{e}}te The Cologne studio The scene in America in the 1950s A chronology of electronic music from musique concr{\`{e}}te (1948) to the death of Var{\`{e}}se (1965) The experimental sixties Music and computers Pandora's box Music and numbers Electronic music on records Reviews of selected recordings A partial discography of electronic music on recordings by Peter Frank},
author = {Var{\`{e}}se, Edgard and Wen-chung, Chou},
doi = {10.2307/832385},
isbn = {0826416144},
issn = {00316016},
journal = {Perspectives of New Music},
number = {1},
pages = {11},
pmid = {21490948},
title = {{The Liberation of Sound}},
url = {http://www.jstor.org/stable/832385?origin=crossref},
volume = {5},
year = {1966}
}
@article{Xiang2002,
author = {Xiang, Pei},
file = {:Users/carthach/Documents/Mendeley Desktop/Xiang - 2002 - A New Scheme for Real-Time Loop Music Production Based on Granular Similarity and Probability Control.pdf:pdf},
journal = {Proceedings of the 5th International Conference on Digital Audio Effects (DAFx-02)},
pages = {89--92},
title = {{A New Scheme for Real-Time Loop Music Production Based on Granular Similarity and Probability Control}},
year = {2002}
}
@article{Hoskinson2001,
author = {Hoskinson, Reynald and Pai, Dinesh},
file = {:Users/carthach/Documents/Mendeley Desktop/Hoskinson, Pai - 2001 - Manipulation and Resynthesis with Natural Grains.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference (ICMC)},
pages = {338--341},
title = {{Manipulation and Resynthesis with Natural Grains}},
url = {http://hdl.handle.net/2027/spo.bbp2372.2001.057},
year = {2001}
}
@inproceedings{Toussaint2016,
address = {Athens, Greece},
author = {Toussaint, Godfried T and Oh, Seung Man},
booktitle = {Proceedings of the International Conference on Artificial Intelligence},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint, Oh - 2016 - Measuring Musical Rhythm Similarity Edit Distance versus Minimum-Weight Many-to-Many Matchings.pdf:pdf},
isbn = {1601324383},
keywords = {and vice-,by a silent pulse,consider the,duration unaltered,easures,edit distance,hungarian algorith m,leaves the rhyth m,m any m atchings,m any-to-,m usical rhyth m,of a sounded pulse,perception,reversal,si m ilarity m,versa},
pages = {186--},
title = {{Measuring Musical Rhythm Similarity: Edit Distance versus Minimum-Weight Many-to-Many Matchings}},
year = {2016}
}
@misc{Ellis2009,
author = {Ellis, Daniel P. W.},
title = {{Gammatone-like spectrograms}},
urldate = {2017-11-01},
year = {2009}
}
@article{Guastavino2009a,
author = {Guastavino, Catherine and G{\'{o}}mez, Francisco and Toussaint, Godfried and Marandola, Fabrice and G{\'{o}}mez, Emilia},
doi = {10.1080/09298210903229968},
file = {:Users/carthach/Documents/Mendeley Desktop/Guastavino et al. - 2009 - Measuring Similarity between Flamenco Rhythmic Patterns.pdf:pdf},
issn = {0929-8215},
journal = {Journal of New Music Research},
month = {nov},
number = {2},
pages = {129--138},
title = {{Measuring Similarity between Flamenco Rhythmic Patterns}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09298210903229968},
volume = {38},
year = {2009}
}
@article{Gibson1991,
author = {Gibson, PM and Byrne, JA},
file = {:Users/carthach/Documents/Mendeley Desktop/Gibson, Byrne - 1991 - Neurogen, musical composition using genetic algorithms and cooperating neural networks.pdf:pdf},
journal = {Second International Conference on Artificial Neural Networks},
title = {{Neurogen, musical composition using genetic algorithms and cooperating neural networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=140338},
year = {1991}
}
@article{Lartillot2008,
author = {Lartillot, Olivier and Toiviainen, Petri and Eerola, Tuomas},
journal = {Data analysis, machine learning and applications},
pages = {261--268},
publisher = {Springer},
title = {{A matlab toolbox for music information retrieval}},
year = {2008}
}
@article{Robindore2009,
author = {Robindore, Brigitte and Xenakis, Iannis},
file = {:Users/carthach/Documents/Mendeley Desktop/Robindore1996a.pdf:pdf},
journal = {Computer Music Journal},
number = {4},
pages = {11--16},
title = {{Eskhate Ereuna : Limits of Extending the Musical Thought: Comments On and By lannis Xenakis}},
volume = {20},
year = {1996}
}
@article{Zapata2012,
abstract = {In this paper we establish a threshold for perceptually acceptable beat tracking based on the mutual agreement of a committee of beat trackers. In the ﬁrst step we use an existing annotated dataset to show that mutual agreement can be used to select one committee member as the most reliable beat tracker for a song. Then we conduct a listening test using a subset of the Million Song Dataset to establish a threshold which results in acceptable quality of the chosen beat output. For both datasets, we obtain a percentage of trackable music of about 73{\%}, and we investigate which data tags are related to acceptable and problematic beat tracking. The results indicate that current datasets are biased towards genres which tend to be easy for beat tracking. The proposed methods provide a means to automatically obtain a conﬁdence value for beat tracking in nonannotated data and to choose between a number of beat tracker outputs.},
author = {Zapata, Jose R and Holzapfel, Andr{\'{e}} and Davied, Matthew EP and Oliveira, Jo{\~{a}}o L and Gouyon, Fabien},
file = {:Users/carthach/Documents/Mendeley Desktop/Zapata et al. - 2012 - Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets.pdf:pdf},
isbn = {978-972-752-144-9},
journal = {Proc. of the Int. Conf. on Music Information Retrieval (ISMIR)},
number = {Ismir},
pages = {157--162},
title = {{Assigning a Confidence Threshold on Automatic Beat Annotation in Large Datasets}},
url = {http://telecom.inescporto.pt/{~}mdavies/pdfs/ZapataEtAl12-ismir.pdf},
year = {2012}
}
@article{Coleman2010,
abstract = {We propose a strategy for integrating descriptor-driven transforma- tion into mosaicing sound synthesis, in which samples are selected by taking into account potential distances in the transformed space. Target descriptors consisting of chroma, mel-spaced filter banks, and energy are modeled with respect to windowed bandlimited resampling and mel-spaced filters, and later corrected with gain. These transformations, however simple, allow some adaptation of textural sound material to musical contexts.},
author = {Coleman, Graham and Maestre, Esteban and Bonada, Jordi},
file = {:Users/carthach/Documents/Mendeley Desktop/dafx10{\_}augmos.pdf:pdf},
isbn = {9783200019409},
journal = {Proc. Digital Audio Effects (DAFx-10)},
pages = {1--4},
title = {{Augmenting Sound Mosaicing with Descriptor-Driven Transformation}},
year = {2010}
}
@article{Donaldson2007,
abstract = {Chroma based representations of acoustic phenomenon are representations of sound as pitched acoustic energy. A framewise chroma distribution over an entire musical piece is a useful and straightforward representation of its musical pitch over time. This paper examines a method of condensing the block-wise chroma information of a musical piece into a two dimensional embedding. Such an embedding is a representation or map of the different pitched energies in a song, and how these energies relate to each other in the context of the song. The paper presents an interactive version of this representation as an exploratory analytical tool or instrument for granular synthesis. Pointing and clicking on the interactive map recreates the acoustical energy present in the chroma blocks at that location, providing an effective way of both exploring the relationships between sounds in the original piece, and recreating a synthesized approximation of these sounds in an instrumental fashion. },
author = {Donaldson, Justin and Knopke, Ian and Raphael, Chris},
doi = {10.1145/1279740.1279782},
file = {:Users/carthach/Documents/Mendeley Desktop/Donaldson, Knopke, Raphael - 2007 - Chroma Palette Chromatic Maps of Sound As Granular Synthesis Interface.pdf:pdf},
journal = {Proceedings of the 2007 Conference on New Interfaces for Musical Expression (NIME07)},
keywords = {Chroma,dimensionality reduction,granular synthesis},
pages = {213--219},
title = {{Chroma Palette : Chromatic Maps of Sound As Granular Synthesis Interface}},
url = {http://www.nime.org/proceedings/2007/nime2007{\_}213.pdf},
year = {2007}
}
@article{Collins2005,
abstract = {This paper considers how to achieve new creative advances in the design of programming languages. It is based on the analysis of a single application domain, the practice of Live Coding in a new area of musical performance known as " Laptop " music. Analysis of live coding as a context for programming allows us to escape the implicit assumptions of the commercial office environment in which so much end-user programming has been studied. The programming environments of the future, with increasing deployment of ubiquitous computing technologies, will be unlike offices in many ways. We can prepare for this future by studying extreme varieties of programming today. Live coding is thus an ideal research opportunity for psychology of programming},
author = {Collins, Nick and Blackwell, Allan Alan},
file = {:Users/carthach/Documents/Mendeley Desktop/proglangasmusicinstr.pdf:pdf},
journal = {17th Workshop of the Psychology of Programming Interest Group},
pages = {1--11},
title = {{The Programming Language as a Musical Instrument}},
url = {http://composerprogrammer.com/research/proglangasmusicinstr.pdf},
year = {2005}
}
@article{McLeod2009,
abstract = {Copyright infringement, billboard ‘alteration', an evil secret society known as the Illuminati, country music legend Tammy Wynette, the incineration of {\pounds}1,000,000 in cash, and No Music Day. These odd, interconnected events were engineered by Bill Drummond and Jimmy Cauty, an anarchic British pop duo who used several pseudonyms: The Timelords, The Justified Ancients of Mu Mu, the JAMS, and the KLF. Between 1987 and 1992 they racked up seven U.K. top ten hits, even crossing over in America with the songs ‘3 A.M. Eternal' and ‘Justified and Ancient'—the later of which went to number one in eighteen countries (Simpson, 2003: 199). Those super-cheesy singles are the main reason why this duo is remembered, if they are remembered at all—especially in the U.S.— as a novelty techno-pop act. (The hook ‘KLF is gonna rock ya!', from ‘3 A.M. Eternal', is wired directly into the synapses of millions.)},
author = {McLeod, Kembrew},
file = {:Users/carthach/Documents/Mendeley Desktop/McLeod - 2009 - Crashing the Spectacle a Forgotten History of Digital Sampling, Infringement, Copyright Liberation and the.pdf:pdf},
journal = {Culture Machine},
pages = {114--130},
title = {{Crashing the Spectacle: a Forgotten History of Digital Sampling, Infringement, Copyright Liberation and the}},
url = {http://ir.uiowa.edu/cgi/viewcontent.cgi?article=1009{\&}context=commstud{\_}pubs},
volume = {10},
year = {2009}
}
@misc{Fujishima1999,
abstract = {This paper describes a realtime software system which recognzies musical chords from input sound signals. I designedanalgorithm and implemented it in Common Lisp Music/Realtime environment. The system runs on Silicon Graphics workstations and on the Intel PC platform. Experiments verified that the system succeeded in correctly identifying chords even on orchestral sounds.},
author = {Fujishima, T},
booktitle = {ICMC Proceedings},
file = {:Users/carthach/Documents/Mendeley Desktop/fujishima{\_}1999.pdf:pdf},
issn = {15233782},
number = {6},
pages = {464--467},
pmid = {17999873},
title = {{Realtime Chord Recognition of Musical Sound: A System Using Common Lisp Music}},
volume = {9},
year = {1999}
}
@article{Colannino2005,
abstract = {Let S and T be two finite sets of points on the real line with |S|+|T|=n and |S|{\textgreater}|T|. The restriction scaffold assignment problem in computational biology assigns each point of S to a point of T such that the sum of all the assignment costs is minimized, with the constraint that every element of T must be assigned at least one element of S. The cost of assigning an element si of S to an element tj of T is |si - tj|, i.e., the distance between si and tj. In 2003 Ben-Dor, Karp, Schwikowski and Shamir [J. Comput. Biol. 10 (2) (2003) 385] published an O(nlogn) time algorithm for this problem. Here we provide a counterexample to their algorithm and present a new algorithm that runs in O(n2) time, improving the best previous complexity of O(n3). ?? 2005 Elsevier B.V. All rights reserved.},
author = {Colannino, Justin and Toussaint, Godfried},
doi = {10.1016/j.ipl.2005.05.007},
file = {:Users/carthach/Documents/Mendeley Desktop/Colannino, Toussaint - 2005 - An algorithm for computing the restriction scaffold assignment problem in computational biology.pdf:pdf},
issn = {00200190},
journal = {Information Processing Letters},
keywords = {Analysis of algorithms,Assignment problems,Computational biology,Computational geometry,Information retrieval,Operations research,Rhythmic similarity measures},
pages = {466--471},
title = {{An algorithm for computing the restriction scaffold assignment problem in computational biology}},
volume = {95},
year = {2005}
}
@article{Boden1998,
abstract = {Creativity is a fundamental feature of human intelligence, and a challenge for AI. AI techniques can be used to create new ideas in three ways: by producing novel combinations of familiar ideas; by exploring the potential of conceptual spaces; and by making transformations that enable the generation of previously impossible ideas. AI will have less difficulty in modelling the generation of new ideas than in automating their evaluation.},
author = {Boden, Margaret A.},
doi = {10.1016/S0004-3702(98)00055-1},
file = {:Users/carthach/Documents/Mendeley Desktop/1-s2.0-S0004370298000551-main.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {347--356},
title = {{Creativity and artificial intelligence}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0004370298000551},
volume = {103},
year = {1998}
}
@article{Rocha2013,
abstract = {This report describes the digital humanities project on music similarity. The project is a collaboration between the University of Amsterdam and audio software company Elephantcandy. The project's aim was to investigate timbre and rhythm similarity and to develop an application that finds similar segments of music. In this report three models are described, one for structural segmentation, one for timbre similarity, and one for rhythm similarity of electronic dance music (EDM). The segmentation algorithm performs well on an EDM dataset as well as on a standard MIREX dataset. The timbre similarity algorithm has been tested in a pilot study and preliminary results are presented. Issues related to segmentation and similarity are discussed.},
author = {Rocha, Bruno and Bogaards, Niels and Honingh, Aline},
doi = {10.6084/m9.figshare.1181798},
file = {:Users/carthach/Documents/Mendeley Desktop/PP-2013-10.text.pdf:pdf},
journal = {Proceedings of the Sound and Music Computing Conference (SMC)},
keywords = {25,29,based on similar users,genre and more,meaning that the mu-,or collabo-,predicted,ratings,rative filtering,relevance of a song,tagged by the user,tion about artist,title,to a user is,which can include informa-},
number = {1},
pages = {1--29},
title = {{Segmentation and timbre-and rhythm-similarity in Electronic Dance Music}},
url = {http://dare.uva.nl/record/448463},
year = {2013}
}
@article{Cutler1994,
author = {Cutler, Chris},
journal = {MusicWorks},
title = {{Plunderphonia}},
volume = {60},
year = {1994}
}
@book{nzewi2008musical,
author = {Nzewi, Meki and Anyahuru, Israel and Ohiaraumunna, Tom},
publisher = {Rozenberg Publishers},
title = {{Musical sense and musical meaning: An indigenous African perception}},
year = {2008}
}
@article{Guastavino2009,
author = {Guastavino, Catherine and Marandola, F},
file = {:Users/carthach/Documents/Mendeley Desktop/Guastavino, Marandola - 2009 - Perception of Rhythmic Similarity in Flamenco Music Comparing Musicians and Non-Musicians.pdf:pdf},
pages = {6--9},
title = {{Perception of Rhythmic Similarity in Flamenco Music: Comparing Musicians and Non-Musicians.}},
url = {http://oa.upm.es/4407/},
year = {2009}
}
@inproceedings{Pachet2008a,
address = {Bologna, Italy},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre and Ghedini, Fiammetta},
booktitle = {Marconi Institute for Creativity Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/a9120a1766e10b62d94b35819eead2f1612d.pdf:pdf},
title = {{Creativity through Style Manipulation: the Flow Machines project}},
url = {http://csl.sony.fr/downloads/papers/2013/pachet-13b.pdf},
year = {2008}
}
@inproceedings{Biles1999,
author = {Biles, John A},
booktitle = {Systems, Man, and Cybernetics, 1999. IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on},
organization = {IEEE},
pages = {652--656},
title = {{Life with GenJam: Interacting with a musical IGA}},
volume = {3},
year = {1999}
}
@article{Collins2001,
author = {Collins, Nick},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2001 - Algorithmic composition methods for breakbeat science.pdf:pdf},
journal = {Proceedings of Music Without Walls},
title = {{Algorithmic composition methods for breakbeat science}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://ariada.uea.ac.uk/ariadatexts/ariada3/collins/collins{\_}breakbeat.pdf},
year = {2001}
}
@article{Colton2012,
abstract = {Notions relating to computational systems exhibiting creative behaviours have been explored since the very early days of computer science, and the field of Computational Creativity research has formed in the last dozen years to scientifically explore the potential of such systems. We describe this field via a working definition; a brief history of seminal work; an exploration of the main issues, technologies and ideas; and a look towards future directions. As a society, we are jealous of our creativity: creative people and their contributions to cultural progression are highly valued. Moreover, creative behaviour in people draws on a full set of intelligent abilities, so simulating such behaviour represents a serious technical challenge for Artificial Intelligence research. As such, we believe it is fair to characterise Computational Creativity as a frontier for AI research beyond all others—maybe, even, the final frontier.},
author = {Colton, Simon and Wiggins, Geraint A.},
doi = {10.3233/978-1-61499-098-7-21},
file = {:Users/carthach/Documents/Mendeley Desktop/7488aa3da9e04a08d23531528c3c7864d516.pdf:pdf},
isbn = {9781614990970},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
pages = {21--26},
title = {{Computational creativity: The final frontier?}},
volume = {242},
year = {2012}
}
@inproceedings{miron2017monaural,
author = {Miron, Marius and Janer, Jordi and G{\'{o}}mez, Emilia},
title = {{Monaural score-informed source separation for classical music using convolutional neural networks}}
}
@article{Puckette1997,
abstract = {A new software system, called Pure Data, is in the early stages of development. Its design attempts to remedy some of the deficiencies of the Max program while preserving its strengths. The most important weakness of Max is the difficulty of maintaining compound data structures of the type that might arise when analyzing and resynthesizing sounds or when recording and modifying sequences of events of many different types. Also, it has proved hard to integrate non-audio signals (video, for instance, and also audio spectra) into Max's rigid "tilde object" system. Finally, the whole issue of maintaining two separate copies of all data structures (one to edit and one to access in real time) has caused much confusion and difficulty. Pd's working prototype attempts to simplify the data structures in Max to make them more readily combined into novel user-defined data structures. Also, the relationship between the graphical process and the real-time one (which is handled in one way on the Macintosh and another way on the ISPW) is replaced by yet a third solution.},
author = {Puckette, Miller},
file = {:Users/carthach/Documents/Mendeley Desktop/Puckette - 1997 - Pure Data another integrated computer music environment.pdf:pdf},
journal = {Proceedings, Second Intercollege Computer Music Concerts},
pages = {37--41},
title = {{Pure Data : another integrated computer music environment}},
year = {1997}
}
@inproceedings{Daniel,
author = {Mar{\'{i}}n, Daniel G{\'{o}}mez and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {11th International Symposium on Computer Music Multidisciplinary Research (CMMR) Music, Mind, Embodiment},
file = {:Users/carthach/Documents/Mendeley Desktop/Mar{\'{i}}n, Jord{\`{a}}, Herrera - 2015 - Strictly Rhythm Exploring the effects of identical regions and meter induction in rhythmic similarity.pdf:pdf},
keywords = {edit distance,rhythm,similarity metrics,syncopation},
title = {{Strictly Rhythm : Exploring the effects of identical regions and meter induction in rhythmic similarity perception}},
year = {2015}
}
@article{Bell2010,
author = {Bell, Robert and Bartel, A and Barrass, S},
file = {:Users/carthach/Documents/Mendeley Desktop/Bell, Bartel, Barrass - 2010 - A Systematic Method for Describing and Cataloguing Percussive Sounds, Based on Spectral, Temporal and Per.pdf:pdf},
journal = {International Journal of Arts and Sciences},
keywords = {analysis,description,language,percussion,sound},
number = {9},
pages = {240--251},
title = {{A Systematic Method for Describing and Cataloguing Percussive Sounds, Based on Spectral, Temporal and Perceptual Criteria}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Systematic+Method+for+Describing+and+Cataloguing+Percussive+Sounds+,+Based+on+Spectral+,+Temporal+and+Perceptual+Criteria{\#}0},
volume = {3},
year = {2010}
}
@inproceedings{giantsteps,
author = {Knees, P and Andersen, K and Jord{\`{a}}, S and Hlatky, M and Geiger, G and Gaebele, W and Kaurson, R},
booktitle = {Multimedia Expo Workshops (ICMEW), 2015 IEEE International Conference on},
doi = {10.1109/ICMEW.2015.7169826},
keywords = {Collaboration,Conferences,EU-funded project,GiantSteps project,HCI,Instruments,MIR,Multiple signal classification,Music,Nickel,Production,collaborative interfaces,groupware,human computer interaction,information retrieval,intelligent agents,intelligent interfaces,intelligent musical expert agent development,interactive systems,live performance,low-complexity methods,multi-agent systems,music,music information retrieval,music performance,music production,user interfaces,user-centric design,user-centric research approach},
month = {jun},
pages = {1--4},
title = {{Giantsteps - progress towards developing intelligent and collaborative interfaces for music production and performance}},
year = {2015}
}
@article{Rietveld1995,
author = {Rietveld, Hillegonda},
file = {:Users/carthach/Documents/Mendeley Desktop/Rietveld - 1995 - Pure Bliss - Intertextuality in House Music.pdf:pdf},
journal = {Popular Musicology},
pages = {1--9},
title = {{Pure Bliss - Intertextuality in House Music}},
year = {1995}
}
@book{Toussaint2013,
author = {Toussaint, Godfried T.},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint - 2013 - The Geometry of Musical Rhythm What Makes a good Rhythm Good.pdf:pdf},
isbn = {9781466512030},
publisher = {CRC Press},
title = {{The Geometry of Musical Rhythm: What Makes a " good" Rhythm Good?}},
year = {2013}
}
@article{Bock2012,
author = {B{\"{o}}ck, S and Arzt, A and Krebs, F and Schedl, M},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck et al. - 2012 - Online Real-time Onset Detection With Recurrent Neural Networks.pdf:pdf},
journal = {Proceedings of the 15th {\ldots}},
pages = {15--18},
title = {{Online Real-time Onset Detection With Recurrent Neural Networks}},
url = {http://dafx12.york.ac.uk/papers/dafx12{\_}submission{\_}4.pdf},
year = {2012}
}
@article{Paulus,
author = {Paulus, Jouni},
file = {:Users/carthach/Documents/Mendeley Desktop/Paulus - Unknown - AUDIO-BASED MUSIC STRUCTURE ANALYSIS.pdf:pdf},
title = {{AUDIO-BASED MUSIC STRUCTURE ANALYSIS}}
}
@inproceedings{Vogl2017,
address = {Copenhagen},
author = {Vogl, Richard and Knees, Peter},
booktitle = {New Interfaces for Musical Expression},
file = {:Users/carthach/Documents/Mendeley Desktop/Vogl, Knees - 2017 - An Intelligent Drum Machine for Electronic Dance Music Production and Performance.pdf:pdf},
pages = {251--256},
title = {{An Intelligent Drum Machine for Electronic Dance Music Production and Performance}},
year = {2017}
}
@inproceedings{Sequera2006,
author = {Sequera, Rodrigo Segnini},
booktitle = {International Conference on Music Perception and Cognition},
file = {:Users/carthach/Documents/Mendeley Desktop/Timbrescape{\_}a{\_}Musical{\_}Timbre{\_}and{\_}Structure{\_}Visuali.pdf:pdf},
isbn = {8873951554},
keywords = {music structure,timbre analysis,visualization},
number = {March},
pages = {352--356},
title = {{Timbrescape : a Musical Timbre and Structure Visualization Method Using Tristimulus Data}},
year = {2006}
}
@incollection{Jorda2007a,
author = {Jord{\`{a}}, Sergi (Universitat Pompeu Fabra)},
booktitle = {The Cambridge Companion to Electronic Music},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}} - 2007 - 5 Interactivity and live computer music.pdf:pdf},
isbn = {9780521868617},
title = {{5 Interactivity and live computer music}},
year = {2007}
}
@article{Boden1998,
author = {Boden, Margaret A},
journal = {Artificial Intelligence},
number = {1},
pages = {347--356},
publisher = {Elsevier},
title = {{Creativity and artificial intelligence}},
volume = {103},
year = {1998}
}
@article{Nilsson2001,
author = {Nilsson, Dennis and Goldberger, Jacob},
file = {:Users/carthach/Documents/Mendeley Desktop/Nilsson, Goldberger - 2001 - Sequentially finding the N-best list in hidden Markov models.pdf:pdf},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {2},
pages = {1280--1285},
title = {{Sequentially finding the N-best list in hidden Markov models}},
year = {2001}
}
@inproceedings{Schwarz2003,
author = {Schwarz, Diemo},
booktitle = {Proceedings of the 6th Int. Conference on Digital Audio Effects (DAFx-03)},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz - 2003 - The Caterpillar System for Data-Driven Concateantive Sound Synthesis.pdf:pdf},
pages = {1--6},
title = {{The Caterpillar System for Data-Driven Concateantive Sound Synthesis}},
year = {2003}
}
@incollection{Krey2010,
author = {Krey, Sebastian and Ligges, Uwe},
booktitle = {Classification as a Tool for Research},
pages = {759--766},
publisher = {Springer},
title = {{SVM based instrument and timbre classification}},
year = {2010}
}
@misc{Rabiner1993,
abstract = {Provides a theoretically sound, technically accurate, and complete description of the basic knowledge and ideas that constitute a modern system for speech recognition by machine. Covers production, perception, and acoustic-phonetic characterization of the speech signal; signal processing and analysis methods for speech recognition; pattern comparison techniques; speech recognition system design and implementation; theory and implementation of hidden Markov models; speech recognition based on connected word models; large vocabulary continuous speech recognition; and task- oriented application of automatic speech recognition. For practicing engineers, scientists, linguists, and programmers interested in speech recognition.},
author = {Rabiner, Lawrence and Juang, Biing-Hwang},
booktitle = {Prentice Hall},
doi = {10.1002/ev.1647},
file = {:Users/carthach/Documents/Mendeley Desktop/Rabiner, Juang - 1993 - Fundamentals of Speech Recognition.pdf:pdf},
isbn = {0130151572},
issn = {0164-7989},
pages = {224--225},
title = {{Fundamentals of Speech Recognition}},
volume = {103},
year = {1993}
}
@misc{Ellis2005,
annote = {online web resource},
author = {Ellis, Daniel P W},
title = {{PLP and RASTA (and MFCC, and inversion) in Matlab}},
url = {http://www.ee.columbia.edu/{~}dpwe/resources/matlab/rastamat/},
urldate = {2017-11-15},
year = {2005}
}
@article{Stober2012a,
abstract = {Music Information Retrieval (MIR) systems have to deal with multi- faceted music information and very heterogeneous users. Especially when the task is to organize a music collection, the diverse perspec- tives of users caused by their different level of expertise, musical back- ground or taste pose a great challenge. This challenge is addressed here by proposing adaptive methods for several elements of MIR sys- tems: Data-adaptive feature extraction techniques are described that aim to increase the quality and robustness of the information extracted from audio recordings. The classical genre classification problem is approached from a novel user-centric perspective – promoting the idea of idiosyncratic genres that better reflect a user's personal lis- tening habits. An adaptive visualization technique for exploration and organization of music collections is elaborated that especially addresses the common and inevitable problem of projection errors introduced by dimensionality reduction approaches. Furthermore, it is outlined how this technique can be applied to facilitate serendipitous music discoveries in a recommendation scenario and to enable novel gaze-supported interaction techniques. Finally, a general approach for adaptive music similarity is presented which serves as the core of many adaptive MIR applications. Application prototypes demonstrate the usability of the described approaches.},
author = {Stober, Sebastian},
file = {:Users/carthach/Documents/Mendeley Desktop/stober2011thesis.pdf:pdf},
isbn = {978-3-8439-0229-8},
pages = {242},
title = {{Adaptive methods for user-centered organization of music collections}},
year = {2012}
}
@article{Davies2005,
author = {Davies, M E P and Plumbley, M D},
doi = {10.1109/ICASSP.2005.1415691},
file = {:Users/carthach/Documents/Mendeley Desktop/Davies, Plumbley - 2005 - Beat tracking with a two state model.pdf:pdf},
issn = {15206149},
journal = {Proceedings of the IEEE International Conference on Acoustics Speech and Signal Processing ICASSP},
pages = {241--244},
title = {{Beat tracking with a two state model}},
url = {http://www.elec.qmul.ac.uk/people/markp/2005/DaviesPlumbley05-icassp.pdf},
volume = {3},
year = {2005}
}
@book{Roads2004,
abstract = {Below the level of the musical note lies the realm of microsound, of sound particles lasting less than one-tenth of a second. Recent technological advances allow us to probe and manipulate these pinpoints of sound, dissolving the traditional building blocks of music -- notes and their intervals -- into a more fluid and supple medium. The sensations of point, pulse (series of points), line (tone), and surface (texture) emerge as particle density increases. Sounds coalesce, evaporate, and mutate into other sounds. Composers have used theories of microsound in computer music since the 1950s. Distinguished practitioners include Karlheinz Stockhausen and Iannis Xenakis. Today, with the increased interest in computer and electronic music, many young composers and software synthesis developers are exploring its advantages. Covering all aspects of composition with sound particles, Microsound offers composition theory, historical accounts, technical overviews, acoustical experiments, descriptions of musical works, and aesthetic reflections. The book is accompanied by an audio CD of examples.},
author = {Roads, Curtis},
booktitle = {The MIT Press},
keywords = {book,granular-synthesis,sound-synthesis,tkk-sahko-wishlist},
title = {{Microsound}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\%}5C{\&}path=ASIN/0262681544},
year = {2004}
}
@phdthesis{Jose2017,
author = {Bosch, Juan Jos{\'{e}}},
file = {:Users/carthach/Documents/Mendeley Desktop/Jos{\'{e}}, Vicente - 2017 - From Heuristics-Based to Data-Driven Audio Melody Extraction.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{From Heuristics-Based to Data-Driven Audio Melody Extraction}},
year = {2017}
}
@article{Lee2003,
author = {Lee, Minkyu and Lopresti, Daniel P. and Olive, Joseph P.},
doi = {10.1023/A:1025752731945},
file = {:Users/carthach/Documents/Mendeley Desktop/Lee, Lopresti, Olive - 2003 - A Text-to-Speech Platform for Variable Length Optimal Unit Searching Using Perception Based Cost Functions.pdf:pdf},
issn = {13812416},
journal = {International Journal of Speech Technology},
keywords = {Text-to-speech,Unit selection,Viterbi searching},
number = {4},
pages = {347--356},
title = {{A Text-to-Speech Platform for Variable Length Optimal Unit Searching Using Perception Based Cost Functions}},
volume = {6},
year = {2003}
}
@article{Bullock2011,
abstract = {In this paper we describe a new application, Integra Live, designed to address the problems associated with software usability in live electronic music. We begin by outlining the primary usability and user-experience issues relating to the predominance of graphical dataflow languages for the composition and performance of live electronics. We then discuss the specific development methodologies chosen to address these issues, and illustrate how adopting a usercentred approach has resulted in a more usable and humane interface design. The main components and workflows of the user interface are discussed, giving a rationale for key design decisions. User testing processes and results are presented. Finally, a critical evaluation application usability is given based on user-testing processes, with key findings presented for future consideration.},
author = {Bullock, Jamie and Beattie, Daniel and Turner, Jerome},
file = {:Users/carthach/Documents/Mendeley Desktop/nime2011{\_}387.pdf:pdf},
issn = {2220-4806},
journal = {Proceedings of The International Conference on New Interfaces For Musical Expression (NIME)},
keywords = {live electronics,software,usability,user experience; NIME 2011},
number = {June},
pages = {387--392},
title = {{Integra Live: a new graphical user interface for live electronic music}},
url = {http://www.nime2011.org/proceedings/papers/K05-Bullock.pdf},
year = {2011}
}
@article{Orio2006,
abstract = {The increasing availability of music in digital format needs to be matched by the development of tools for music accessing, filtering, classification, and retrieval. The research area of Music Information Retrieval (MIR) covers many of these aspects. The aim of this paper is to present an overview of this vast and new field. A number of issues, which are peculiar to the music language, are describedincluding forms, formats, and dimensions of musictogether with the typologies of users and their information needs. To fulfil these needs a number of approaches are discussed, from direct search to information filtering and clustering of music documents. An overview of the techniques for music processing, which are commonly exploited in many approaches, is also presented. Evaluation and comparisons of the approaches on a common benchmark are other important issues. To this end, a description of the initial efforts and evaluation campaigns for MIR is provided.},
author = {Orio, Nicola},
doi = {10.1561/1500000002},
file = {:Users/carthach/Documents/Mendeley Desktop/1500000002.pdf:pdf},
isbn = {1933019395},
issn = {1554-0669},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
number = {1},
pages = {1--96},
title = {{Music Retrieval: A Tutorial and Review}},
url = {http://www.nowpublishers.com/article/Details/INR-002},
volume = {1},
year = {2006}
}
@inproceedings{Gill2004,
address = {Montreal, Canada},
author = {Gillet, Olivier and Richard, Ga{\"{e}}l},
booktitle = {IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
file = {:Users/carthach/Documents/Mendeley Desktop/01326815.pdf:pdf},
pages = {2--5},
title = {{Automatic Transcription of Drum Loops}},
year = {2004}
}
@article{Bown2015,
abstract = {In this paper we draw on previous research in musical meta- creation (MuMe) to propose that novel creative forms are needed to propel innovation in autonomous creative musical agents. We propose the “musebot”, and the “musebot ensemble”, as one such novel form that we argue will provide new opportunities for artis- tic practitioners working in the MuMe field to better collaborate, evaluate work, and make meaningful contributions both creative- ly and technically. We give details of our specification and de- signs for the musebot ensemble project.},
author = {Bown, Oliver and Carey, Benjamin and Eigenfeldt, Arne},
file = {:Users/carthach/Documents/Mendeley Desktop/ISEA2015{\_}submission{\_}141 (1).pdf:pdf},
isbn = {978-1-910172-00-1},
issn = {2451-8611},
journal = {Proceedings of the 21st Interntaional Symposium on Electronic Art},
keywords = {autonomous agents,computer,generative music,live algorithms,music,musical metacreation,performance},
number = {August},
title = {{Manifesto for a Musebot Ensemble: A platform for live interactive performance between multiple autonomous musical agents}},
year = {2015}
}
@article{Frisson2014,
author = {Frisson, Christian and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
file = {:Users/carthach/Documents/Mendeley Desktop/Frisson et al. - 2014 - a Proximity Grid Optimization Method To Improve Audio Search for Sound Design.pdf:pdf},
journal = {15th International Society for Music Information Retrieval Conference ( ISMIR 2014 )},
number = {Ismir},
pages = {349--354},
title = {{a Proximity Grid Optimization Method To Improve Audio Search for Sound Design}},
year = {2014}
}
@article{Degara2012,
abstract = {A new probabilistic framework for beat tracking of musical audio is presented. The method estimates the time between consecutive beat events and exploits both beat and non-beat information by explicitly modeling non-beat states. In addition to the beat times, a measure of the expected accuracy of the estimated beats is provided. The quality of the observations used for beat tracking is measured and the reliability of the beats is automatically calculated. A k -nearest neighbor regression algorithm is proposed to predict the accuracy of the beat estimates. The performance of the beat tracking system is statistically evaluated using a database of 222 musical signals of various genres. We show that modeling non-beat states leads to a significant increase in performance. In addition, a large experiment where the parameters of the model are automatically learned has been completed. Results show that simple approximations for the parameters of the model can be used. Furthermore, the performance of the system is compared with existing algorithms. Finally, a new perspective for beat tracking evaluation is presented. We show how reliability information can be successfully used to increase the mean performance of the proposed algorithm and discuss how far automatic beat tracking is from human tapping.},
author = {Degara, Norberto and Rua, Enrique Argones and Pena, Antonio and Torres-Guijarro, Soledad and Davies, Matthew E P and Plumbley, Mark D.},
doi = {10.1109/TASL.2011.2160854},
file = {:Users/carthach/Documents/Mendeley Desktop/Degara et al. - 2012 - Reliability-informed beat tracking of musical signals.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Beat-tracking,beat quality,beat-tracking reliability,k-nearest neighbor (k-NN) regression,music signal processing},
number = {1},
pages = {278--289},
title = {{Reliability-informed beat tracking of musical signals}},
volume = {20},
year = {2012}
}
@article{Smith2002a,
abstract = {This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook Elementary Linear Algebra 5e by Howard Anton, Publisher JohnWiley {\&} Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground. 1},
archivePrefix = {arXiv},
arxivId = {1511.06448},
author = {Smith, Lindsay I},
doi = {10.1080/03610928808829796},
eprint = {1511.06448},
file = {:Users/carthach/Documents/Mendeley Desktop/principal{\_}components.pdf:pdf},
isbn = {0471852236},
issn = {03610926},
journal = {Statistics},
keywords = {analysis,utorial on principal components},
pages = {52},
pmid = {16765218},
title = {{A tutorial on Principal Components Analysis Introduction}},
url = {http://www.mendeley.com/research/computational-genome-analysis-an-introduction-statistics-for-biology-and-health/},
volume = {51},
year = {2002}
}
@article{garshol2003bnf,
author = {Garshol, Lars Marius},
journal = {acedida pela {\'{u}}ltima vez em},
title = {{BNF and EBNF: What are they and how do they work}},
volume = {16},
year = {2003}
}
@article{Eigenfeldt2010,
author = {Eigenfeldt, Arne},
doi = {10.1145/1873951.1874292},
file = {:Users/carthach/Documents/Mendeley Desktop/0299129feca9db62fc626b0b65a029515773.pdf:pdf},
isbn = {9781605589336},
journal = {Mm},
keywords = {emergent art,generative music,multi-agents},
pages = {1583--1586},
title = {{Coming Together – Negotiated Content by Multi-agents}},
year = {2010}
}
@article{Jacob1996,
abstract = {There are two distinct types of creativity: the flash out of the blue (inspiration? genius?), and the process of incremental revisions (hard work). Not only are we years away from modelling the former, we do not even begin to understand it. The latter is algorithmic in nature and has been modelled in many systems both musical and non{\&}hyphen;musical. Algorithmic composition is as old as music composition. It is often considered a cheat, a way out when the composer needs material and/or inspiration. It can also be thought of as a compositional tool that simply makes the composer's work go faster. This article makes a case for algorithmic composition as such a tool. The ‘hard work' type of creativity often involves trying many different combinations and choosing one over the others. It seems natural to express this iterative task as a computer algorithm. The implementation issues can be reduced to two components: how to understand one's own creative process well enough to reproduce it as an algorithm, and how to program a computer to differentiate between ‘good' and ‘bad' music. The philosophical issues reduce to the question who or what is responsible for the music produced?},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jacob, Bruce L},
doi = {10.1017/S1355771896000222},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/OrganizedSound.pdf:pdf},
isbn = {9788578110796},
issn = {13557718},
journal = {Organised Sound},
number = {3},
pages = {S1355771896000222},
pmid = {25246403},
title = {{Algorithmic composition as a model of creativity}},
url = {http://www.journals.cambridge.org/abstract{\_}S1355771896000222},
volume = {1},
year = {1996}
}
@article{Fitz2004,
author = {Fitzgerald, Derry},
journal = {Dublin Institute of Technology},
pages = {3},
title = {{Automatic drum transcription and source separation}},
year = {2004}
}
@article{Collins2006,
author = {Collins, Nick},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2006 - Towards a style-specific basis for computational beat tracking.pdf:pdf},
keywords = {beat tracking,metre perception,re-synchronisation},
title = {{Towards a style-specific basis for computational beat tracking}},
url = {http://sro.sussex.ac.uk/1287/},
year = {2006}
}
@article{Brent2009,
author = {Brent, William},
file = {:Users/carthach/Documents/Mendeley Desktop/Brent - 2009 - Cepstral analysis tools for percussive timbre identification.pdf:pdf},
journal = {Proceedings of the 3rd International Pure Data Convention},
keywords = {cepstrum mfcc barks timbre},
title = {{Cepstral analysis tools for percussive timbre identification}},
url = {http://www.williambrent.conflations.com/papers/features.pdf},
year = {2009}
}
@misc{Bradshaw1973,
abstract = {1. Free Stochastic Music 2. Markovian Stochastic Music---Theory 3. Markovian Stochastic Music---Applications 4. Musical Strategy 5. Free Stochastic Music by Computer 6. Symbolic Music 7. Towards a Metamusic 8. Towards a Philosophy of Music 9. New Proposals in Microsound Structure},
author = {Bradshaw, Merrill and Xenakis, Iannis},
booktitle = {Music Educators Journal},
doi = {10.2307/3394288},
isbn = {0918728061},
issn = {00274321},
number = {8},
pages = {85},
pmid = {4835550},
title = {{Formalized Music: Thought and Mathematics in Composition}},
volume = {59},
year = {1973}
}
@article{Thul2008a,
author = {Thul, Eric and Toussaint, GT},
file = {:Users/carthach/Documents/Mendeley Desktop/Thul, Toussaint - 2008 - Rhythm Complexity Measures A Comparison of Mathematical Models of Human Perception and Performance.pdf:pdf},
journal = {ISMIR},
pages = {663--668},
title = {{Rhythm Complexity Measures: A Comparison of Mathematical Models of Human Perception and Performance.}},
url = {http://www.mirlab.org/conference{\_}papers/International{\_}Conference/ISMIR 2008/papers/ISMIR2008{\_}125.pdf},
year = {2008}
}
@book{Enemy2014,
author = {Enemy, Public},
publisher = {Def Jam Recordings},
title = {{It takes a nation of millions to hold us back}},
year = {2014}
}
@article{Manaris2007,
author = {Manaris, Bill and Roos, Patrick and Machado, Penousal},
file = {:Users/carthach/Documents/Mendeley Desktop/Manaris, Roos, Machado - 2007 - A corpus-based hybrid approach to music analysis and composition.pdf:pdf},
journal = {Proceedings of Twenty-Second Conference on Artificial Intelligence},
title = {{A corpus-based hybrid approach to music analysis and composition}},
url = {http://www.aaai.org/Papers/AAAI/2007/AAAI07-133.pdf},
year = {2007}
}
@inproceedings{McCartney1996,
abstract = {SuperCollider is an environment for real time audio synthesis which runs on a Power Macintosh with no additional hardware. SuperCollider features: a built in programming language with real time incremental garbage collection, first class functions/closures, a small object oriented class system, a mini GUI builder for creating a patch control panel, a graphical interface for creating wave tables and breakpoint envelopes, MIDI control, a large library of signal processing and synthesis functions, and a large library of functions for list processing of musical data. The user can write both the synthesis and compositional algorithms for their piece in the same high level language. This allows the creation of synthesis instruments with considerably more flexibility than allowed in lower level synthesis languages. Since it is easy to create control panels and graphic displays, SuperCollider is well suited as a tool for teaching various synthesis techniques.},
author = {McCartney, James},
booktitle = {Proceedings of the 1996 International Computer Music Conference},
pages = {257--258},
title = {{SuperCollider: a new real time synthesis language}},
url = {http://www.audiosynth.com/icmc96paper.html},
year = {1996}
}
@misc{Murray,
author = {Murray, Adam},
title = {{Musical Grammar Evolution In Max Via Ruby}},
url = {http://compusition.com/news/2008/4/23}
}
@inproceedings{Nuanain2017,
address = {Porto, Portugal},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {Proceedings of the 13th International Symposium on Computer Music Multidisciplinary Research},
file = {:Users/carthach/Documents/Mendeley Desktop/Nuan{\'{a}}in, Jord{\`{a}}, Herrera - 2017 - k -Best Hidden Markov Model Decoding for Unit Selection in Concatenative Sound Synthesis.pdf:pdf},
keywords = {artificial,concatenative synthesis,hidden markov models,intelligence,musical signal processing},
title = {{k -Best Hidden Markov Model Decoding for Unit Selection in Concatenative Sound Synthesis}},
year = {2017}
}
@book{Lerch2012,
abstract = {With the proliferation of digital audio distribution over digital media, audio content analysis is fast becoming a requirement for designers of intelligent signal-adaptive audio processing systems. Written by a well-known expert in the field, this book provides quick access to different analysis algorithms and allows comparison between different approaches to the same task, making it useful for newcomers to audio signal processing and industry experts alike. A review of relevant fundamentals in audio signal processing, psychoacoustics, and music theory, as well as downloadable MATLAB files are also included. Please visit the companion website: www.AudioContentAnalysis.org},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lerch, Alexander},
booktitle = {John Wiley {\&} Sons},
doi = {10.1002/9781118393550},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Lerch - 2012 - An introduction to audio content analysis Applications in signal processing and music informatics.pdf:pdf},
isbn = {9781118393550},
issn = {1098-6596},
pages = {1--248},
pmid = {25246403},
title = {{An introduction to audio content analysis: Applications in signal processing and music informatics.}},
url = {http://doi.wiley.com/10.1002/9781118393550},
year = {2012}
}
@article{Eigenfeldt2006a,
abstract = {Kinetic Engine is an initial attempt to create an intelligent improvising instrument. An interactive performance system that models a drum ensemble, it is comprised of four multi-agents (players) that collaborate to create complex rhythms. Each agent assumes a role, and makes decisions according to this understanding. Furthermore, the four players are under the coordination of a software conductor that operates as the “eyes and ears” of the ensemble.},
author = {Eigenfeldt, Arne},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt - 2006 - Kinetic Engine Toward an Intelligent Improvising Instrument.pdf:pdf},
journal = {Proceedings of the Sound and Music Computing Conference},
pages = {97--100},
title = {{Kinetic Engine: Toward an Intelligent Improvising Instrument}},
year = {2006}
}
@article{Ravelli2007,
abstract = {We propose a novel system able to modify the rhythm of a given drum loop, known as the original, to match the rhythmic pattern of a second loop, known as the model. Our approach is fully automated, thus eliminating the need for MIDI sequencing. The presented methodology combines standard and state-of-the-art techniques for the segmentation and classification of drum sounds, the matching of drum sequences, and the transformation of the original loop. We discuss the advantages and disadvantages of the proposed approach and provide links to examples for the qualitative evaluation of the system's output},
author = {Ravelli, Emmanuel and Bello, Juan P. and Sandler, Mark},
doi = {10.1109/LSP.2006.887783},
file = {:Users/carthach/Documents/Mendeley Desktop/Ravelli, Bello, Sandler - 2007 - Automatic rhythm modification of drum loops.pdf:pdf},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Audio effects,Automatic drum transcription,Content-based transformation,Rhythm patterns,Sequence alignment},
number = {4},
pages = {228--231},
title = {{Automatic rhythm modification of drum loops}},
volume = {14},
year = {2007}
}
@article{Hockman2012,
author = {Hockman, J and Davies, MEP and Fujinaga, Ichiro},
file = {:Users/carthach/Documents/Mendeley Desktop/Hockman, Davies, Fujinaga - 2012 - One in the Jungle Downbeat Detection in Hardcore, Jungle, and Drum and Bass.pdf:pdf},
journal = {ISMIR},
number = {Ismir},
pages = {169--174},
title = {{One in the Jungle: Downbeat Detection in Hardcore, Jungle, and Drum and Bass.}},
url = {http://telecom.inescporto.pt/{~}mdavies/pdfs/HockmanDaviesFujinaga12-ismir.pdf},
year = {2012}
}
@article{Peeters2004,
author = {Peeters, Geoffroy},
file = {:Users/carthach/Documents/Mendeley Desktop/Peeters - 2004 - Deriving musical structures from signal analysis for music audio summary generation sequence and state approach.pdf:pdf},
journal = {Lecture Notes in Computer Science},
title = {{Deriving musical structures from signal analysis for music audio summary generation:" sequence" and" state" approach}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-540-39900-1{\_}14.pdf},
year = {2004}
}
@article{Fox2005a,
abstract = {This paper describes the design of SoniMime, a system for the sonification of hand movement for real-time timbre shaping. We explore the application of the tristimulus timbre model for the sonification of gestural data, working toward the goals of musical expressivity and physical responsiveness. SoniMime uses two 3-D accelerometers connected to an Atmel microprocessor which outputs OSC control messages. Data filtering, parameter mapping, and sound synthesis take place in Pd running on a Linux computer.},
author = {Fox, Jesse and Carlile, Jennifer},
file = {:Users/carthach/Documents/Mendeley Desktop/nime2005{\_}242.pdf:pdf},
journal = {Proceedings of the 5th International Conference on New Interfaces for Musical Expression (NIME05)},
keywords = {Human computer interaction,Musical controller,Sonification},
pages = {242--243},
title = {{SoniMime: movement sonification for real-time timbre shaping}},
year = {2005}
}
@misc{ONuanain,
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach},
title = {{Fresh Intelligence - A Study in Realtime Composition with the ReacTable Tangible User Interface}},
url = {http://vimeo.com/7392845},
year = {2009}
}
@article{Stark2009,
abstract = {In this paper we present a model for beat-synchronous anal- ysis of musical audio signals. Introducing a real-time beat track- ing model with performance comparable to offline techniques, we discuss its application to the analysis of musical performances segmented by beat. We discuss the various design choices for beat-synchronous analysis and their implications for real-time im- plementations before presenting some beat-synchronous harmonic analysis examples. We make available our beat tracker and beat- synchronous analysis techniques as externals for Max/MSP.},
author = {Stark, Adam M and Davies, Matthew E P and Plumbley, Mark D},
file = {:Users/carthach/Documents/Mendeley Desktop/StarkDaviesPlumbley09-dafx.pdf:pdf},
journal = {Proc. of the 12th Int. Conference on Digital Audio Effects},
pages = {1--6},
title = {{Real-Time Beat-Synchronous Analysis of Musical Audio}},
year = {2009}
}
@article{Kumar2004,
author = {Kumar, R.},
file = {:Users/carthach/Documents/Mendeley Desktop/Kumar - 2004 - A Genetic Algorithm for Unit Selection based Speech Synthesis.pdf:pdf},
journal = {Eighth International Conference on Spoken Language},
number = {October},
pages = {1233----1236},
title = {{A Genetic Algorithm for Unit Selection based Speech Synthesis}},
url = {http://www.isca-speech.org/archive/interspeech{\_}2004/i04{\_}1233.html},
year = {2004}
}
@article{Agostini2003,
abstract = {A set of features is evaluated for musical instrument recognition$\backslash$nout of monophonic musical signals. Aiming to achieve a compact$\backslash$nrepresentation, the adopted features regard only spectral$\backslash$ncharacteristics of sound and are limited in number. On top of these$\backslash$ndescriptors, various classification methods are implemented and tested.$\backslash$nOver a dataset of 1007 tones from 27 musical instruments and without$\backslash$nemploying any hierarchical structure, quadratic discriminant analysis$\backslash$nshows the lowest error rate (7.19{\%} for the individual instrument and$\backslash$n3.13{\%} for instrument families), outperforming all the other$\backslash$nclassification methods (canonical discriminant analysis, support vector$\backslash$nmachines, nearest neighbours). The most relevant features are$\backslash$ndemonstrated to be the inharmonicity, the spectral centroid and the$\backslash$nenergy contained in the first partial},
author = {Agostini, Giulio and Longari, Maurizio and Pollastri, Emanuele},
doi = {10.1155/S1110865703210118},
file = {:Users/carthach/Documents/Mendeley Desktop/p5-agostini.pdf:pdf},
isbn = {0-7803-7025-2},
issn = {11108657},
journal = {Eurasip Journal on Applied Signal Processing},
keywords = {Audio features extraction,Content-based audio indexing/searching,Pattern recognition,Timbre classification},
number = {1},
pages = {5--14},
title = {{Musical instrument timbres classification with spectral features}},
volume = {2003},
year = {2003}
}
@article{keith1991polychords,
author = {Keith, Michael},
publisher = {Vinculum Press},
title = {{From polychords to polya: adventures in musical combinatorics}},
year = {1991}
}
@article{Fletcher1933,
author = {Fletcher, Harvey and Munson, W A},
doi = {10.1121/1.1915637},
file = {:Users/carthach/Documents/Mendeley Desktop/1.1915637.pdf:pdf},
issn = {00014966},
journal = {Journal of the Acoustical Society of America (JASA)},
number = {2},
pages = {82},
title = {{Loudness, Its Definition, Measurement and Calculation}},
url = {http://link.aip.org/link/JASMAN/v5/i2/p82/s1{\&}Agg=doi},
volume = {5},
year = {1933}
}
@article{Romero2008,
abstract = {The article reviews the book {\{}"The{\}} Art of Artificial Evolution: A Handbook on Evolutionary Art and Music," edited by Juan Romero and Penousal Machado.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Romero, Juan J and Machado, Penousal and Ashlock, Dan},
doi = {10.1080/17513470802310721},
eprint = {arXiv:1011.1669v3},
isbn = {9783540728764},
issn = {1751-3472},
journal = {Journal of Mathematics and the Arts},
pages = {103--106},
pmid = {23879482},
title = {{The Art of Artificial Evolution: A Handbook on Evolutionary Art and Music}},
volume = {2},
year = {2008}
}
@article{Young2002,
author = {Young, Steve and Evermann, Gunnar and Gales, Mark and Hain, Thomas and Kershaw, Dan and Liu, Xunying and Moore, Gareth and Odell, Julian and Ollason, Dave and Povey, Dan and Others},
journal = {Cambridge University},
pages = {175},
title = {{The HTK book}},
volume = {3},
year = {2002}
}
@book{Cope1996,
author = {Cope, David and Mayer, Melanie J},
publisher = {AR editions Madison, WI},
title = {{Experiments in musical intelligence}},
volume = {12},
year = {1996}
}
@inproceedings{tzan2000,
address = {Verona},
author = {Tzanetakis, George and Cook, Perry},
booktitle = {Proceedings of the COST G-6 Conference on Digital Audio Effects (DAFX-00)},
file = {:Users/carthach/Documents/Mendeley Desktop/dafx00gtzan.pdf:pdf},
title = {{3D Graphics Tools for Sound Collections}},
year = {2000}
}
@inproceedings{Roads1991,
author = {Roads, Curtis},
booktitle = {Representations of musical signals},
organization = {MIT Press},
pages = {143--186},
title = {{Asynchronous granular synthesis}},
year = {1991}
}
@article{Orio2003,
author = {Orio, Nicola and Lemouton, Serge and Schwarz, D},
file = {:Users/carthach/Documents/Mendeley Desktop/Orio, Lemouton, Schwarz - 2003 - Score following State of the art and new developments.pdf:pdf},
journal = {Proceedings of the Conference on New Interfaces for Musical Expression},
keywords = {real time audio alignment,score following,score recognition,virtual accompaniment},
pages = {36--41},
title = {{Score following: State of the art and new developments}},
url = {http://dl.acm.org/citation.cfm?id=1085724},
year = {2003}
}
@article{Reynolds1987,
author = {Reynolds, Craig W},
journal = {ACM SIGGRAPH computer graphics},
number = {4},
pages = {25--34},
publisher = {ACM},
title = {{Flocks, herds and schools: A distributed behavioral model}},
volume = {21},
year = {1987}
}
@article{Duignan2010,
abstract = {An abstract is not available.},
author = {Duignan, Matthew and Noble, James and Biddle, Robert},
doi = {10.1162/COMJ_a_00023},
file = {:Users/carthach/Documents/Mendeley Desktop/06792890.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {4},
pages = {22--33},
title = {{Abstraction and Activity in Computer-Mediated Music Production}},
volume = {34},
year = {2010}
}
@incollection{Pampalk2005,
abstract = {As digital music collections grow, so does the need to organizing them automatically. In this paper we present an approach to hierarchically organize music collections at the artist level. Artists are grouped according to similarity which is computed using a web search engine and standard text retrieval techniques. The groups are described by words found on the webpages using term selection techniques and domain knowledge. We compare different term selection techniques, present a simple demonstration, and discuss our findings.},
address = {Berlin, Heidelberg},
author = {Pampalk, Elias and Flexer, Arthur and Widmer, Gerhard},
booktitle = {Research and Advanced Technology for Digital Libraries: 9th European Conference, ECDL 2005, Vienna, Austria, September 18-23, 2005. Proceedings},
doi = {10.1007/11551362_4},
editor = {Rauber, Andreas and Christodoulakis, Stavros and Tjoa, A Min},
isbn = {978-3-540-31931-3},
pages = {37--48},
publisher = {Springer Berlin Heidelberg},
title = {{Hierarchical Organization and Description of Music Collections at the Artist Level}},
url = {https://doi.org/10.1007/11551362{\_}4},
year = {2005}
}
@article{Ellis2007,
author = {Ellis, DPW},
journal = {{\ldots} International Conference on Music {\ldots}},
title = {{Classifying music audio with timbral and chroma features}},
url = {http://academiccommons.columbia.edu/catalog/ac:148541},
year = {2007}
}
@article{Gomez2004,
abstract = {In this paper we evaluate two methods for key estimation from polyphonic audio recordings. Our goal is to compare between a strategy using a cognition-inspired model and several machine learning techniques to find a model for tonality (mode and key note) determination of polyphonic music from audio files. Both approaches have as an input a vector of values related to the intensity of each of the pitch classes of a chromatic scale. In this study, both methods are explained and evaluated in a large database of audio recordings of classical pieces. one is based on machine learning algorithms trained on a database of labelled pieces. After a description of both approaches, we evaluate them, present the results and discuss some of our findings. 2. SYSTEM BLOCK DIAGRAM The overall system block diagram is presented in Figure 1. In order to estimate the key from polyphonic recordings, we first extract a set of low-level features from the audio signal. These features are then compared to a model of tonality in order to estimate the key of the piece.},
author = {G{\'{o}}mez, E and Herrera, P},
file = {:Users/carthach/Documents/Mendeley Desktop/54b90082c23ff2a3725bb05ccc4dc3fdd5cc (1).pdf:pdf},
isbn = {84-88042-44-2},
journal = {Ismir},
pages = {1--4},
title = {{Estimating The Tonality Of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.}},
url = {http://www.dtic.upf.edu/{~}egomez/TonalDescription/GomezHerrera-ISMIR2004.pdf},
year = {2004}
}
@inproceedings{Tokui2008,
author = {Tokui, Nao},
booktitle = {Proceedings of the 3rd international conference on Digital Interactive Media in Entertainment and Arts},
organization = {ACM},
pages = {526--527},
title = {{Massh!: a web-based collective music mashup system}},
year = {2008}
}
@article{Risset1999,
abstract = {The aim of the psychology of music is to understand musical phenomena in terms of mental functions--to characterize the ways in which one perceives, remembers, creates, and performs music. Since the First Edition of The Psychology of Music was published the field has emerged from an interdisciplinary curiosity into a fully ramified subdiscipline of psychology due to several factors. The opportunity to generate, analyze, and transform sounds by computer is no longer limited to a few researchers with access to large multi-user facilities, but rather is available to individual investigators on a widespread basis. Second, dramatic advances in the field of neuroscience have profoundly influenced thinking about the way that music is processed in the brain. Third, collaborations between psychologists and musicians, which were evolving at the time the First Edition was written, are now quite common; to a large extent now speaking a common language and agreeing on basic philosophical issues.The Psychology of Music, Second Edition has been completely revised to bring the reader the most up-to-date information, additional subject matter, and new contributors to incorporate all of these important variables.},
author = {Risset, Jean-Claude and Wessel, David L},
doi = {10.1016/B978-012213564-4/50006-8},
file = {:Users/carthach/Documents/Mendeley Desktop/113-169.pdf:pdf},
isbn = {0122135652},
journal = {The Psychology of Music},
number = {Second Edition},
pages = {113--169},
title = {{Exploration of Timbre by Analysis and Synthesis}},
year = {1999}
}
@article{Charulatha2013,
author = {Charulatha, BS and Rodrigues, P and Chitralekha, T},
file = {:Users/carthach/Documents/Mendeley Desktop/Charulatha, Rodrigues, Chitralekha - 2013 - A Comparative study of different distance metrics that can be used in Fuzzy Clustering Algor.pdf:pdf},
isbn = {9789380609140},
journal = {Ijettcs.Org},
keywords = {canberra,clustering,cosine,euclidean,fuzzy c means,fuzzy clustering,in order to represent,manhattan,similarity between two things,tchebychev,there are many ways,to determine the},
title = {{A Comparative study of different distance metrics that can be used in Fuzzy Clustering Algorithms}},
url = {http://www.ijettcs.org/NCASG-2013/NCASG 38.pdf},
volume = {2013},
year = {2013}
}
@inproceedings{Panteli2014a,
abstract = {A model for rhythm similarity in electronic dance music (EDM) is presented in this paper. Rhythm in EDM is built on the concept of a ‘loop', a repeating sequence typically associated with a four-measure percussive pattern. The presented model calculates rhythm similarity between seg- ments of EDM in the following steps. 1) Each segment is split in different perceptual rhythmic streams. 2) Each stream is characterized by a number of attributes, most no- tably: attack phase of onsets, periodicity of rhythmic el- ements, and metrical distribution. 3) These attributes are combined into one feature vector for every segment, af- ter which the similarity between segments can be calcu- lated. The stages of stream splitting, onset detection and downbeat detection have been evaluated individually, and a listening experiment was conducted to evaluate the over- all performance of the model with perceptual ratings of rhythm similarity.},
address = {Taipei, Taiwan},
author = {Panteli, Maria and Bogaards, N and Honingh, A},
booktitle = {15th International Society for Music Information Retrieval Conference (ISMIR 2014) MODELING},
file = {:Users/carthach/Documents/Mendeley Desktop/ismir{\_}2014.pdf:pdf},
isbn = {9781632662842},
number = {Ismir},
pages = {0--5},
title = {{Modeling Rhythm Similarity for Electronic Dance Music}},
url = {https://staff.fnwi.uva.nl/a.k.honingh/publicaties/Pantelietal{\_}ISMIRSubmission{\_}July18th.pdf},
year = {2014}
}
@article{Bown2013,
abstract = {This paper discusses the approach to curation of the MuMe Weekend (held in Sydney, Australia, 2013) and the experience the authors gained from hosting the event. We identify open challenges arising from this applied demonstration of musically metacreative systems. Identified challenges are not technical, but rather methodological, concerning pragmatic aspects of presenting and collaboratively innovating musically metacreative work. The paper presents our approach to curating the event, the categories of performances offered – and responses to these – issues to do with the selection and presentation of the work, and issues to do with the evaluation by audience and performers.},
author = {Bown, Oliver and Eigenfeldt, Arne and Martin, Aengus},
file = {:Users/carthach/Documents/Mendeley Desktop/7444-32757-1-PB.pdf:pdf},
isbn = {9781577356370},
journal = {Proceedings of the Artificial Intelligence and Interactive Digital Entertainment (AIIDE'13) Conference. 2013.},
pages = {27--34},
title = {{The musical metacreation weekend: challenges arising from the live presentation of musically metacreative systems}},
url = {http://www.aaai.org/ocs/index.php/AIIDE/AIIDE13/paper/download/7444/7671},
year = {2013}
}
@article{Simon2005,
author = {Simon, Ian and Basu, Sumit and Salesin, David and Agrawala, Maneesh},
file = {:Users/carthach/Documents/Mendeley Desktop/Simon et al. - 2005 - Audio analogies creating new music from an existing performance by concatenative synthesi.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {65--72},
title = {{Audio analogies: creating new music from an existing performance by concatenative synthesi}},
url = {http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2005.161},
volume = {2005},
year = {2005}
}
@article{Jorda2016,
abstract = {This paper presents a generative drumming agent built from the$\backslash$nresults of an extensive survey carried out with electronic music producers, in$\backslash$ntwo phases. Following the techniques of user-centered interaction design, an$\backslash$ninternational group of beat producers was reviewed on the possibility of using AI$\backslash$nalgorithms to help them in the beat production workflow. The analyzed results of$\backslash$nthese tests were used as design requirements for constructing a system that would$\backslash$nindeed perform some tasks alongside the producer. The first results of this$\backslash$nworking prototype are presented with a description of the system. The prototype$\backslash$nis a stylistic drum generator that creates new rhythmic patterns after being$\backslash$ntrained with a collection of drum tracks. Further stages of development and$\backslash$npotential algorithms are discussed.},
author = {Jord{\`{a}}, Sergi and G{\'{o}}mez-Mar{\'{i}}n, Daniel and Faraldo, {\'{A}}ngel and Herrera, Perfecto},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}} et al. - 2016 - Drumming with style From user needs to a working prototype.pdf:pdf},
isbn = {978-1-925455-13-7},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {365--370},
title = {{Drumming with style: From user needs to a working prototype}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0071.pdf},
volume = {16},
year = {2016}
}
@article{Effects2003,
author = {Effects, Digital Audio},
file = {:Users/carthach/Documents/Mendeley Desktop/Effects - 2003 - MOSIEVIUS FEATURE DRIVEN INTERACTIVE AUDIO MOSAICING Ari Lazier , Perry Cook † Princeton University {\{} alazier , prc.pdf:pdf}},
journal = {Audio},
keywords = {2003,and music,ari lazier,c,conference on digital audio,dafx-03,department of computer science,effects,feature driven interactive audio,int,london,mosaicing,mosievius,of the 6 th,perry cook,september 8-11,uk},
pages = {1--6},
title = {{MOSIEVIUS : FEATURE DRIVEN INTERACTIVE AUDIO MOSAICING Ari Lazier , Perry Cook † Princeton University {\{} alazier , prc {\}}@ cs . princeton . edu}},
year = {2003}
}
@article{An2012,
author = {An, Steven S. and James, Doug L. and Marschner, Steve},
doi = {10.1145/2185520.2335453},
file = {:Users/carthach/Documents/Mendeley Desktop/An, James, Marschner - 2012 - Motion-driven concatenative synthesis of cloth sounds.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {Sound synthesis, cloth simulation, data-driven met,cloth simulation,data-driven met,data-driven meth-,sound synthesis},
number = {4},
pages = {1--10},
title = {{Motion-driven concatenative synthesis of cloth sounds}},
volume = {31},
year = {2012}
}
@inproceedings{Eyben2010,
address = {Utrecht, Netherlands},
author = {Eyben, Florian and B{\"{o}}ck, Sebastian and Schuller, Bjorn and Graves, Alex},
booktitle = {International Society for Music Information Retrieval Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Eyben et al. - 2010 - Universal Onset Detection with Bidirectional Long-Short Term Memory Neural Networks.pdf:pdf},
keywords = {ML,deepLearning},
mendeley-tags = {ML,deepLearning},
pages = {589--594},
title = {{Universal Onset Detection with Bidirectional Long-Short Term Memory Neural Networks}},
year = {2010}
}
@inproceedings{Brent2010,
author = {Brent, William},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Brent - 2010 - A timbre analysis and classification toolkit for Pure Data.pdf:pdf},
pages = {2--7},
title = {{A timbre analysis and classification toolkit for Pure Data}},
url = {http://williambrent.conflations.com/papers/timbreID.pdf},
year = {2010}
}
@article{Schwarz2007a,
abstract = {Corpus-based concatenative synthesis (CBCS) builds on a large database of segmented and descriptor-analysed sounds that are selected and played according to proximity to a target position in the descriptor space. This can be seen as a content-based extension to granular synthesis providing direct access to specific sound characteristics in real-time. The aim of the article is to show how cbcs supports--or actually inspires--new musical ideas by exploring the corpus interactively or via a written target score. We will show this with 4 musical examples of pieces that explore the concepts made possible by cbcs of live corpus recording, navigation in 3D in a metaphor of a score, interaction and corpus cross-synthesis, and harmonic selection tighly integrated with an orchestra score.},
author = {Schwarz, Diemo and Britton, Sam and Cahen, Roland and Goepfer, Thomas},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz et al. - 2007 - Musical Applications of Real-Time Corpus-Based Concatenative Synthesis.pdf:pdf},
journal = {International Computer Music Conference},
title = {{Musical Applications of Real-Time Corpus-Based Concatenative Synthesis}},
url = {http://mediatheque.ircam.fr/articles/textes/Schwarz07b/},
year = {2007}
}
@article{Hoskinson2007,
author = {Hoskinson, Reynald and Pai, Dinesh K.},
doi = {10.1162/pres.16.1.84},
file = {:Users/carthach/Documents/Mendeley Desktop/Hoskinson, Pai - 2007 - Synthetic Soundscapes with Natural Grains.pdf:pdf},
issn = {10547460},
journal = {Presence: Teleoperators {\&} Virtual Environments},
number = {1},
pages = {84--99},
title = {{Synthetic Soundscapes with Natural Grains}},
volume = {16},
year = {2007}
}
@misc{Frederickson,
author = {Frederickson, Ben},
title = {{Venn Diagrams with D3.js}},
url = {http://www.benfrederickson.com/venn-diagrams-with-d3.js/},
urldate = {2015-09-25},
year = {2012}
}
@phdthesis{Schwarz2004,
abstract = {Concatenative data-driven sound synthesis methods use a large database of source sounds, segmented into heterogeneous units, and a unit selection algorithm that finds the units that match best the sound or musical phrase to be synthesised, called the target. The selection is performed according to the features of the units. These are characteristics extracted from the source sounds, e.g. pitch, or attributed to them, e.g. instrument class. The selected units are then transformed to fully match the target specification, and concatenated. However, if the database is sufficiently large, the probability is high that a matching unit will be found, so the need to apply transformations is reduced. Usual synthesis methods are based on a model of the sound signal. It is very difficult to build a model that would preserve all the fine details of sound. Concatenative synthesis achieves this by using actual recordings. This data-driven approach (as opposed to a rule-based approach) takes advantage of the information contained in the many sound recordings. For example, very naturally sounding transitions can be synthesized, since unit selection is aware of the context of the database units. In speech synthesis, concatenative synthesis methods are the most widely used. They resulted in a considerable gain of naturalness and intelligibility. Results in other fields, for instance speech recognition, confirm the general superiority of data-driven approaches. Concatenative data-driven approaches have made their way into somemusical synthesis applications which are briefly presented. The Caterpillar software system developed in this thesis allows data-driven musical sound syn- thesis from a large database. However, musical creation is an artistic activity and thus not based on clearly definable criteria, like in speech synthesis. That's why a flexible, interactive use of the system allows composers to obtain new sounds. To constitute a unit database, alignment of music to a score is used to segment musical instrument recordings. It is based on spectral peak structure matching and the two approaches using Dynamic Time Warping and Hidden Markov Models are compared. Descriptor extraction analyses the sounds for their signal, spectral, harmonic, and perceptive char- acteristics, and temporal modeling techniques characterise the temporal evolution of the units uni- formly. However, it is possible to attribute score information like playing style, or arbitrary infor- mation to the units, which can later be used for selection. The database is implemented using a relational SQL database management system for optimal flexibility and reliability. A database interface cleanly separates the synthesis system from the database. The best matching sequence of units is found by a Viterbi unit selection algorithm. To incorporate a more flexible specification of the resulting sequence of units, the constraint solving algorithm of adaptive local search has been alternatively applied to unit selection. Both algorithms are based on two distance functions: the target distance expresses the similarity of a target unit to the database units, and the concatenation distance the quality of the join of two database units. Data-driven concatenative synthesis is then applied to instrument synthesis with high level control, explorative free synthesis from arbitrary sound databases, resynthesis of a recording with sounds from the database, and artistic speech synthesis. For these applications, unit corpora of violin sounds, environmental noises, and speech have been built.},
author = {Schwarz, Diemo},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz - 2004 - Data-driven concatenative sound synthesis.pdf:pdf},
school = {Universit{\'{e}} Pierre-et-Marie-Curie},
title = {{Data-driven concatenative sound synthesis}},
year = {2004}
}
@inproceedings{Horowitz2004a,
author = {Horowitz, Damon},
booktitle = {The Twelfth National Conference on Artificial Intelligence},
file = {:Users/carthach/Documents/Mendeley Desktop/Horowitz - 2004 - Generating Rhythms with Genetic Algorithms.pdf:pdf},
title = {{Generating Rhythms with Genetic Algorithms}},
year = {2004}
}
@inproceedings{Hulshof2016,
address = {Dublin, Ireland},
author = {Hulshof, Carolien and Siebert, Xavier and Melot, Hadrien},
booktitle = {International Workshop on Folk Music Analysis},
file = {:Users/carthach/Documents/Mendeley Desktop/NeoMI{\_} a New Environment for the Organization of Musical Instrume.pdf:pdf},
title = {{NeoMI : a New Environment for the Organization of Musical Instruments}},
year = {2016}
}
@article{Janer2008,
abstract = {This paper presents a system for controlling audio mosaicing with a voice signal, which can be interpreted as a further step in voice-driven sound synthesis. Compared to voice-driven instrumental synthesis, it increases the variety in the synthesized timbre. Also, it provides a more direct interface for audio mosaicing applications, where the performer voice controls rhythmic, tonal and timbre properties of the output sound. In a first step, voice signal is segmented into syllables, extracting a set of acoustic features for each segment. In the concatenative synthesis process, the voice acoustic features (target) are used to retrieve the most similar segment from the corpus of audio sources. We implemented a system working in pseudo-realtime, which analyzes voice input and sends control messages to the concatenative synthesis module. Additionally, this work raises questions to be further explored about mapping the input voice timbre space onto the audio sources timbre space.},
author = {Janer, Jordi and Boer, Maarten De},
file = {:Users/carthach/Documents/Mendeley Desktop/Janer, Boer - 2008 - Extending voice-driven synthesis to audio mosaicing.pdf:pdf},
isbn = {9783798320949},
journal = {5th Sound and Music Computing Conference},
title = {{Extending voice-driven synthesis to audio mosaicing}},
url = {http://www.mtg.upf.es/files/publications/SMC08-jjanermdeboer.pdf},
year = {2008}
}
@article{Chandra2012,
abstract = {The paper presents the interactive music system SoloJam, which allows a group of participants with little or no musical training to effectively play together in a ``band-like'' setting. It allows the participants to take turns playing solos made up of rhythmic pattern sequences. We specify the issue at hand for allowing such participation as being the requirement of decentralised coherent circulation of playing solos. This is to be realised by some form of intelligence within the devices used for participation. Here we take inspiration from the Economic Sciences, and propose this intelligence to take the form of making devices possessing the capability of evaluating their utility of playing the next solo, the capability of holding auctions, and of bidding within them. We show that holding auctions and bidding within them enables decentralisation of co-ordinating solo circulation, and a properly designed utility function enables coherence in the musical output. The approach helps achieve decentralised coherent circulation with artificial agents simulating human participants. The effectiveness of the approach is further supported when human users participate. As a result, the approach is shown to be effective at enabling participants with little or no musical training to play together in SoloJam.},
author = {Chandra, Arjun and Nymoen, Kristian and Voldsund, Arve and Jensenius, Alexander Refsum and Glette, Kyrre and Torresen, Jim},
file = {:Users/carthach/Documents/Mendeley Desktop/Chandra et al. - 2012 - Enabling Participants to Play Rhythmic Solos Within a Group via Auctions.pdf:pdf},
journal = {Proceedings of the 9th International Symposium on Computer Music Modelling and Retrieval},
keywords = {active music,algorithmic auctions,collaborative performance,conflict resolution},
pages = {674--689},
title = {{Enabling Participants to Play Rhythmic Solos Within a Group via Auctions}},
url = {http://www.duo.uio.no/sok/work.html?WORKID=167891{\&}fid=101016},
year = {2012}
}
@article{Stowell2008,
abstract = {Live performance situations can lead to degradations in the vocal signal from a typical microphone, such as ambient noise or echoes due to feedback. We investigate the robustness of continuous- valued timbre features measured on vocal signals (speech, singing, beatboxing) under simulated degradations. We also consider non- parametric dependencies between features, using information the- oretic measures and a feature-selection algorithm. We discuss how robustness and independence issues reflect on the choice of acous- tic features for use in constructing a continuous-valued vocal tim- bre space. While some measures (notably spectral crest factors) emerge as good candidates for such a task, others are poor, and some features such as ZCR exhibit an interaction with the type of voice signal being analysed.},
author = {Stowell, Dan and Plumbley, Mark D},
file = {:Users/carthach/Documents/Mendeley Desktop/StowellPlumbley08-dafx.pdf:pdf},
isbn = {9789512295173},
journal = {Proc. of the 11th Int. Conference on Digital Audio Effects (DAFx-08)},
pages = {1--8},
title = {{Robustness and independence of voice timbre features under live performance acoustic degradations}},
url = {https://www.acoustics.hut.fi/dafx08/papers/dafx08{\_}58.pdf},
year = {2008}
}
@article{McKinney2007,
abstract = {This is an extended analysis of eight different algorithms for musical tempo extraction and beat tracking. The algorithms participated in the 2006 Music Information Retrieval Evaluation eXchange (MIREX), where they were evaluated using a set of 140 musical excerpts, each with beats annotated by 40 different listeners. Performance metrics were constructed to measure the algorithms' abilities to predict the most perceptually salient musical beats and tempi of the excerpts. Detailed results of the evaluation are presented here and algorithm performance is evaluated as a function of musical genre, the presence of percussion, musical meter and the most salient perceptual tempo of each excerpt.},
author = {McKinney, M. F. and Moelants, D. and Davies, M. E.P. and Klapuri, A.},
doi = {10.1080/09298210701653252},
file = {:Users/carthach/Documents/Mendeley Desktop/27406232.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
number = {1},
pages = {1--16},
title = {{Evaluation of audio beat tracking and music tempo extraction algorithms}},
volume = {36},
year = {2007}
}
@inproceedings{Norowi2016,
address = {Melaka, Malaysia},
author = {Norowi, Noris Mohd and Mustaffa, Mas Rina and Miranda, Eduardo Reck},
booktitle = {Third International Conference on Information Retrieval and Knowledge Management},
file = {:Users/carthach/Documents/Mendeley Desktop/07806331.pdf:pdf},
isbn = {9781509029549},
keywords = {-concatenative sound synthesis,cocnatenation cost,eduardo reck miranda,homosonic audio,interdisciplinary centre for computer,music research,segments},
pages = {37--42},
title = {{Using Concatenation Cost for Unit Selection of Homosonic Segments in Concatenative Sound Synthesis}},
year = {2016}
}
@article{Boden2009a,
author = {Boden, Margaret A and Edmonds, Ernest A},
journal = {Digital Creativity},
number = {1-2},
pages = {21--46},
publisher = {Taylor {\&} Francis},
title = {{What is generative art?}},
volume = {20},
year = {2009}
}
@inproceedings{Toussaint2004African,
author = {Toussaint, Godfried T},
booktitle = {Papers Presented to the American Mathematical Society},
pages = {248},
title = {{A mathematical measure of preference in African rhythm}},
volume = {25},
year = {2004}
}
@article{countours,
author = {{ISO 226:2003}},
journal = {International Organization for Standardization; Geneva, Switzerland},
title = {{Acoustics—Normal Equal-Loudness Level Contours.}},
year = {2003}
}
@article{Pampalk2001,
author = {Pampalk, Elias},
file = {:Users/carthach/Documents/Mendeley Desktop/pam{\_}oegai03.pdf:pdf},
journal = {Journal of the Austrian Soc. for Artificial Intelligence},
number = {4},
pages = {20--23},
title = {{Islands of Music: Analysis, Organisation, and Visualisat ion of Music Archives}},
url = {http://www.ofai.at/{~}elias.pampalk/publications/pam{\_}oegai03.pdf},
volume = {22},
year = {2001}
}
@article{Ratcliffe2014,
abstract = {The following article contains a proposed typology of sampled material within electronic dance music (EDM). The typology offers a system of classification that takes into account the sonic, musical and referential properties of sampled elements, while also considering the technical realisation of material and the compositional intentions of the producer/DJ. Illustrated with supporting examples drawn from a wide variety of artists and sub-genres, the article seeks to address the current lack of research on the subject of sample-based composition and production, and provide a theoretical framework for further discussion of EDM sampling practices. In addition, it demonstrates how concepts and terminology derived from the field of electroacoustic music may be successfully applied to the study and analysis of EDM, resulting in an expanded analytical methodology.},
author = {Ratcliffe, Robert},
doi = {10.12801/1947-5403.2014.06.01.06},
file = {:Users/carthach/Documents/Mendeley Desktop/369-1590-1-PB.pdf:pdf},
issn = {19475403},
journal = {Dancecult: Journal of Electronic Dance Music Culture},
keywords = {EDM,composition,electroacoustic,mimesis,musical borrowing,production,sample,sampling,source bonding,spectromorphology,transcontextuality},
number = {1},
pages = {97--122},
title = {{A Proposed Typology of Sampled Material Within Electronic Dance Music}},
url = {https://dj.dancecult.net/index.php/dancecult/article/view/369},
volume = {6},
year = {2014}
}
@article{Tindale2004a,
abstract = {Musicians are able to recognise the subtle differences in timbre produced by different playing techniques on an instrument, yet there has been little research into achieving this with a computer. This paper will demonstrate an automatic system that can successfully recognise different timbres produced by different performance techniques and classify them using signal processing and classification tools. Success rates over 90{\%}are achievedwhen classifying snare drum timbres produced by different playing techniques.},
author = {Tindale, Adam and Kapur, Ajay and Tzanetakis, George and Fujinaga, Ichiro},
file = {:Users/carthach/Documents/Mendeley Desktop/Tindale et al. - 2004 - Retrieval of percussion gestures using timbre classification techniques.pdf:pdf},
isbn = {8488042442},
journal = {Proceedings of the International Conference on Music Information Retrieval},
pages = {541--545},
title = {{Retrieval of percussion gestures using timbre classification techniques}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.95.3844{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@article{Dixon2007,
abstract = {BeatRoot is an interactive beat tracking and metrical annotation system which has been used for several years in studies of performance timing. This paper describes improvements to the original system and a large-scale evaluation and analysis of the system's performance. The beat tracking algorithm remains largely unchanged: BeatRoot uses a multiple agent architecture which simultaneously considers several different hypotheses concerning the rate and placement of musical beats, resulting in accurate tracking of the beat, quick recovery from errors, and graceful degradation in cases where the beat is only weakly implied by the data. The original time domain onset detection algorithm, which caused pro- blems for beat tracking music without prominent drums, has been replaced with a more robust onset detector based on spectral flux. Other new features include annotation of multiple metrical levels and phrase boundaries, as well as improvements in the user interface. The software has been rewritten entirely in Java, and it runs on all major operating systems. BeatRoot is evaluated on a new test set of 1360 musical excerpts from a wide range of Western musical styles, and the results are compared with other evaluations such as the MIREX 2006 Audio Beat Tracking Evaluation. 1.},
author = {Dixon, Simon},
doi = {10.1080/09298210701653310},
file = {:Users/carthach/Documents/Mendeley Desktop/Dixon - 2007 - Evaluation of the audio beat tracking system BeatRoot.pdf:pdf},
isbn = {0929-8215},
issn = {09298215},
journal = {Journal of New Music Research},
number = {1},
pages = {39--50},
title = {{Evaluation of the audio beat tracking system BeatRoot}},
volume = {36},
year = {2007}
}
@article{Zils2001,
abstract = {This work addresses the issue of retrieving efficiently sound $\backslash$nsamples in large databases, in the context of digital music $\backslash$ncomposition. We propose a sequence generation mechanism called $\backslash$nmusical mosaicing, which enables to generate automatically $\backslash$nsequences of sound samples by specifying only high-level $\backslash$nproperties of the sequence to generate. The properties of the $\backslash$nsequence specified by the user are translated automatically into $\backslash$nconstraints holding on descriptors of the samples. The system we $\backslash$npropose is able to scale up on databases containing more than $\backslash$n100.000 samples, using a local search method based on constraint $\backslash$nsolving. In this paper, we describe the method for retrieving and $\backslash$nsequencing audio samples, and illustrate it with rhythmic and $\backslash$nmelodic musical sequences. },
author = {Zils, Aymeric and Pachet, Fran{\c{c}}ois},
file = {:Users/carthach/Documents/Mendeley Desktop/Zils, Pachet - 2001 - Musical mosaicing.pdf:pdf},
journal = {Digital Audio Effects (DAFx)},
pages = {1--6},
title = {{Musical mosaicing}},
url = {http://csl.sony.fr/downloads/papers/2001/zils-dafx2001.pdf},
year = {2001}
}
@article{cooper2006visualization,
author = {Cooper, Matthew and Foote, Jonathan and Pampalk, Elias and Tzanetakis, George},
journal = {Computer Music Journal},
number = {2},
pages = {42--62},
publisher = {MIT Press},
title = {{Visualization in audio-based music information retrieval}},
volume = {30},
year = {2006}
}
@article{Gomez-Marin2016,
author = {G{\'{o}}mez-Mar{\'{i}}n, Daniel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
file = {:Users/carthach/Documents/Mendeley Desktop/G{\'{o}}mez-Mar{\'{i}}n, Jord{\`{a}}, Herrera - 2016 - Rhythm Spaces.pdf:pdf},
isbn = {9780864913975},
journal = {Proceeedings of the 4th International Workshop on Musical Metacreation},
title = {{Rhythm Spaces}},
year = {2016}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1201.0490},
file = {:Users/carthach/Documents/Mendeley Desktop/pedregosa11a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
pmid = {1000044560},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=2078195{\%}5Cnhttp://arxiv.org/abs/1201.0490},
volume = {12},
year = {2012}
}
@article{Truax2005,
abstract = {Musical research over the last century has become increasingly entwined with the areas of acoustics, psychoacoustics, and electroacoustics. One of the most striking results has been to push the frontiers of models of sound and music to the micro level, what is generally termed microsound. At this level, concepts of frequency and time are conjoined by a quantum relationship, with an uncertainty principle relating them that is precisely analogous to the more famous uncertainty principle of quantum physics. A class of methods of sound synthesis and signal processing known as time-frequency models have their basis at this quantum level such that changes in a signal's time domain result in spectral alterations and vice versa. One such method, granular synthesis and the granulation of sampled sound, produces results by the generation of high densities of acoustical quanta called grains. Such a radical shift has profound implications for not only our models of sound design, but also for the compositional methods that emerge as well as the role of the composer in guiding complex processes. The paper will argue that these models are examples of a class of complex systems exhibiting emergent form that create a new form of virtual music instrument. ABSTRACT FROM AUTHOR]; Copyright of Journal of the Acoustical Society of America is the property of American Institute of Physics and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)},
author = {Truax, Barry},
file = {:Users/carthach/Documents/Mendeley Desktop/Music{\_}and{\_}science{\_}meet{\_}at{\_}the{\_}micro{\_}leve.pdf:pdf},
isbn = {00014966},
journal = {Journal of the Acoustical Society of America},
keywords = {MUSIC,QUANTUM theory,SIGNAL processing -- Digita},
number = {4},
pages = {2415},
title = {{Music and science meet at the micro level: Time-frequency methods and granular synthesis}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=mah{\&}AN=20264368{\&}site=ehost-live},
volume = {117},
year = {2005}
}
@article{Peeters2002a,
author = {Peeters, Geoffroy and Burthe, A La and Rodet, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Peeters, Burthe, Rodet - 2002 - Toward Automatic Music Audio Summary Generation from Signal Analysis.pdf:pdf},
journal = {ISMIR},
title = {{Toward Automatic Music Audio Summary Generation from Signal Analysis.}},
url = {http://users.cis.fiu.edu/{~}lli003/Music/ms/5.pdf},
year = {2002}
}
@article{Staay2010,
author = {van der Staay, Matthias and Mathis, Heinz},
file = {:Users/carthach/Documents/Mendeley Desktop/Staay, Mathis - 2010 - Improved EGNOS Decoding with the List Viterbi Algorithm.pdf:pdf},
keywords = {a comparison between a,and a list-viterbi-based decoding,channel coding,decoding,egnos,iterative,list viterbi algorithm,procedure is also,turbo-code-based},
pages = {8--11},
title = {{Improved EGNOS Decoding with the List Viterbi Algorithm}},
year = {2010}
}
@article{Pope2011,
author = {Pope, Richard},
journal = {Dancecult: Journal of Electronic Dance Music Culture},
number = {1},
pages = {24--44},
title = {{Hooked on an affect: Detroit techno and dystopian digital culture}},
volume = {2},
year = {2011}
}
@article{Aucouturier2006,
abstract = {This paper proposes to use the techniques of Concatenative Sound Synthesis in the context of real-time Music Interaction. We describe a system that generates an audio track by concatenating audio segments extracted from pre-existing musical files. The track can be controlled in real-time by specifying high-level properties (or constraints) holding on metadata about the audio segments. A constraint-satisfaction mechanism, based on local search, selects audio segments that best match those constraints at any time. We describe the real-time aspects of the system, notably the asynchronous adding/removing of constraints, and report on several constraints and controllers designed for the system. We illustrate the system with several application examples, notably a virtual drummer able to interact with a human musician in real-time.},
author = {Aucouturier, Jean Julien and Pachet, Fran{\c{c}}ois},
doi = {10.1080/09298210600696790},
file = {:Users/carthach/Documents/Mendeley Desktop/aucouturier-05c.pdf:pdf},
issn = {09298215},
journal = {Journal of New Music Research},
number = {1},
pages = {35--50},
title = {{Jamming with plunderphonics: Interactive concatenative synthesis of music}},
volume = {35},
year = {2006}
}
@article{DeCheveigne2002,
abstract = {An algorithm is presented for the estimation of the fundamental frequency (F0) of speech or musical sounds. It is based on the well-known autocorrelation method with a number of modifications that combine to prevent errors. The algorithm has several desirable features. Error rates are about three times lower than the best competing methods, as evaluated over a database of speech recorded together with a laryngograph signal. There is no upper limit on the frequency search range, so the algorithm is suited for high-pitched voices and music. The algorithm is relatively simple and may be implemented efficiently and with low latency, and it involves few parameters that must be tuned. It is based on a signal model (periodic signal) that may be extended in several ways to handle various forms of aperiodicity that occur in particular applications. Finally, interesting parallels may be drawn with models of auditory processing.},
author = {de Cheveign{\'{e}}, Alain and Kawahara, Hideki},
doi = {10.1121/1.1458024},
file = {:Users/carthach/Documents/Mendeley Desktop/de Cheveign{\'{e}}, Kawahara - 2002 - YIN, a fundamental frequency estimator for speech and music.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {4},
pages = {1917--1930},
pmid = {12002874},
title = {{YIN, a fundamental frequency estimator for speech and music.}},
volume = {111},
year = {2002}
}
@article{Font2015,
abstract = {Online sharing platforms host a vast amount of multimedia content generated by their own users. Such content is typically not uniformly annotated and can not be straightforwardly indexed. Therefore, making it accessible to other users poses a real challenge which is not specific of online sharing platforms. In general, content annotation is a common problem in all kinds of information systems. In this thesis, we focus on this problem and propose methods for helping users to annotate the resources they create in a more comprehensive and uniform way. Specifically, we work with tagging systems and propose methods for recommending tags to the content creators during the annotation process. To this end, we exploit information gathered from previous resource annotations in the same sharing platform, the so called folksonomy. Tag recommendation is evaluated using several methodologies, with and without the intervention of users, and in the context of large-scale tagging systems. We focus on the case of tag recommendation for sound sharing platforms. Besides studying the performance of several methods in this scenario, we analyse the impact of one of our proposed methods on the tagging system of a real-world and large-scale sound sharing site. As an outcome of this thesis, one of the proposed tag recommendation methods is now being daily used by hundreds of users in this sound sharing site. In addition, we explore a new perspective for tag recommendation which, besides taking advantage of information from the folksonomy, employs a sound-specific ontology to guide users during the annotation process. Overall, this thesis contributes to the advancement of the state of the art in tagging systems and folksonomy-based tag recommendation and explores interesting directions for future research. Even though our research is motivated by the particular challenges of sound sharing platforms and mainly carried out in that context, we believe our methodologies can be easily generalised and thus be of use to other information sharing platforms.},
author = {Font, Frederic},
file = {:Users/carthach/Documents/Mendeley Desktop/frederic{\_}font{\_}phd{\_}thesis.pdf:pdf},
keywords = {folksonomy,freesound,online sharing platforms,ontology,sound and music,tag recommendation,tagging},
pages = {184},
title = {{Tag Recommendation using Folksonomy Information for Online Sound Sharing Platforms}},
year = {2015}
}
@book{wirth1996compiler,
author = {Wirth, Niklaus and Wirth, Niklaus and Wirth, Niklaus and Informaticien, Suisse and Wirth, Niklaus},
publisher = {Addison-Wesley Reading},
title = {{Compiler construction}},
volume = {1},
year = {1996}
}
@phdthesis{CarthachONuanain2009,
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in - 2009 - The ReacTable as a Platform for Real-time Algorithmic Composition.pdf:pdf},
school = {Trinity College Dublin},
title = {{The ReacTable as a Platform for Real-time Algorithmic Composition}},
year = {2009}
}
@inproceedings{LeGroux2008,
author = {{Le Groux}, Sylvain and {FMJ Verschure}, Paul},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing},
file = {:Users/carthach/Documents/Mendeley Desktop/Le Groux, FMJ Verschure - 2008 - Perceptsynth mapping perceptual musical features to sound synthesis parameters.pdf:pdf},
isbn = {1424414849},
number = {1},
pages = {125--128},
title = {{Perceptsynth: mapping perceptual musical features to sound synthesis parameters}},
year = {2008}
}
@article{Jorda2007b,
author = {Jord{\`{a}}, Sergi and Geiger, G{\"{u}}nter},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}}, Geiger - 2007 - The reacTable exploring the synergy between live music performance and tabletop tangible interfaces.pdf:pdf},
journal = {Proceedings of the 1st international conference on Tangible and embedded interaction - TEI '07},
title = {{The reacTable: exploring the synergy between live music performance and tabletop tangible interfaces}},
url = {http://dl.acm.org/citation.cfm?id=1226998},
year = {2007}
}
@article{Donnelly2016,
author = {Donnelly, Patrick J. and Sheppard, John W.},
doi = {10.1109/ICDMW.2015.213},
file = {:Users/carthach/Documents/Mendeley Desktop/07395658.pdf:pdf},
isbn = {9781467384926},
journal = {Proceedings - 15th IEEE International Conference on Data Mining Workshop, ICDMW 2015},
keywords = {binary relevance classification,classification,crossdataset validation,feature extraction,instrument recognition,k-nearest neighbor,machine learning,music,music information retrieval,musical note separation,timbre},
pages = {94--101},
title = {{Cross-Dataset Validation of Feature Sets in Musical Instrument Classification}},
year = {2016}
}
@article{ansi,
address = {New York, USA},
author = {ANSI},
journal = {American National Standards Institute},
title = {{Psychoacoustic Terminology: Timbre}},
year = {1994}
}
@article{Klugel,
author = {Kl{\"{u}}gel, Niklas and Hagerer, Gerhard and Groh, Georg},
file = {:Users/carthach/Documents/Mendeley Desktop/Kl{\"{u}}gel, Hagerer, Groh - Unknown - TreeQuencer Collaborative Rhythm Sequencing-A Comparative Study.pdf:pdf},
journal = {nime2014.org},
keywords = {collaborative music making,creativity support,user study},
pages = {50--53},
title = {{TreeQuencer: Collaborative Rhythm Sequencing-A Comparative Study}},
url = {http://nime2014.org/proceedings/papers/498{\_}paper.pdf}
}
@article{Terasawa2006,
abstract = {We describe a perceptual space for timbre, define an objective metric that takes into account perceptual orthogonality and measure the quality of timbre interpolation. We discuss two timbre representations and using these two representations, measure perceived relationships between pairs of sounds on a equivalent range of timbre variety. We determine that a timbre space based on Mel-frequency cepstral coefficients (MFCC) is a good model for a perceptual timbre space.},
author = {Terasawa, Hiroko and Slaney, Malcolm and Berger, Jonathan},
file = {:Users/carthach/Documents/Mendeley Desktop/Terasawa2006(ICMPC-EuclideanDistance).pdf:pdf},
isbn = {8873951554},
journal = {Proceedings of the 9th {\ldots}},
keywords = {mfcc,perception,timbre},
title = {{Determining the Euclidean distance between two steady state sounds}},
url = {http://www.tara.tsukuba.ac.jp/{~}terasawa/Papers/Terasawa2006{\_}ICMPC9.pdf},
year = {2006}
}
@incollection{Gore1997,
author = {Gore, Georgiana},
booktitle = {Dance in the City},
pages = {50--67},
publisher = {Springer},
title = {{The beat goes on: trance, dance and tribalism in rave culture}},
year = {1997}
}
@article{herman1998politics,
author = {Herman, Andrew and Sloop, John M},
journal = {Critical Studies in Media Communication},
number = {1},
pages = {1--20},
publisher = {Taylor {\&} Francis Group},
title = {{The politics of authenticity in postmodern rock culture: The case of Negativland and $\backslash$textit{\{}the letter U and the numeral 2{\}}}},
volume = {15},
year = {1998}
}
@article{Patricio2016,
author = {Patricio, L and Dittmar, Christian},
file = {:Users/carthach/Documents/Mendeley Desktop/065{\_}Paper(1).pdf:pdf},
journal = {Proc. 17th International Society for Music Information Retrieval Conference},
pages = {502--508},
title = {{Towards Modeling and Decomposing Loop-Based Electronic Music}},
year = {2016}
}
@inproceedings{Schwarz2009,
abstract = {The article presents methods for sound search in large effects or instrument sound databases by interactive content- based navigation in a space of descriptors and categories, based on the principle of real-time corpus-based concatenative synthesis. We focus on three algorithms: fast similarity- based search by a kD-tree in the high-dimensional descriptor space, a mass–spring model with added repulsion for layout, and efficient dimensionality reduction for visualisation by hybrid multi-dimensional scaling based on these. Special at- tention is given to scalability to very large databases by per- formance evaluations and measurements. The algorithms are implemented and tested as C-libraries and Max/MSP ex- ternals within a prototype sound exploration application.},
author = {Schwarz, Diemo and Schnell, Norbert and Gulluni, Sebastien},
booktitle = {Proceedings of the International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/index (1).pdf:pdf},
isbn = {9780971319271},
pages = {253--258},
title = {{Scalability in Content-Based Navigation of Sound Databases}},
url = {https://www.researchgate.net/profile/Diemo{\_}Schwarz/publication/256436929{\_}Scalability{\_}in{\_}Content-Based{\_}Navigation{\_}of{\_}Sound{\_}Databases/links/560912fc08ae13969149bfea.pdf},
year = {2009}
}
@article{Jathal2017,
author = {Jathal, Kunal},
file = {:Users/carthach/Documents/Mendeley Desktop/07938136.pdf:pdf},
journal = {Computer Music Journal},
number = {2},
pages = {38--51},
publisher = {MITP},
title = {{Real-Time Timbre Classification for Tabletop Hand Drumming}},
volume = {41},
year = {2017}
}
@article{Essid2006,
author = {Essid, Slim and Richard, Ga{\"{e}}l and David, Bertrand},
file = {:Users/carthach/Documents/Mendeley Desktop/01643665.pdf:pdf},
number = {4},
pages = {1401--1412},
title = {{Pairwise Classification Strategies}},
volume = {14},
year = {2006}
}
@article{ebu,
author = {{ITU-R BS.1770-3}},
file = {:Users/carthach/Documents/Mendeley Desktop/R-REC-BS.1770-4-201510-I!!PDF-E.pdf:pdf},
journal = {International Telecommunications Union, Geneva Switzerland},
pages = {1770--4},
title = {{Algorithms to measure audio programme loudness and true-peak audio level}},
url = {https://www.itu.int/dms{\_}pubrec/itu-r/rec/bs/R-REC-BS.1770-4-201510-I!!PDF-E.pdf{\%}0Ahttps://www.itu.int/dms{\_}pubrec/itu-r/rec/bs/R-REC-BS.1770-1-200709-S!!PDF-E.pdf},
volume = {3},
year = {2013}
}
@article{Biles2002,
author = {Biles, John A},
journal = {Creative evolutionary systems},
pages = {2},
publisher = {San Francisco: Morgan Kaufmann},
title = {{GenJam: Evolution of a jazz improviser}},
volume = {168},
year = {2002}
}
@book{Apel1969,
author = {Apel, Willi},
publisher = {Harvard University Press},
title = {{Harvard dictionary of music}},
year = {1969}
}
@article{Tzanetakis2000,
author = {Tzanetakis, G and Cook, P R},
file = {:Users/carthach/Documents/Mendeley Desktop/Audio{\_}information{\_}retrieval{\_}AIR{\_}tools.pdf:pdf},
journal = {In Proc. International Symposium on Music Information Retrieval. ISMIR},
title = {{Audio information retrieval tools}},
year = {2000}
}
@inproceedings{Knees2016a,
abstract = {We report on the progress of GiantSteps, an EU-funded project involving institutions from academia, practition-ers, and industrial partners with the goal of developing new concepts for intelligent and collaborative interfaces for music production and performance. At the core of the project is an iterative, user-centric research approach to music information retrieval (MIR) and human computer interaction (HCI) that is designed to allow us to accom-plish three main targets, namely (1) the development of in-telligent musical expert agents to support and inspire mu-sic makers, (2) more intuitive and collaborative interfaces, and (3) low-complexity methods addressing low-cost de-vices to enable affordable and accessible production tools and apps. In this paper, we report on the main findings and achievements of the project's first two years.},
author = {Knees, Peter and Andersen, Kristina and Jord{\'{a}}, Sergi and Hlatky, Michael and Bucci, Andr{\'{e}}s and Gaebele, Wulf and Kaurson, Roman},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Knees et al. - Unknown - The GiantSteps Project A Second-Year Intermediate Report.pdf:pdf},
isbn = {0984527451},
pages = {363--365},
title = {{The GiantSteps Project: A Second-Year Intermediate Report}},
url = {http://www.cp.jku.at/research/papers/Knees{\_}etal{\_}ICMC{\_}2016.pdf},
year = {2016}
}
@phdthesis{OConnell2011,
abstract = {This thesis investigates the use of high level descriptors (like genre, mood, instrumentation, singer's gender, etc.) in audio mosaicing, a form of data driven concatenative sound synthesis (CSS). The document begins by discussing the advances made in the eld of music content description over the last 10 years, explaining the meaning of high level music content description and highlighting the relevance of automatic music content description in general, to the eld of audio mosaicing. It proceeds, tracing the origins of mosaicing from its beginnings as a time consuming manual process, through to modern eorts to automate mosaicing and enhance the productivity of artists seeking to create mosaics. The essential components of a mosaicing system are described. Existing mosaicing systems are dissected and categorised into a taxonomy based on their potential application area. The time resolution of high level descriptors is investigated and a new hierarchical framework for incorporating high level descriptors into mosaicing applications is introduced and evaluated. This framework is written in Python and utilises pure data as both user interface and audio engine. Descriptors, stemming from Music Information Retrieval (MIR) research are calculated using an in-house analysis extraction tool. In-house audio-matching software is used as the similarity search engine. Many other libraries have also been integrated to aid the research, in particular Aubio for note detection, and Rubberband, for time stretching. The high level descriptors included in this project are; mood (happy, sad, relaxed or happy), gender (male or female), key, scale (major or minor), instrumental, vocal. A mini application for augmenting audio loops with mosaics is presented. This is used to show how the framework can be extended to cater for a given mosaicing paradigm. The musical applications of mosaics in the traditional song-based composition are also explored. Finally, conclusions are drawn and directions for future work postulated.},
author = {O'Connell, John},
file = {:Users/carthach/Documents/Mendeley Desktop/O'Connell - 2011 - Musical Mosaicing with High Level Descriptors.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Musical Mosaicing with High Level Descriptors}},
url = {http://mtg.upf.edu/node/2332},
year = {2011}
}
@article{Clifford2008,
author = {Clifford, G.D.},
file = {:Users/carthach/Documents/Mendeley Desktop/ch15{\_}bss.pdf:pdf},
journal = {Biomedical signal and image processing},
pages = {1--47},
title = {{Chapter 15 - Blind source separation: principle {\&} independent component analysis}},
year = {2008}
}
@article{Lindemann2007,
abstract = {This article describes a new synthesis technology called reconstructive phrase modeling (RPM). A goal of RPM is to combine the realistic sound quality of sampling with the performance interaction of functional synthesis. Great importance is placed on capturing the dynamics of note transitions-slurs, legato, bow changes, etc. Expressive results are achieved with conventional keyboard controllers. Mastery of special performance techniques is not needed. RPM is an analysis-synthesis system that is related to two important trends in computer music research. The first is a form of additive synthesis in which sounds are represented as a sum of time-varying harmonics plus noise elements. RPM creates expressive performances by searching a database of idiomatic instrumental phrases and combining modified fragments of these phrases to form a new expressive performance. This approach is related to another research trend called concatenative synthesis},
author = {Lindemann, Eric},
doi = {10.1109/MSP.2007.323267},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {2},
pages = {80--91},
title = {{Music synthesis with reconstructive phrase modeling}},
volume = {24},
year = {2007}
}
@book{Chomsky1957,
abstract = {This article extends Howard Lasnik's commentary in Syntactic Structures Revisited (2000) on Chomsky's transformational analyses of the verbal morphology system. It reviews Lasnik's critique of the transformational machinery in Syntactic Structures and shows how the problems he identifies are resolved under a more minimal theory of transformations that developed circa 1980. Considering the three central topics in the analysis of this system--the order and form of verbal elements, the main verb movement parameter, and the distribution of periphrastic do--it discusses some problems that arise in Lasnik's hybrid account and proposes a lexicalist alternative that eliminates the rules of affix hopping and do-support.},
author = {Chomsky, Noam},
booktitle = {Syntax},
doi = {10.1111/j.1467-9612.2004.00004.x},
isbn = {3110172798},
issn = {13680005},
number = {2},
pages = {116},
pmid = {12287628},
title = {{Syntactic Structures}},
url = {http://doi.wiley.com/10.1111/j.1467-9612.2004.00004.x},
volume = {7},
year = {1957}
}
@inproceedings{black1994chatr,
author = {Black, Alan W and Taylor, Paul},
booktitle = {Proceedings of the 15th conference on Computational linguistics-Volume 2},
organization = {Association for Computational Linguistics},
pages = {983--986},
title = {{CHATR: a generic speech synthesis system}},
year = {1994}
}
@article{Kiefer2008,
author = {Kiefer, Chris and Collins, N and Fitzpatrick, G},
file = {:Users/carthach/Documents/Mendeley Desktop/Kiefer, Collins, Fitzpatrick - 2008 - Evaluating the wiimote as a musical controller.pdf:pdf},
journal = {{\ldots} International Computer Music {\ldots}},
title = {{Evaluating the wiimote as a musical controller}},
url = {http://sro.sussex.ac.uk/37113/1/evaluating-the-wiimote-as-a-musical-controller.pdf},
year = {2008}
}
@book{Albiez2010,
author = {Albiez, Sean and Pattie, David},
publisher = {Bloomsbury Publishing USA},
title = {{Kraftwerk: music non-stop}},
year = {2010}
}
@inproceedings{Oswald1985,
abstract = {Musical instruments produce sounds. Composers produce music. Musical instruments reproduce music. Tape recorders, radios, disc players, etc., reproduce sound. A device such as a wind-up music box produces sound and reproduces music. A phonograph in the hands of a hip hop/scratch artist who plays a record like an electronic washboard with a phonographic needle as a plectrum, produces sounds which are unique and not reproduced - the record player becomes a musical instrument. A sampler, in essence a recording, transforming instrument, is simultaneously a documenting device and a creative device, in effect reducing a distinction manifested by copyright.},
address = {Toronto},
author = {Oswald, John},
booktitle = {Wired Society Electro-Acoustic Conference},
keywords = {file-import-09-04-17},
title = {{Plunderphonics, or Audio Piracy as a Compositional Prerogative}},
year = {1985}
}
@article{Julia2015,
abstract = {The progressive appearance of affordable tabletop technology and devices urges human-computer interaction researchers to provide the necessary methods to make this kind of devices the most useful to their users. Studies show that tabletops have distinctive characteristics that can be specially useful to solve some types of problems, but this potential is arguably not yet translated into real-world applications. We theorize that the important components that can transform those systems into useful tools are application frameworks that take into account the devices affordances, a third party application ecosystem, and multi-application systems supporting concurrent multitasking. In this dissertation we approach these key components: First, we explore the distinctive affordances of tabletops, with two cases: TurTan, a tangible programming language in the education context, and SongExplorer, a music collection browser for large databases. Next, in order to address the difficulty of building such applications in a way that they can exploit these affordances, we focus on software frameworks to support the tabletop application making process, with two different approaches: ofxTableGestures, targeting programmers, and MTCF, designed for music and sound artists. Finally, recognizing that making useful applications is just one part of the problem, we focus on a fundamental issue of multi-application tabletop systems: the difficulty to support multi-user concurrent multitasking with third-party applications. After analyzing the possible approaches, we present GestureAgents, a content-based distributed application-centric disambiguation mechanism and its implementation, which solves this problem in a generic fashion, being also useful to other shareable interfaces, including uncoupled ones.},
author = {Juli{\`{a}}, Carles F.},
file = {:Users/carthach/Documents/Mendeley Desktop/thesis-Julia-Carles-F.pdf:pdf},
journal = {Department of Information and Communication Technologies},
keywords = {Applications,Collaboration,Frameworks,HCI,Multi-Tasking,Shared interfaces,interaction,tabletop},
pages = {210},
title = {{Making Tabletops Useful with Applications, Frameworks and Multi-Tasking}},
year = {2015}
}
@book{Shiffman2012,
abstract = {How can we capture the unpredictable evolutionary and emergent properties of nature in software? How can understanding the mathematical principles behind our physical world help us to create digital worlds? This book focuses on a range of programming strategies and techniques behind computer simulations of natural systems, from elementary concepts in mathematics and physics to more advanced algorithms that enable sophisticated visual results. Readers will progress from building a basic physics engine to creating intelligent moving objects and complex systems, setting the foundation for further experiments in generative design. Subjects covered include forces, trigonometry, fractals, cellular automata, self-organization, and genetic algorithms. The book's examples are written in Processing, an open-source language and development environment built on top of the Java programming language. On the book's website (http: //www.natureofcode.com), the examples run in the browser via Processing's JavaScript mode.},
author = {Shiffman, Daniel},
booktitle = {The Nature of Code},
isbn = {978-0985930806},
pages = {Chapter 10.},
title = {{The Nature of Code}},
year = {2012}
}
@article{Gomez2006,
author = {G{\'{o}}mez, Emilia},
doi = {10.1287/ijoc.1040.0126},
file = {:Users/carthach/Documents/Mendeley Desktop/G{\'{o}}mez - 2006 - Tonal Description of Polyphonic Audio for Music Content Processing.pdf:pdf},
issn = {1091-9856},
journal = {INFORMS Journal on Computing},
keywords = {a great amount of,accepted by elaine chew,accepted november 2004,audio material,cluster on computation in,content analysis and indexing,february 2004,guest editor for special,history,in the last few,key estimation,music,received,revised june 2004,sound and music computing,tonal description,years},
number = {3},
pages = {294--304},
title = {{Tonal Description of Polyphonic Audio for Music Content Processing}},
url = {http://pubsonline.informs.org/doi/10.1287/ijoc.1040.0126},
volume = {18},
year = {2006}
}
@inproceedings{Diaz-Banez2004,
address = {Kansas},
author = {D{\'{i}}az-B{\'{a}}{\~{n}}ez, J. Miguel and Farigu, Giovanna and G{\'{o}}mez, Francisco and Rappaport, David and Toussaint, Godfried T.},
booktitle = {Proceedings of BRIDGES: Mathematical Connections in Art, Music and Science},
file = {:Users/carthach/Documents/Mendeley Desktop/D{\'{i}}az-B{\'{a}}{\~{n}}ez et al. - 2004 - El comp{\'{a}}s flamenco a phylogenetic analysis.pdf:pdf},
title = {{El comp{\'{a}}s flamenco: a phylogenetic analysis}},
url = {http://archive.bridgesmathart.org/2004/bridges2004-61.html},
year = {2004}
}
@article{Hunt2003,
author = {Hunt, Andy and Wanderley, MM and Paradis, Matthew},
file = {:Users/carthach/Documents/Mendeley Desktop/Hunt, Wanderley, Paradis - 2003 - The importance of parameter mapping in electronic instrument design.pdf:pdf},
journal = {Journal of New Music {\ldots}},
keywords = {electronic instruments and t,electronic musical instruments,h e,human-computer interaction,mapping strategies},
pages = {1--6},
title = {{The importance of parameter mapping in electronic instrument design}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.4.429.18853},
year = {2003}
}
@article{Marolt2002,
abstract = {This paper presents a brief overview of our researches in the use of connectionist systems for transcription of polyphonic piano music and concentrates on the issue of onset detection in musical signals. We propose a technique for detecting onsets in a piano performance, based on a combination of a bank of auditory filters, a network of integrate-and-fire neurons and a multilayer perceptron. Such structure has certain advantages over the more commonly used peak-picking methods and we present its performance on several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing onset detection algorithms.},
author = {Marolt, Matija and Kavcic, Alenka and Privosnik, Marko},
file = {:Users/carthach/Documents/Mendeley Desktop/Marolt, Kavcic, Privosnik - 2002 - Neural networks for note onset detection in piano music.pdf:pdf},
journal = {Proceedings of ICMC 2002},
pages = {2--5},
title = {{Neural networks for note onset detection in piano music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.2277{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Cai2011,
abstract = {—The paper describes a method of identifying singers' voice from the monophonic music including sounds of various musical instruments based on auditory features. In this system, there are four problems to solve, vocal segment detection, feature extraction, modeling of the singing voice and identification. For a song to be identified, the vocal/nonvocal segment is detected via a new classifier—Sparse Representation-based Classification (SRC). The feature extraction is of the most importance. Human ear can distinguish among different types of sounds, so auditory features to describe the singer's voice are important. To describe the auditory features, we calculate features of each frame including Mel-frequency Cepstral Coefficient (MFCC), Liner Prediction Mel-frequency Cepstral Coefficient (LPMCC) and Gammatone Cepstral Coefficient (GTCC). Finally, we introduce the Gaussian Mixture Model (GMM) to model the singers' voice. This system is demonstrated to improve the performance of an automatic singer identification system in Music Information Retrieval (MIR).},
author = {Cai, Wei and Li, Qiang and Guan, Xin},
doi = {10.1109/ICNC.2011.6022500},
file = {:Users/carthach/Documents/Mendeley Desktop/06022500.pdf:pdf},
isbn = {9781424499533},
journal = {Proceedings - 2011 7th International Conference on Natural Computation, ICNC 2011},
keywords = {GMM,Gammatone Cepstrum Coefficient,SRC,auditory feature,singer identification,singing voice detection},
pages = {1624--1628},
title = {{Automatic singer identification based on auditory features}},
volume = {3},
year = {2011}
}
@article{Duxbury2002,
abstract = {Common problems with current methods of musical note onset detection are detection of fast passages of musical audio, detection of all onsets within a passage with a strong dynamic range and detection of onsets of varying types, such as multi-instrumental music. We present a method that uses a subband decomposition approach to onset detection. An energy-based detector is used on the upper subbands to detect strong transient events. This yields precision in the time resolution of the onsets, but does not detect softer or weaker onsets. A frequency based distance measure is formulated for use with the lower subbands, improving detection accuracy of softer onsets. We also present a method for improving the detection function, by using a smoothed difference metric. Finally, we show that the detection threshold may be set automatically from analysis of the statistics of the detection function, with results comparable in most places to manual setting of thresholds.},
author = {Duxbury, Chris and Sandler, Mark and Davies, Mike},
file = {:Users/carthach/Documents/Mendeley Desktop/Duxbury, Sandler, Davies - 2002 - A hybrid approach to musical note onset detection.pdf:pdf},
journal = {5th Int. Conference on Digital Audio Effects (DAFx-02), Hamburg, Germany},
number = {November 2002},
pages = {33--38},
title = {{A hybrid approach to musical note onset detection}},
url = {http://www.unibw-hamburg.de/EWEB/ANT/dafx2002/papers/DAFX02{\_}Duxbury{\_}Sandler{\_}Davis{\_}note{\_}onset{\_}detection.pdf},
year = {2002}
}
@article{Elliott2013,
abstract = {Attempts to relate the perceptual dimensions of timbre to quantitative acoustical dimensions have been tenuous, leading to claims that timbre is an emergent property, if measurable at all. Here, a three-pronged analysis shows that the timbre space of sustained instrument tones occupies 5 dimensions and that a specific combination of acoustic properties uniquely determines gestalt perception of timbre. Firstly, multidimensional scaling (MDS) of dissimilarity judgments generated a perceptual timbre space in which 5 dimensions were cross-validated and selected by traditional model comparisons. Secondly, subjects rated tones on semantic scales. A discriminant function analysis (DFA) accounting for variance of these semantic ratings across instruments and between subjects also yielded 5 significant dimensions with similar stimulus ordination. The dimensions of timbre space were then interpreted semantically by rotational and reflectional projection of the MDS solution into two DFA dimensions. Thirdly, to relate this final space to acoustical structure, the perceptual MDS coordinates of each sound were regressed with its joint spectrotemporal modulation power spectrum. Sound structures correlated significantly with distances in perceptual timbre space. Contrary to previous studies, most perceptual timbre dimensions are not the result of purely temporal or spectral features but instead depend on signature spectrotemporal patterns.},
author = {Elliott, Taffeta M. and Hamilton, Liberty S. and Theunissen, Fr{\'{e}}d{\'{e}}ric E.},
doi = {10.1121/1.4770244},
file = {:Users/carthach/Documents/Mendeley Desktop/1.4770244.pdf:pdf},
isbn = {0001-4966},
issn = {0001-4966},
journal = {The Journal of the Acoustical Society of America},
number = {1},
pages = {389--404},
pmid = {23297911},
title = {{Acoustic structure of the five perceptual dimensions of timbre in orchestral instrument tones}},
url = {http://asa.scitation.org/doi/10.1121/1.4770244},
volume = {133},
year = {2013}
}
@article{Battier2007a,
abstract = {Sixty years ago, musique concr{\`{e}}te was born of the single-handed efforts of one man, Pierre Schaeffer. How did the first experiments become a School and produce so many rich works? As this issue of Organised Sound addresses various aspects of the GRM activities throughout sixty years of musical adventure, this article discusses the musical thoughts behind the advent and the development of the music created and theoretised at the Paris School formed by the Schaefferian endeavours. Particular attention is given to the early twentieth-century conceptions of musical sounds and how poets, artists and musicians were expressing their quest for, as Apollinaire put it, ‘new sounds new sounds new sounds'. The questions of naming, gesture, sound capture, processing and diffusion are part of the concepts thoroughly revisited by the GRMC, then the GRM in 1958, up to what is known as acousmatic music. Other contributions, such as Teruggi's, give readers insight into the technical environments and innovations that took place at the GRM. This present article focuses on the remarkable unity of the GRM. This unity has existed alongside sixty years of activity and dialogue with researchers of other fields and constant attention to the latter-day scientific, technological and philosophical ideas which have had a strong influence in shaping the development of GRM over the course of its history.},
author = {Battier, Marc},
doi = {10.1017/S1355771807001902},
file = {:Users/carthach/Documents/Mendeley Desktop/Battier - 2007 - What the GRM brought to music from musique concr{\`{e}}te to acousmatic music.pdf:pdf},
isbn = {1355-7718},
issn = {1355-7718},
journal = {Organised Sound},
number = {03},
pages = {189--202},
title = {{What the GRM brought to music: from musique concr{\`{e}}te to acousmatic music}},
volume = {12},
year = {2007}
}
@article{gilbert1997soundtrack,
author = {Gilbert, Jeremy},
journal = {New Formations},
pages = {5--22},
publisher = {LAWRENCE {\&} WISHART},
title = {{Soundtrack for an uncivil society: rave culture, the criminal justice act and the politics of modernity}},
year = {1997}
}
@article{Collins2008,
abstract = {Composers have spent more than fifty years devising computer programs for the semi-automated production of music. This article shall focus in particular on the case of minimal run-time human intervention, where a program allows the creation of a musical variation, typically unravelling in realtime, on demand. These systems have the capacity to vary their output with each run, often from no more input information than the seeding of a random number generator with the start time. Such artworks are accumulating, released online as downloads, or exhibited through streaming radio sites such as rand(){\%}. Listener/users and composer/designers may wish for deeper insight into these programs' ontological status, mechanisms and creative potential. These works are challenging to dissect; this article makes a tentative start at confronting the unique problems and rich behaviours of computer-program-based generative music, from the social and historical context to the backwards engineering of programs in relation to their sound world. After a discussion of exemplars and definitions of generative art, strategies for analysis are outlined. To provide practical examples, analyses are provided of two small scale works by James McCartney.},
author = {Collins, Nick},
doi = {10.1017/S1355771808000332},
file = {:Users/carthach/Documents/Mendeley Desktop/analysis{\_}of{\_}generative{\_}music{\_}programs.pdf:pdf},
issn = {1355-7718},
journal = {Organised Sound},
number = {03},
pages = {237},
title = {{The Analysis of Generative Music Programs}},
url = {http://www.journals.cambridge.org/abstract{\_}S1355771808000332},
volume = {13},
year = {2008}
}
@article{Schluter2014,
author = {Schl{\"{u}}ter, Jan and B{\"{o}}ck, Sebastian},
file = {:Users/carthach/Documents/Mendeley Desktop/Schl{\"{u}}ter, B{\"{o}}ck - 2014 - Improved musical onset detection with convolutional neural networks.pdf:pdf},
isbn = {9781479928934},
journal = {IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP)},
pages = {7029--7033},
title = {{Improved musical onset detection with convolutional neural networks}},
year = {2014}
}
@article{Paulus2002b,
abstract = {A system is described which measures the similarity of two arbi- trary rhythmic patterns. The patterns are represented as acoustic signals, and are not assumed to have been performed with similar sound sets. Two novel methods are presented that constitute the algorithmic core of the system. First, a probabilistic musical meter estimation process is described, which segments a continuous musical signal into patterns. As a side-product, the method outputs tatum, tactus (beat), and measure lengths. A subsequent process performs the actual similarity measurements. Acoustic features are extracted which model the fluctuation of loudness and brightness within the pattern, and dynamic time warping is then applied to align the patterns to be compared. In simulations, the system behaved consistently by assigning high similarity measures to sim- ilar musical rhythms, even when performed using different sound sets.},
author = {Paulus, Jouni and Klapuri, Anssi},
file = {:Users/carthach/Documents/Mendeley Desktop/Paulus, Klapuri - 2002 - Measuring the Similarity of Rhythmic Patterns.pdf:pdf},
isbn = {2844261663},
journal = {Signal Processing},
pages = {44},
title = {{Measuring the Similarity of Rhythmic Patterns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.3698{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {2002}
}
@article{Wiggins2006,
abstract = {Philosophical account of creativity has been criticised on the grounds that it does not properly capture some aspects of creative situations.5) Wiggins13) has presented a formalisation of Bodens account, which allows such issues to be examined more precisely. We explore the relationship between traditional AI search methods and Bodens abstraction of creative behaviour, and revisit Bundys argument in the context of that exploration.},
author = {Wiggins, Geraint A.},
doi = {10.1007/BF03037332},
file = {:Users/carthach/Documents/Mendeley Desktop/Wiggins3b - wiggins{\_}ngc.pdf:pdf},
isbn = {0288-3635},
issn = {02883635},
journal = {New Generation Computing},
keywords = {Complexity,Computational creativity,Search},
number = {3},
pages = {209--222},
title = {{Searching for computational creativity}},
volume = {24},
year = {2006}
}
@misc{Eno1996,
author = {Eno, Brian},
publisher = {SSEYO Koan Software},
title = {{Generative Music}},
url = {http://intermorphic.com/sseyo/koan/generativemusic1/},
year = {1996}
}
@inproceedings{Einbond2010,
abstract = {Corpus-based concatenative synthesis presents unique possibilities for the visualization of audio descriptor data. These visualization tools can be applied to sound diffusion in the physical space of the concert hall using current spatialization technologies. Using CataRT and the FTM library for Max/MSP we develop a technique for the organization of a navigation space for synthesis based on user-defined spatial zones and informed by the perceptual concept of timbre space. Spatialization responds automatically to descriptor data and is controllable in real-time or can be recorded for later playback. This work has been realized in recent compositions for instruments, electronics, and sound installation using Wave Field Synthesis and Vector-Based Amplitude Panning. The goal is to place the listener in the midst of a virtual space of sounds organized by their descriptor data, simulating an immersion in timbre space.},
address = {New York, USA},
author = {Einbond, Aaron and Schwarz, Diemo},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/spatializing-timbre-with-corpus-based-concatenative (1).pdf:pdf},
isbn = {0971319286},
title = {{Spatializing Timbre With Corpus-Based Concatenative Synthesis}},
year = {2010}
}
@inproceedings{Knees2015,
address = {Malaga, Spain},
author = {Knees, Peter and Faraldo, {\'{A}}ngel and Herrera, Perfecto and Vogl, Richard and B{\"{o}}ck, Sebastian and Florian, H{\"{o}}rschl{\"{a}}ger and {Le Goff}, Mickael},
booktitle = {Proceedings of the 16th International Society for Music Information Retrieval Conference (ISMIR)},
title = {{Two Data Sets for Tempo Estimation and Key Detection in Electronic Dance Music Annotated from User Corrections}},
year = {2015}
}
@article{koetting1970analysis,
author = {Koetting, James},
journal = {Selected reports in ethnomusicology},
number = {3},
pages = {115--146},
title = {{Analysis and notation of West African drum ensemble music}},
volume = {1},
year = {1970}
}
@article{Orio2001,
author = {Orio, Nicola and Schnell, Norbert and Wanderley, MM},
file = {:Users/carthach/Documents/Mendeley Desktop/Orio, Schnell, Wanderley - 2001 - Input devices for musical expression borrowing tools from HCI.pdf:pdf},
journal = {{\ldots} interfaces for musical expression},
keywords = {gestural control,input device design,interactive systems},
title = {{Input devices for musical expression: borrowing tools from HCI}},
url = {http://dl.acm.org/citation.cfm?id=1085157},
year = {2001}
}
@inproceedings{Reiss2001,
author = {Skovenborg, Esben and Nielsen, S{\o}ren H.},
booktitle = {Proceedings of the Audio Engineering Society Convention},
file = {:Users/carthach/Documents/Mendeley Desktop/skovenborg{\_}2004{\_}loudness{\_}m.pdf:pdf},
title = {{Evaluation of Different Loudness Models with Music and Speech Material}},
year = {2004}
}
@inproceedings{leimeister2014rhythmic,
author = {Leimeister, Matthias and Gaertner, Daniel and Dittmar, Christian},
booktitle = {Audio Engineering Society Conference: 53rd International Conference: Semantic Audio},
month = {jan},
title = {{Rhythmic Classification of Electronic Dance Music}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=17109},
year = {2014}
}
@incollection{Biles2007,
author = {Biles, John A},
booktitle = {Evolutionary Computer Music},
pages = {137--169},
publisher = {Springer},
title = {{Improvizing with genetic algorithms: GenJam}},
year = {2007}
}
@article{SaidurgaSubramanian2016,
abstract = {Music synthesis is a process of synthesizing musical instruments using signal processing. This thesis is aimed to deliver a realistic concatenative music synthesis, where the system is statistically modelled with the musical notes from recorded music. This is implemented in three phases. First by iteratively extracting out the musical notes from the individual instrumental tracks to create a concordance of musical notes, then by statistically modelling the note parameters and finally reconstructing the parameters to form a concatenative music pattern. Individual musical note is extracted from the separated tracks by Onset Velocity Detection. Hidden Markov Model based Music Synthesis System is built to statistically model the musical parameters. Waveform is reconstructed by passing the parameters through synthetic filters to synthesize music following artists playing and compositional style.},
author = {{Saidurga Subramanian}},
file = {:Users/carthach/Documents/Mendeley Desktop/Subramanian{\_}sdsu{\_}0220N{\_}11197.pdf:pdf},
journal = {San Diego state university},
keywords = {TTS,musicxml,singing synthesis,sound synthesis,source separation},
pages = {1--45},
title = {{Concatenation music synthesis by note separation using onset velocity detection and statistical models}},
year = {2016}
}
@inproceedings{Fitz2013,
author = {FitzGerald, Derry},
booktitle = {Audio Engineering Society Convention 134},
organization = {Audio Engineering Society},
title = {{The good vibrations problem}},
year = {2013}
}
@inproceedings{Pachet2015,
address = {Austin, Texas USA},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
booktitle = {Proceedings of the 29th Conference on Artificial Intelligence (AAAI 2015), Workshop on ``Beyond the Turing Test''},
file = {:Users/carthach/Documents/Mendeley Desktop/Pachet, Roy - 2015 - (Manufac) Turing Tests for Music.pdf:pdf},
title = {{(Manufac) Turing Tests for Music}},
year = {2015}
}
@book{Wang2006,
author = {Wang, DeLiang and Brown, Guy J},
publisher = {Wiley-IEEE press},
title = {{Computational auditory scene analysis: Principles, algorithms, and applications}},
year = {2006}
}
@article{Cao2014,
author = {Cao, Erica and Lotstein, Max and Johnson-Laird, PN},
doi = {10.1525/MP.2014.31.5.444},
file = {:Users/carthach/Documents/Mendeley Desktop/Cao, Lotstein, Johnson-Laird - 2014 - Similarity and Families of Musical Rhythms.pdf:pdf;:Users/carthach/Documents/Mendeley Desktop/Cao, Lotstein, Johnson-Laird - 2014 - Similarity and Families of Musical Rhythms(2).pdf:pdf},
issn = {15338312},
journal = {Music Perception},
keywords = {meter,perceptual groups,rhythm,similar-},
number = {5},
pages = {444--469},
title = {{Similarity and Families of Musical Rhythms}},
url = {http://www.jstor.org/stable/10.1525/mp.2014.31.5.444},
volume = {31},
year = {2014}
}
@inproceedings{McCormack2013,
abstract = {This paper discusses issues in evolutionary art related to Art Theory and Aesthetics with a view to better understanding how they might contribute to both research and practice. Aesthetics is a term often used in evolutionary art, but is regularly used with conflicting or na¨ıve understandings. A selective history of evolutionary art as art is provided, with an examination of some art theories from within the field. A brief review of aesthetics as studied in philosophy and art theory follows. It is proposed that evolutionary art needs to resolve some important con- flicts and be clearer about what what it means by terms like “art” and “aesthetics”. Finally some possibilities for how to resolve these conflicts are described.},
author = {McCormack, Jon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-36955-1_1},
isbn = {9783642369544},
issn = {03029743},
keywords = {Aesthetics,Art Theory,Evolutionary Art},
pages = {1--12},
title = {{Aesthetics, art, evolution}},
volume = {7834 LNCS},
year = {2013}
}
@article{Herrera2000a,
abstract = {This paper we concentrate on reviewing thedifferent techniques that have been so far proposed for automatic classification of musical instruments. Asmost of the techniques to be discussed are usable only in "solo" performances we will evaluate theirapplicability to the more complex case of describing sound mixes. We conclude this survey discussing thenecessity of developing new strategies for classifying sound mixes without a priori separation of soundsources},
author = {Herrera, Perfecto and Amatriain, Xavier and Batlle, Eloi and Serra, Xavier},
doi = {10.1016/j.ijfoodmicro.2004.10.039},
file = {:Users/carthach/Documents/Mendeley Desktop/ismir2000-herrera.pdf:pdf},
issn = {0168-1605},
journal = {International Symposium on Music Information Retrieval},
keywords = {classification,multimedia content,music content processing,segmentation,timbre models},
number = {1},
pages = {1--9},
pmid = {529270},
title = {{Towards instrument segmentation for music content description : a critical review of instrument classification techniques}},
url = {http://users.cis.fiu.edu/{~}lli003/Music/cla/15.pdf{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.6351{\&}rep=rep1{\&}type=pdf},
volume = {8},
year = {2000}
}
@article{Frane2017,
author = {Frane, Andrew V.},
doi = {10.1525/mp.2017.34.3.291},
file = {:Users/carthach/Documents/Mendeley Desktop/Frane{\_}SwingInBreakbeats{\_}2017.pdf:pdf},
issn = {0730-7829},
journal = {Music Perception: An Interdisciplinary Journal},
number = {3},
pages = {291--302},
title = {{Swing Rhythm in Classic Drum Breaks From Hip-Hop's Breakbeat Canon}},
url = {http://mp.ucpress.edu/lookup/doi/10.1525/mp.2017.34.3.291},
volume = {34},
year = {2017}
}
@article{Blickle1996a,
abstract = {Evolutionary algorithms are a common probabilistic optimisation method based on the model of natural evolution. One important operator in these algorithms is the selection scheme, for which in this paper a new description model, based on fitness distributions, is introduced. With this, a mathematical analysis of tournament selection, truncation selection, ranking selection, and exponential ranking selection is carried out that allows an exact prediction of the fitness values after selection. The correspondence of binary tournament selection and ranking selection in the expected fitness distribution is proved. Furthermore, several properties of selection schemes are derived (selection intensity, selection variance, loss of diversity), and the three selection schemes are compared using these properties.},
author = {Blickle, Tobias and Thiele, Lothar},
doi = {10.1162/evco.1996.4.4.361},
file = {:Users/carthach/Documents/Mendeley Desktop/Blickle, Thiele - 1996 - A Comparison of Selection Schemes Used in Evolutionary Algorithms.pdf:pdf},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {4},
pages = {361--394},
title = {{A Comparison of Selection Schemes Used in Evolutionary Algorithms}},
volume = {4},
year = {1996}
}
@misc{Essl2014,
author = {Essl, Karlheinz},
title = {{Real Time Composition Library (RTC-lib)}},
url = {http://www.essl.at/works/rtc.html},
urldate = {2014-01-01},
year = {1992}
}
@inproceedings{bulyko2001joint,
author = {Bulyko, Ivan and Ostendorf, Mari},
booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing, 2001 .(ICASSP'01).},
organization = {IEEE},
pages = {781--784},
title = {{Joint prosody prediction and unit selection for concatenative speech synthesis}},
volume = {2},
year = {2001}
}
@article{Viterbi1967,
abstract = {The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above{\textless}tex{\textgreater}R{\_}{\{}0{\}}{\textless}/tex{\textgreater}, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above{\textless}tex{\textgreater}R{\_}{\{}0{\}}{\textless}/tex{\textgreater}and whose performance bears certain similarities to that of sequential decoding algorithms.},
author = {Viterbi, Andrew J.},
doi = {10.1109/TIT.1967.1054010},
file = {:Users/carthach/Documents/Mendeley Desktop/01054010.pdf:pdf},
isbn = {0018-9448},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {260--269},
pmid = {24025428},
title = {{Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm}},
volume = {13},
year = {1967}
}
@inproceedings{Roy2007b,
abstract = {There is an increasing need for automatically classifying sounds for MIR and interactive music applications. In the context of supervised classification, we describe an approach that improves the performance of the general bag-of-frame scheme without loosing its generality. This method is based on the construction and exploitation of specific audio features, called analytical, as input to classifiers. These features are better, in a sense we define precisely than standard, general features, or even than ad hoc features designed by hand for specific problems. To construct these features, our method explores a very large space of functions, by composing basic operators in syntactically correct ways. These operators are taken from the Mathematical and Audio Processing domains. Our method allows us to build a large number of these features, evaluate and select them automatically for arbitrary audio classification problems. We present here a specific study concerning the analysis of Pandeiro (Brazilian tambourine) sounds. Two problems are considered: the classification of entire sounds, for MIR applications, and the classification of attacks portions of the sound only, for interactive music applications. We evaluate precisely the gain obtained by analytical features on these two problems, in comparison with stndard approaches.},
author = {Roy, Pierre and Pachet, Fran{\c{c}}ois and Krakowski, Sergio},
booktitle = {10th Int. Conference on Digital Audio Effects (DAFx-07)},
file = {:Users/carthach/Documents/Mendeley Desktop/Roy, Pachet, Krakowski - 2007 - Analytical Features for the classification of Percussive Sounds the case of the pandeiro.pdf:pdf},
isbn = {978-3-85403-218},
pages = {1--8},
title = {{Analytical Features for the classification of Percussive Sounds: the case of the pandeiro}},
year = {2007}
}
@article{Bogdanov2013,
abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
author = {Bogdanov, D and Wack, Nicolas and G{\'{o}}mez, Emilia and Gulati, Sankalp and Herrera, P and Mayor, O and Roma, G and Salamon, J and Zapata, J and Serra, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Bogdanov et al. - 2013 - ESSENTIA an Audio Analysis Library for Music Information Retrieval.pdf:pdf},
journal = {International Society for Music Information Retrieval Conference (ISMIR 2013)},
pages = {493--498},
title = {{ESSENTIA: an Audio Analysis Library for Music Information Retrieval}},
year = {2013}
}
@article{Zapata2014,
author = {Zapata, Jos{\'{e}} R. and Davies, Matthew E P and G{\'{o}}mez, Emilia},
doi = {10.1109/TASLP.2014.2305252},
file = {:Users/carthach/Documents/Mendeley Desktop/Zapata, Davies, G{\'{o}}mez - 2014 - Multi-feature beat tracking.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Beat tracking,Evaluation,Music information retrieval,Music signal processing},
number = {4},
pages = {816--825},
title = {{Multi-feature beat tracking}},
volume = {22},
year = {2014}
}
@article{Alfonseca2006,
author = {Alfonseca, M and Cebrian, M and Ortega, A},
file = {:Users/carthach/Documents/Mendeley Desktop/Alfonseca, Cebrian, Ortega - 2006 - A Fitness Function for Computer-Generated Music using Genetic Algorithms.pdf:pdf},
journal = {WSEAS Transactions on Computers},
title = {{A Fitness Function for Computer-Generated Music using Genetic Algorithms}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0 http://arantxa.ii.uam.es/{~}alfonsec/docs/artint/music2b.pdf},
year = {2006}
}
@article{Effects2005,
author = {Effects, Digital Audio},
file = {:Users/carthach/Documents/Mendeley Desktop/Effects - 2005 - FAST IMPLEMENTATION FOR NON-LINEAR TIME-SCALING OF STEREO SIGNALS Emmanuel Ravelli , Mark Sandler and Juan P . Bello Ce.pdf:pdf},
number = {4},
pages = {20--23},
title = {{FAST IMPLEMENTATION FOR NON-LINEAR TIME-SCALING OF STEREO SIGNALS Emmanuel Ravelli , Mark Sandler and Juan P . Bello Centre for Digital Music , Dept . of Electronic Engineering Queen Mary , University of London , UK}},
volume = {0},
year = {2005}
}
@misc{Horner1991,
abstract = {Genetic Algorithms have been used with increasing frequency and effectiveness in a variety of problems. This paper investigates the application of genetic algorithms to music composition. A thechnique of thematic bridging is presented to the genetic algorithm. A look at the effects of building block linkage subsequently establishes a basis for implementing a GA-optimizable operation set for the problem. Some preliminary results are then discussed with an eye toward future work in GA-assisted composition.},
author = {Horner, Andrew and Goldberg, David E.},
booktitle = {Proceedings of the Fourth International Conference on Genetic Algorithms},
doi = {Proceedings Paper},
file = {:Users/carthach/Documents/Mendeley Desktop/Horner, Goldberg - 1991 - Genetic Algorithms and Computer-Assisted Music Composition.pdf:pdf},
pages = {437},
title = {{Genetic Algorithms and Computer-Assisted Music Composition}},
volume = {51},
year = {1991}
}
@book{Levitin,
address = {New York},
author = {Levitin, Daniel J.},
publisher = {Dutton/Penguin Books},
title = {{This is Your Brain on Music: The Science of a Human Obsession.}},
year = {2007}
}
@misc{WaveDNA2015,
author = {WaveDNA},
title = {{Liquid Music for Live}},
url = {https://www.wavedna.com/liquid-music/ableton-live-plugin-max-for-live/},
urldate = {2015-09-28},
year = {2015}
}
@inproceedings{Tzanetakis2001,
abstract = {Musical genres are categorical descriptions that are used to describe music. They are commonly used to structure the increasing amounts of music available in digital form on the  Web and are important for music information retrieval. Genre categorization for audio has traditionally been performed manually. A particular musical genre is characterized by statistical properties related to the instrumentation, rhythmic structure and form of its members. In this work, algorithms for the automatic genre categorization of audio signals are described. More specifically, we propose a set of features for representing texture and instrumentation. In addition a novel set of features for representing rhythmic structure and strength is proposed. The performance of those feature sets has been evaluated by training statistical pattern recognition classifiers using real world audio collections. Based on the  automatic hierarchical genre classification two graphical user interfaces for browsing and interacting with large audio collections have been developed.},
author = {Tzanetakis, G and Essl, G and Cook, P},
booktitle = {Proceedings of the Second International Symposium on Music Information Retrieval},
doi = {10.1109/TSA.2002.800560},
file = {:Users/carthach/Documents/Mendeley Desktop/Tzanetakis, Essl, Cook - 2001 - Automatic musical genre classification of audio signals.pdf:pdf},
issn = {1063-6676},
keywords = {Folder - Feature extraction,file-import-09-02-12,music},
mendeley-tags = {Folder - Feature extraction,file-import-09-02-12,music},
pages = {6 total pages},
title = {{Automatic musical genre classification of audio signals}},
year = {2001}
}
@article{Masri1996,
abstract = {Current music analysis-resynthesis models represent sounds through a set of features, which are extracted from a time-frequency representation. So that each time-frame can present a good approximation to the instantaneous spectrum, it is necessary to analyse the waveform in short segments. This is achieved with a window function whose position is advanced by a fixed amount between frames. When the window encompasses a transient event, such as the percussive onset of a note, it contains information both before and after the event. These partially- correlated spectra often become confused during analysis and cause audible ‘diffusion' upon resynthesis. This paper presents a simple, novel technique to avoid the problem, by synchronising the analysis window to transient events. Event locations are identified by observing short-term changes in the spectrum. Thereafter the position of the analysis window is constrained, to prevent it capturing the signal both sides of an event simultaneously. This method, which has been automated, yields an improvement that is clearly audible, particularly for percussive sounds which retain their ‘crispness'.},
author = {Masri, Paul and Bateman, Andrew},
file = {:Users/carthach/Documents/Mendeley Desktop/Masri, Bateman - 1996 - Improved modelling of attack transients in music analysis-resynthesis.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {100--103},
title = {{Improved modelling of attack transients in music analysis-resynthesis}},
url = {http://hans.fugal.net/comps/papers/masri{\_}1996.pdf},
year = {1996}
}
@inproceedings{Toussaint2003,
address = {Granada, Spain},
author = {Toussaint, Godfried},
booktitle = {BRIDGES: Mathematical Connections in Art, Music and Science},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint - 2003 - Classification and Phylogenetic Analysis of African Ternary Rhythm Timelines.pdf:pdf},
pages = {25--36},
title = {{Classification and Phylogenetic Analysis of African Ternary Rhythm Timelines}},
year = {2003}
}
@misc{Bock2016,
abstract = {In this paper, we present madmom, an open-source audio processing and music information retrieval (MIR) library written in Python. madmom features a concise, NumPy-compatible, object oriented design with simple calling conventions and sensible default values for all parameters, which facilitates fast prototyping of MIR applications. Prototypes can be seamlessly converted into callable processing pipelines through madmom's concept of Processors, callable objects that run transparently on multiple cores. Processors can also be serialised, saved, and re-run to allow results to be easily reproduced anywhere. Apart from low-level audio processing, madmom puts emphasis on musically meaningful high-level features. Many of these incorporate machine learning techniques and madmom provides a module that implements some in MIR commonly used methods such as hidden Markov models and neural networks. Additionally, madmom comes with several state-of-the-art MIR algorithms for onset detection, beat, downbeat and meter tracking, tempo estimation, and piano transcription. These can easily be incorporated into bigger MIR systems or run as stand-alone programs.},
address = {Amsterdam, Netherlands},
archivePrefix = {arXiv},
arxivId = {1605.07008},
author = {B{\"{o}}ck, Sebastian and Korzeniowski, Filip and Schl{\"{u}}ter, Jan and Krebs, Florian and Widmer, Gerhard},
booktitle = {Proceedings of the 2016 ACM on Multimedia Conference},
doi = {10.1145/2964284.2973795},
eprint = {1605.07008},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck et al. - 2016 - madmom a new Python Audio and Music Signal Processing Library(2).pdf:pdf},
isbn = {9781450336031},
keywords = {audio analysis,cessing,machine learning,music information retrieval,open source,python,signal pro-},
title = {{madmom: a new Python Audio and Music Signal Processing Library}},
url = {http://arxiv.org/abs/1605.07008},
year = {2016}
}
@article{Campbell1997,
abstract = {In this era of internationalism and multiculturalism, some music programmes may be hanging by this very thread: the belief that through the mere exposure of students to 'songs from many lands', a cultural harmony and a multicultural understanding might be achieved. Yet the question must arise among educators as it has frequently done among ethnomusicologists throughout the twentieth century. Is music, in fact, universally understood? And with little or no formal education to undergird the culture-specific meanings that music may encompass? This paper will review relevant ethnomusicological literature from the turn of the century onward, in an attempt to make sense of the truth or deception behind the statement 'music, the universal language', and as a means of seeking out why romantic notions and politically-correct directives may be driving the continuance of the premise that 'all music is created equal'. Emic and etic views of music, and issues of bimusicality and multimusicality are also addressed relevant to teaching music from the perspective of diversity (culture-specific distinctions) or universality (hom ogenous properties).},
author = {Campbell, Patricia Shehan},
doi = {10.1177/025576149702900105},
issn = {0255-7614},
journal = {International Journal of Music Education},
number = {1},
pages = {32--39},
title = {{Music, the universal language: fact or fallacy?}},
url = {http://ijm.sagepub.com/cgi/content/abstract/os-29/1/32},
volume = {os-29},
year = {1997}
}
@article{Pishdadian2017,
author = {Pishdadian, Fatemeh and Pardo, Bryan and Liutkus, Antoine},
doi = {10.1109/ICASSP.2017.7952219},
file = {:Users/carthach/Documents/Mendeley Desktop/07952219.pdf:pdf},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Audio source separation,Multi-resolution Common Fate Transform},
pages = {566--570},
title = {{A Multi-resolution approach to Common Fate-based audio separation}},
year = {2017}
}
@article{Roads1998,
author = {Roads, Curtis},
journal = {Computer Music Journal},
number = {2},
pages = {11--13},
publisher = {JSTOR},
title = {{Introduction to granular synthesis}},
volume = {12},
year = {1988}
}
@article{Siedenburg2017,
author = {Siedenburg, K. and McAdams, S.},
doi = {10.3389/fpsyg.2017.01747},
file = {:Users/carthach/Documents/Mendeley Desktop/fpsyg-08-01747.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Conceptual framework,Definitions,Music cognition,Psychoacoustics,Timbre perception},
number = {OCT},
pages = {6--9},
title = {{Four distinctions for the auditory "wastebasket" of timbre}},
volume = {8},
year = {2017}
}
@article{Tihelka2010,
abstract = {The paper describes the optimisation of Viterbi search used in unit selection TTS, since with a large speech corpus necessary to achieve a high level of naturalness, the performance still suffers. To improve the search speed, the combination of sophisticated stopping schemes and pruning thresholds is employed into the baseline search. The optimised search is, moreover, extremely flexible in configuration, requiring only three intuitively comprehensible coefficients to be set. This provides the means for tuning the search depending on device resources, while it allows reaching significant performance increase. To illustrate it, several configuration scenarios, with speed-up ranging from 6 to 58 times, are presented. Their impact on speech quality is verified by CCR listening test, taking into account only the phrases with the highest number of differences when compared to the baseline search. {\textcopyright} 2010 ISCA.},
author = {Tihelka, Daniel and Kala, Jiř{\'{i}} and Matou{\v{s}}ek, Jindřich},
doi = {10.1.1.148.7986},
file = {:Users/carthach/Documents/Mendeley Desktop/Tihelka, Kala, Matou{\v{s}}ek - 2010 - Enhancements of Viterbi search for fast unit selection synthesis.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {174--177},
title = {{Enhancements of Viterbi search for fast unit selection synthesis}},
year = {2010}
}
@article{Holland1975,
author = {Holland, John H},
publisher = {University of Michigan Press},
title = {{Adaptation in natural and artificial systems}},
year = {1975}
}
@inproceedings{Berry2001,
author = {Berry, Rodney and Rungsarityotin, Wasinee and Dorin, Alan},
booktitle = {Artificial Life Models for Music Applications},
file = {:Users/carthach/Documents/Mendeley Desktop/Berry, Rungsarityotin, Dorin - 2001 - Unfinished Symphonies-songs of 3 12 worlds.pdf:pdf},
title = {{Unfinished Symphonies-songs of 3 1/2 worlds}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Unfinished+Symphonies+?+songs+of+3+1+/+2+worlds{\#}0},
year = {2001}
}
@article{antonopoulos2007self,
author = {Antonopoulos, Iasonas and Pikrakis, Aggelos and Theodoridis, Sergios},
journal = {Journal of New Music Research},
number = {1},
pages = {27--38},
publisher = {Taylor {\&} Francis},
title = {{Self-similarity analysis applied on tempo induction from music recordings}},
volume = {36},
year = {2007}
}
@article{Sys-2017,
author = {Sys-, Live Concert and Agres, Kathleen and Forth, Jamie and Wang, Cheng-i and Hsu, Jennifer},
doi = {10.1145/3029374},
file = {:Users/carthach/Documents/Mendeley Desktop/Sys- et al. - 2017 - Special Issue on Musical Metacreation, Part II.pdf:pdf},
issn = {15443574},
journal = {Computers in Entertainment},
number = {3},
pages = {1--3},
title = {{Special Issue on Musical Metacreation, Part II}},
url = {http://dl.acm.org/citation.cfm?doid=3023312.3029374},
volume = {14},
year = {2017}
}
@phdthesis{frisson2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Frisson, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Frisson - 2015 - Designing interaction for browsing media collections (by similarity).pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
pmid = {15991970},
school = {Universit{\'{e}} de Mons},
title = {{Designing interaction for browsing media collections (by similarity)}},
year = {2015}
}
@article{Font2017,
address = {London, UK},
author = {Font, Frederic and Bandiera, Giuseppe},
journal = {Web Audio Conference (WAC)},
title = {{Freesound Explorer: Make Music While Discovering Freesound!}},
year = {2017}
}
@article{Wenger2011,
abstract = {Music and sound play an important role for the emotional involvement of the spectator in visual media such as movies, video games or artistic shows. However, the generation of suitable background music and atmospheric sound matching the visual context is a tedious and typically completely manual task. For content creators, it would be desirable to be able to specify a sound example and a few key points at which certain parts are to be played, and to then automatically generate a soundtrack subject to these constraints. For interactive media such as video games, on the other hand, example-based background sound could automatically adapt to the game events. We propose a new example-based audio synthesis method for both applications which works for musical and texture-like sound examples alike. Our algorithm is evaluated on a variety of musical styles, from classical music to punk rock, and on several texture-like atmospheric sounds.},
author = {Wenger, Stephan and Magnor, Marcus},
doi = {10.1109/ICME.2011.6011902},
file = {:Users/carthach/Documents/Mendeley Desktop/Wenger, Magnor - 2011 - Constrained example-based audio synthesis.pdf:pdf},
isbn = {9781612843490},
issn = {19457871},
journal = {Proceedings - IEEE International Conference on Multimedia and Expo},
keywords = {audio synthesis,example-based synthesis,sound textures},
title = {{Constrained example-based audio synthesis}},
year = {2011}
}
@inproceedings{Yen1972,
abstract = {This paper presents a new algorithm for finding the K loopless$\backslash$npaths from the origin to a sink that have the shortest lengths in the$\backslash$nnetwork. The significance of the new algorithm is that it requires fewer$\backslash$ncomputations than other available algorithms proposed by Bock, Kantner$\backslash$nand Haynes (1957), Pollack (1961), Clarke, Krikorian, and Rausan (1963),$\backslash$nSakarovitch (1968), and Yen (1971). A comparison of the efficiencies$\backslash$nof different algorithms is also presented.},
author = {Yen, Jin Y},
booktitle = {Bull. Operations Research Soc. of America},
pages = {B/185},
title = {{Another algorithm for finding the K shortest-loopless network paths}},
volume = {20},
year = {1972}
}
@phdthesis{Vogiatzoglou2016,
author = {Vogiatzoglou, Iakovos},
file = {:Users/carthach/Documents/Mendeley Desktop/IakovosVogiatzoglouThesis.pdf:pdf},
school = {Aalborg University, Copenhagen},
title = {{Application}},
year = {2016}
}
@article{Gresham-Lancaster1998,
abstract = {Discusses the aesthetic and performance history of the music group Hub. Information on the interactive computer network music of the group; Impact of changes in technology on the music of the group; Details of experiments with software synthesis and the Internet.},
author = {Gresham-Lancaster, Scot},
doi = {10.2307/1513398},
file = {:Users/carthach/Documents/Mendeley Desktop/1513398.pdf:pdf},
isbn = {0961-1215},
issn = {09611215},
journal = {Leonardo Music Journal},
keywords = {electronic music,internet,music {\&} technology},
number = {1},
pages = {39--44},
title = {{The Aesthetics and History of the Hub: The Effects of Changing Technology on Network Computer Music}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=aph{\&}AN=9152101{\&}site=ehost-live},
volume = {8},
year = {1998}
}
@article{Mongillo2009,
author = {Mongillo, David},
journal = {Pittsburgh Journal of Technology Law and Policy},
pages = {1},
publisher = {HeinOnline},
title = {{The Girl Talk Dilemma: Can Copyright Law Accommodate New Forms of Sample-Based Music}},
volume = {9},
year = {2009}
}
@article{Hennig2011,
abstract = {Although human musical performances represent one of the most valuable achievements of mankind, the best musicians perform imperfectly. Musical rhythms are not entirely accurate and thus inevitably deviate from the ideal beat pattern. Nevertheless, computer generated perfect beat patterns are frequently devalued by listeners due to a perceived lack of human touch. Professional audio editing software therefore offers a humanizing feature which artificially generates rhythmic fluctuations. However, the built-in humanizing units are essentially random number generators producing only simple uncorrelated fluctuations. Here, for the first time, we establish long-range fluctuations as an inevitable natural companion of both simple and complex human rhythmic performances. Moreover, we demonstrate that listeners strongly prefer long-range correlated fluctuations in musical rhythms. Thus, the favorable fluctuation type for humanizing interbeat intervals coincides with the one generically inherent in human musical performances.},
author = {Hennig, Holger and Fleischmann, Ragnar and Fredebohm, Anneke and Hagmayer, York and Nagler, Jan and Witt, Annette and Theis, Fabian J. and Geisel, Theo},
doi = {10.1371/journal.pone.0026457},
file = {:Users/carthach/Documents/Mendeley Desktop/Hennig et al. - 2011 - The nature and perception of fluctuations in human musical rhythms.pdf:pdf},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pmid = {22046289},
title = {{The nature and perception of fluctuations in human musical rhythms}},
volume = {6},
year = {2011}
}
@article{Eigenfeldt2016a,
author = {Eigenfeldt, Arne},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt - 2016 - Exploring Moment-form in Generative Music.pdf:pdf;:Users/carthach/Documents/Mendeley Desktop/MomentForminGenerativeMusic.pdf:pdf},
journal = {Proceedings of the Sound and Music Computing Conference},
number = {August},
title = {{Exploring Moment-form in Generative Music}},
year = {2016}
}
@article{Boden2000,
abstract = {Creativity isn't magical. It's an aspect of normal human intelligence, not a special faculty granted to a tiny elite. There are three forms: combinational, exploratory, and transformational. All three can be modeled by AI—in some cases, with impressive results. AI techniques underlie various types of computer art. Whether computers could “really” be creative isn't a scientific question but a philosophical one, to which there's no clear answer. But we do have the beginnings of a scientific understanding of creativity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boden, Margaret A.},
doi = {10.1609/aimag.v30i3.2254},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/2254-3076-1-PB.pdf:pdf},
isbn = {0 521 57284 1},
issn = {09528229},
journal = {AI Magazine},
number = {2},
pages = {72--76},
pmid = {25246403},
title = {{Computer models of creativity}},
volume = {13},
year = {2009}
}
@article{Schwarz2006b,
abstract = {Concatenative sound synthesis is a promising method of musical sound synthesis with a steady stream of work and publications for over ve years now. This article offers a comparative survey and taxonomy of the many different approaches to concatenative synthesis throughout the history of electronic music, starting in the 1950s, even if they weren't known as such at their time, up to the recent surge of contemporary methods. Concatenative sound synthesis methods use a large database of source sounds, segmented into units, and a unit selection algorithm that nds the units that match best the sound or musical phrase to be synthesised, called the target. The selection is performed according to the descriptors of the units. These are characteristics extracted from the source sounds, e.g. pitch, or attributed to them, e.g. instrument class. The selected units are then transformed to fully match the target specication, and concatenated. However, if the database is sufciently large, the probability is high that a matching unit will be found, so the need to apply transformations is reduced. The most urgent and interesting problems for further work on concatenative synthesis are listed concerning segmentation, descriptors, efficiency, legality, data mining, and real time interaction. Finally, the conclusion tries to provide some insight into the current and future state of concatenative synthesis research.},
author = {Schwarz, Diemo},
doi = {10.1080/09298210600696857},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz - 2006 - Concatenative Sound Synthesis The Early Years.pdf:pdf},
isbn = {0929-8215},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1},
pages = {3--22},
title = {{Concatenative Sound Synthesis: The Early Years}},
volume = {35},
year = {2006}
}
@book{Miranda2007a,
author = {Miranda, Eduardo R and Biles, Al John},
file = {:Users/carthach/Documents/Mendeley Desktop/Miranda, Biles - 2007 - Evolutionary Computer Music.pdf:pdf},
isbn = {9781846285998},
publisher = {Springer-Verlag New York, Inc.},
title = {{Evolutionary Computer Music}},
year = {2007}
}
@book{Russell2002,
abstract = {Artificial Intelligence: A Modern Approach introduces basic ideas in artificial intelligence from the perspective of building intelligent agents,which the authors define as "anything that can be viewed as perceiving its environment through sensors and acting upon the environment through effectors." This textbook is up-to-date and is organized using the latest principles of good textbook design. It includes historical notes at the end of every chapter, exercises, margin notes, a bibliography, and a competent index.Artificial Intelligence: A Modern Approach covers a wide array of material,including first-order logic, game playing, knowledge representation, planning,and reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Russell, Stuart and Norvig, Peter},
booktitle = {Prentice Hall},
doi = {10.1017/S0269888900007724},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Russell, Norvig - 2002 - Artificial Intelligence A Modern Approach (2nd Edition).pdf:pdf},
isbn = {0137903952},
issn = {00206539},
keywords = {artificial{\_}intelligence,computing},
pages = {1--85},
pmid = {20949757},
title = {{Artificial Intelligence: A Modern Approach (2nd Edition)}},
url = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20{\&}path=ASIN/0137903952},
year = {2002}
}
@article{Schwarz2000,
abstract = {In speech synthesis, concatenative data-driven synthesis methods$\backslash$nprevail. They use a database of recorded speech and a unit selection$\backslash$nalgorithm that selects the segments that match best the utterance$\backslash$nto be synthesized. Transferring these ideas to musical sound$\backslash$nsynthesis allows a new method of high quality sound synthesis.$\backslash$nUsual synthesis methods are based on a model of the sound signal.$\backslash$nIt is very difficult to build a model that would preserve the entire$\backslash$nfine details of sound. Concatenative synthesis achieves this by using$\backslash$nactual recordings. This data-driven approach (as opposed to a$\backslash$nrule-based approach) takes advantage of the information contained$\backslash$nin the many sound recordings. For example, very naturally sounding$\backslash$ntransitions can be synthesized, since unit selection is aware$\backslash$nof the context of the database units. The CATERPILLAR software$\backslash$nsystem has been developed to allow data-driven concatenative unit$\backslash$nselection sound synthesis. It allows high-quality instrument synthesis$\backslash$nwith high level control, explorative free synthesis from arbitrary$\backslash$nsound databases, or resynthesis of a recording with sounds$\backslash$nfrom the database. It is based on the new software-engineering$\backslash$nconcept of component-oriented software, increasing flexibility and$\backslash$nfacilitating reuse.},
author = {Schwarz, Diemo},
journal = {Digital Audio Effects (DAFx)},
pages = {97--102},
title = {{A system for data-driven concatenative sound synthesis}},
year = {2000}
}
@article{Hackbarth2010,
author = {Hackbarth, Benjamin},
file = {:Users/carthach/Documents/Mendeley Desktop/Hackbarth - 2011 - Audioguide A Framework for Creative Exploration of Concatenative Sound Synthesis.pdf:pdf},
journal = {IRCAM Research Report},
title = {{Audioguide : A Framework for Creative Exploration of Concatenative Sound Synthesis}},
year = {2011}
}
@phdthesis{Marin2014,
author = {Mar{\'{i}}n, Daniel G{\'{o}}mez},
file = {:Users/carthach/Documents/Mendeley Desktop/Mar{\'{i}}n - 2014 - Percussive spaces Definition and use of percussive spaces for analysis and interaction.pdf:pdf},
title = {{Percussive spaces: Definition and use of percussive spaces for analysis and interaction}},
year = {2014}
}
@inproceedings{Sioros2013,
author = {Sioros, George and Miron, Marius},
booktitle = {Proc. of the 10th International Symposium on Computer Music Multidisciplinary Research},
file = {:Users/carthach/Documents/Mendeley Desktop/Sioros, Miron - 2013 - Syncopalooza manipulating the syncopation in rhythmic performances.pdf:pdf},
keywords = {automatic rhythm,generation,meter,real time,rhythm,syncopation,transformations},
pages = {454--469},
title = {{Syncopalooza: manipulating the syncopation in rhythmic performances}},
url = {http://www.fabiengouyon.org/publis/sioros13cmmr.pdf},
year = {2013}
}
@article{Lukashevich2008a,
author = {Lukashevich, HM},
file = {:Users/carthach/Documents/Mendeley Desktop/Lukashevich - 2008 - Towards Quantitative Measures of Evaluating Song Segmentation.pdf:pdf},
journal = {ISMIR},
pages = {375--380},
title = {{Towards Quantitative Measures of Evaluating Song Segmentation.}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=OHp3sRnZD-oC{\&}oi=fnd{\&}pg=PA375{\&}dq=TOWARDS+QUANTITATIVE+MEASURES+OF+EVALUATING+SONG+SEGMENTATION{\&}ots=oDRSvClv83{\&}sig=STxA2yALKfQyZN1JpwjmbLt9oc4},
year = {2008}
}
@article{Glover2011a,
abstract = {Real-time musical note onset detection plays a vital role in many audio analysis processes, such as score following, beat detection and various sound synthesis by analysis methods. This article provides a review of some of the most commonly used techniques for real-time onset detection. We suggest ways to improve these techniques by incorporating linear prediction as well as presenting a novel algorithm for real-time onset detection using sinusoidal modelling. We provide comprehensive results for both the detection accuracy and the computational performance of all of the described techniques, evaluated using Modal, our new open source library for musical onset detection, which comes with a free database of samples with hand-labelled note onsets.},
author = {Glover, John and Lazzarini, Victor and Timoney, Joseph},
doi = {10.1186/1687-6180-2011-68},
file = {:Users/carthach/Documents/Mendeley Desktop/Glover, Lazzarini, Timoney - 2011 - Real-time detection of musical onsets with linear prediction and sinusoidal modeling(2).pdf:pdf},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
number = {1},
pages = {68},
title = {{Real-time detection of musical onsets with linear prediction and sinusoidal modeling}},
volume = {2011},
year = {2011}
}
@inproceedings{chloe,
author = {Turquois, Chlo{\'{e}} and Hermant, Martin and G{\'{o}}mez, Daniel and Jord{\`{a}}, Sergi},
booktitle = {ACM SIGIR Conference on Human Information Interaction and Retrieval},
file = {:Users/carthach/Documents/Mendeley Desktop/Turquois et al. - 2016 - Exploring the Benefits of 2D Visualizations for Drum Samples Retrieval.pdf:pdf},
isbn = {9781450337519},
keywords = {creativity,music interaction,spatial visualization,user studies},
pages = {329--332},
title = {{Exploring the Benefits of 2D Visualizations for Drum Samples Retrieval}},
year = {2016}
}
@inproceedings{Collins2002,
author = {Collins, Nick},
booktitle = {Proceedings of Cybersonica, Institute of Contemporary Arts, London},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2002 - Interactive evolution of breakbeat cut sequences.pdf:pdf},
keywords = {algorithmic composition,audio cutting,interactive genetic algorithms},
number = {Dahlstedt 2001},
title = {{Interactive evolution of breakbeat cut sequences}},
url = {http://community.dur.ac.uk/nick.collins/research/ieforbbcut.pdf},
year = {2002}
}
@phdthesis{Brossier2006,
author = {Brossier, Paul},
file = {:Users/carthach/Documents/Mendeley Desktop/Brossier - 2006 - Automatic annotation of musical audio for interactive applications.pdf:pdf},
school = {Queen Mary, University of London},
title = {{Automatic annotation of musical audio for interactive applications}},
url = {http://qmro.qmul.ac.uk/jspui/handle/123456789/3809},
year = {2006}
}
@article{Casey2008a,
abstract = {The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.},
author = {Casey, M.A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
doi = {10.1109/JPROC.2008.916370},
file = {:Users/carthach/Documents/Mendeley Desktop/04472077.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Audio signal processing,Helper,Internet,SectionIII: Bag-of-frames,SectionIV: music similarity,Survey,cognition,content-based music information retrieval,content-based retrieval,download services,information retrieval,metadata,music content,music downloading,music industry,music information retrieval,music perception,music streaming,music tracks,musicWorld-Wide Web,musicology,online music collections,online products,online services,semantic gap,sound,symbolic processing,user interfaces},
mendeley-tags = {Helper,Internet,SectionIII: Bag-of-frames,SectionIV: music similarity,Survey,cognition,content-based music information retrieval,content-based retrieval,download services,information retrieval,metadata,music content,music downloading,music industry,music information retrieval,music perception,music streaming,music tracks,musicWorld-Wide Web,musicology,online music collections,online products,online services,semantic gap,sound},
title = {{Content-Based Music Information Retrieval: Current Directions and Future Challenges}},
volume = {96},
year = {2008}
}
@article{Lacoste2007,
abstract = {This paper presents a novel approach to detecting onsets in music audio files. We use a supervised learning algorithm to classify spectrogram frames extracted from digital audio as being onsets or nononsets. Frames classified as onsets are then treated with a simple peak-picking algorithm based on a moving average. We present two versions of this approach. The first version uses a single neural network classifier. The second version combines the predictions of several networks trained using different hyperparameters. We describe the details of the algorithm and summarize the performance of both variants on several datasets. We also examine our choice of hyperparameters by describing results of cross-validation experiments done on a custom dataset. We conclude that a supervised learning approach to note onset detection performs well and warrants further investigation.},
author = {Lacoste, Alexandre and Eck, Douglas},
doi = {10.1155/2007/43745},
file = {:Users/carthach/Documents/Mendeley Desktop/Lacoste, Eck - 2007 - A supervised classification algorithm for note onset detection.pdf:pdf},
issn = {11108657},
journal = {Eurasip Journal on Advances in Signal Processing},
title = {{A supervised classification algorithm for note onset detection}},
volume = {2007},
year = {2007}
}
@article{Lim2012,
abstract = {Corpus based speech synthesis can produce high quality synthetic speech due to it high sensitivity to unit context. Large speech database is embedded in synthesis system and search algorithm (unit selection) is needed to search for the optimal unit sequence. Speech feature which served as target cost is estimated from the input text. The acoustic parameters which served as join cost are derived from mel frequency cepstral coefficients (MFCCs) and Euclidean distance. In this paper, a new method which is Genetic Algorithm is proposed to search for optimal unit sequence. Genetic Algorithm (GA) is a population based search algorithm that is based on the biological principles of selection, reproduction, crossover and mutation. It is a stochastic search algorithm for solving optimization problem. The speech unit sequence that has minimum join cost will be synthesized into complete waveform data. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Lim, Yee Chea and Tan, Tian Swee and {Shaikh Salleh}, Sheikh Hussain and Ling, Dandy Kwong},
doi = {10.1016/j.eswa.2011.11.047},
file = {:Users/carthach/Documents/Mendeley Desktop/Lim et al. - 2012 - Application of Genetic Algorithm in unit selection for Malay speech synthesis system.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Corpus based,Genetic Algorithm,Speech concatenation,Unit selection},
number = {5},
pages = {5376--5383},
publisher = {Elsevier Ltd},
title = {{Application of Genetic Algorithm in unit selection for Malay speech synthesis system}},
url = {http://dx.doi.org/10.1016/j.eswa.2011.11.047},
volume = {39},
year = {2012}
}
@inproceedings{Tian2016,
author = {Tian, Mi and Sandler, Mark B},
booktitle = {17th International Society for Music Information Retrieval Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Tian Music Structural Segmentation 2016 Accepted.pdf:pdf},
title = {{Music Structural Segmentation Across Genres with Gammatone Features}},
year = {2016}
}
@article{Laroche2003,
abstract = {Automatic beat tracking consists of estimating the number of beats per minutes at which a music track is played and identifying exactly when these beats occur. Applications range from music analysis, sound-effect synchronization, and audio editing to automatic playlist generation and deejaying. An off-line beat-tracking technique for estimating a time-varying tempo in an audio track is presented. The algorithm uses an MMSE estimation of local tempo and beat location candidates, followed by a dynamic programming stage used to determine the optimum choice of candidate in each analysis frame. The algorithm is efficient in its use of computation resource, yet provides very good results on a wide range of audio tracks. The algorithm details are presented, followed by a discussion of the performance and suggestions for further improvements.},
author = {Laroche, Jean},
file = {:Users/carthach/Documents/Mendeley Desktop/Laroche - 2003 - Efficient Tempo and Beat Tracking in Audio Recordings.pdf:pdf},
isbn = {0004-7554},
issn = {00047554},
journal = {Journal of the Audio Engineering Society},
number = {4},
pages = {226--233},
title = {{Efficient Tempo and Beat Tracking in Audio Recordings}},
url = {http://www.aes.org/e-lib/browse.cfm?elib=12235},
volume = {51},
year = {2003}
}
@misc{Ellis2010,
author = {Ellis, D P and Bilmes, J A and Fosler-Lussier, E and Hermansky, H and Johnson, D and Kingsbury, B and Morgan, N},
title = {{The SPRACHcore software package}},
year = {2010}
}
@article{Goksu,
author = {G{\"{o}}ksu, H{\"{u}}seyin and Pigg, Paul and Dixit, Vikas},
file = {:Users/carthach/Documents/Mendeley Desktop/G{\"{o}}ksu, Pigg, Dixit - Unknown - Music Composition Using Genetic Algorithms ( GA ) and Multilayer Perceptrons ( MLP ).pdf:pdf},
pages = {1242--1250},
title = {{Music Composition Using Genetic Algorithms ( GA ) and Multilayer Perceptrons ( MLP )}}
}
@article{Pearce2001,
author = {Pearce, Marcus and Wiggins, Geraint},
file = {:Users/carthach/Documents/Mendeley Desktop/Pearce, Wiggins - 2001 - Towards a framework for the evaluation of machine compositions.pdf:pdf},
journal = {Proceedings of the AISB'01 Symposium on AI and Creativity in Arts and Science},
title = {{Towards a framework for the evaluation of machine compositions}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.3026{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@article{Figueiro2009,
author = {Figueir{\'{o}}, C},
file = {:Users/carthach/Documents/Mendeley Desktop/Figueir{\'{o}} - 2009 - Detection of rhythmic patterns in real time with Pd.pdf:pdf},
journal = {3rd pd International Convention},
keywords = {interaction,pattern recognition,pure data},
title = {{Detection of rhythmic patterns in real time with Pd}},
url = {https://pd-graz.mur.at/community/conventions/convention09/figueiro.pdf},
year = {2009}
}
@article{Eppstein2014,
abstract = {We survey {\$}k{\$}-best enumeration problems and the algorithms for solving them, including in particular the problems of finding the {\$}k{\$} shortest paths, {\$}k{\$} smallest spanning trees, and {\$}k{\$} best matchings in weighted graphs.},
archivePrefix = {arXiv},
arxivId = {1412.5075},
author = {Eppstein, David},
doi = {MY/arxiv_1412.5075},
eprint = {1412.5075},
file = {:Users/carthach/Documents/Mendeley Desktop/Eppstein - 2014 - K-Best Enumeration.pdf:pdf},
pages = {1--17},
title = {{K-Best Enumeration}},
url = {http://arxiv.org/abs/1412.5075},
year = {2014}
}
@book{Goldberg1989,
author = {Goldberg, David E},
publisher = {Addison-Wesley Publishing Company},
title = {{Genetic algorithms in search optimization and machine learning}},
year = {1989}
}
@misc{Myers,
author = {Myers, Amy N},
file = {:Users/carthach/Documents/Mendeley Desktop/Myers - 2015 - Are Venn Diagrams Limited to Three or Fewer Sets.pdf:pdf},
title = {{Are Venn Diagrams Limited to Three or Fewer Sets ?}},
url = {http://www.brynmawr.edu/math/people/anmyers/PAPERS/Venn.pdf},
urldate = {2015-09-25},
year = {2015}
}
@book{hook2009hacienda,
author = {Hook, Peter},
publisher = {Simon and Schuster},
title = {{The Hacienda: how not to run a club}},
year = {2009}
}
@conference{Faraldo2016,
abstract = {In this paper we study key estimation in electronic dance music, an umbrella term referring to a variety of electronic music sub- genres intended for dancing at nightclubs and raves. We start by defining notions of tonality and key before outlining the basic architecture of a template-based key estimation method. Then, we report on the tonal characteristics of electronic dance music, in order to infer possible mod- ifications of the method described. We create new key profiles combin- ing these observations with corpus analysis, and add two pre-processing stages to the basic algorithm. We conclude by comparing our profiles to existing ones, and testing our modifications on independent datasets of pop and electronic dance music, observing interesting improvements in the performance or our algorithms, and suggesting paths for future research.},
address = {Padua, Italy},
author = {Faraldo, {\'{A}}ngel and G{\'{o}}mez, Emilia and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {38th European Conference on Information Retrieval},
doi = {10.1007/978-3-319-30671-1},
isbn = {9783319306711},
keywords = {Electronic Dance Music,Key Profiles,Music Information Retrieval. Computational Key Est,music theory,tonality},
organization = {Springer-Verlag},
pages = {335--347},
publisher = {Springer-Verlag},
title = {{Key Estimation in Electronic Dance Music}},
url = {http://hdl.handle.net/10230/33060},
year = {2016}
}
@article{Pease2011,
abstract = {Computational Creativity is the AI subfield in which we$\backslash$r$\backslash$nstudy how to build computational models of creative thought in science$\backslash$r$\backslash$nand the arts. From an engineering perspective, it is desirable to$\backslash$r$\backslash$nhave concrete measures for assessing the progress made from one$\backslash$r$\backslash$nversion of a program to another, or for comparing and contrasting$\backslash$r$\backslash$ndifferent software systems for the same creative task. We describe$\backslash$r$\backslash$nthe Turing Test and versions of it which have been used in order$\backslash$r$\backslash$nto measure progress in Computational Creativity. We show that the$\backslash$r$\backslash$nversions proposed thus far lack the important aspect of interaction,$\backslash$r$\backslash$nwithout which much of the power of the Turing Test is lost.We argue$\backslash$r$\backslash$nthat the Turing Test is largely inappropriate for the purposes of evaluation$\backslash$r$\backslash$nin Computational Creativity, since it attempts to homogenise$\backslash$r$\backslash$ncreativity into a single (human) style, does not take into account the$\backslash$r$\backslash$nimportance of background and contextual information for a creative$\backslash$r$\backslash$nact, encourages superficial, uninteresting advances in front-ends, and$\backslash$r$\backslash$nrewards creativity which adheres to a certain style over that which$\backslash$r$\backslash$ncreates something which is genuinely novel. We further argue that$\backslash$r$\backslash$nalthough there may be some place for Turing-style tests for Computational$\backslash$r$\backslash$nCreativity at some point in the future, it is currently untenable$\backslash$r$\backslash$nto apply any defensible version of the Turing Test.$\backslash$r$\backslash$nAs an alternative to Turing-style tests, we introduce two descriptive$\backslash$r$\backslash$nmodels for evaluating creative software, the FACE model which$\backslash$r$\backslash$ndescribes creative acts performed by software in terms of tuples of$\backslash$r$\backslash$ngenerative acts, and the IDEA model which describes how such creative$\backslash$r$\backslash$nacts can have an impact upon an ideal audience, given ideal$\backslash$r$\backslash$ninformation about background knowledge and the software development$\backslash$r$\backslash$nprocess. While these models require further study and elaboration,$\backslash$r$\backslash$nwe believe that they can be usefully applied to current systems$\backslash$r$\backslash$nas well as guiding further development of creative systems.},
author = {Pease, Alison and Colton, Simon},
file = {:Users/carthach/Documents/Mendeley Desktop/pease{\_}aisb11.pdf:pdf},
isbn = {9781908187031 (ISBN)},
journal = {Proceedings of the AISB symposium on AI {\ldots},},
pages = {39},
title = {{On impact and evaluation in computational creativity: a discussion of the turing test and an alternative proposal}},
url = {ccg.doc.gold.ac.uk},
year = {2011}
}
@article{Reid2002,
author = {Reid, Gordon},
journal = {Sound on Sound, July},
title = {{Synth Secrets: Practical Bass Drum Synthesis}},
year = {2002}
}
@article{Gomez2006a,
abstract = {We present a method to extract a description of the tonal aspects of music from polyphonic audio signals . We define this tonal description using different levels of abstraction, differentiating between low-level signal descriptors and high-level textual labels. We also establish diverse emporal scales for description, defining some features as being attached to a certain time instant, and other global descriptors as related to a wider segment. The description is validated by estimating the key of a piece. We also propose the description as a tonal representation of the polyphonic audio signal to measure tonal similarity between audio excerpts and to establish the tonal structure of a musical piece.},
author = {G{\'{o}}mez, E.},
doi = {10.1287/ijoc.1040.0126},
file = {:Users/carthach/Documents/Mendeley Desktop/ijoc.1040.0126.pdf:pdf},
isbn = {1091-9856},
issn = {1091-9856},
journal = {INFORMS Journal on Computing},
keywords = {a great amount of,accepted by elaine chew,accepted november 2004,audio material,cluster on computation in,content analysis and indexing,february 2004,guest editor for special,history,in the last few,key estimation,music,received,revised june 2004,sound and music computing,tonal description,years},
number = {3},
pages = {294--304},
title = {{Tonal Description of Polyphonic Audio for Music Content Processing}},
volume = {18},
year = {2006}
}
@book{Manzo2015,
author = {Manzo, V J and Kuhn, Will},
publisher = {Oxford University Press, USA},
title = {{Interactive composition: Strategies using Ableton live and max for live}},
year = {2015}
}
@article{McGranahan2010,
author = {McGranahan, Liam},
journal = {Revista Transcultural de M{\'{u}}sica (TRANS)},
number = {14},
publisher = {Sociedad de Etnomusicolog{\{}$\backslash$'$\backslash$i{\}}a},
title = {{Bastards and booties: Production, copyright, and the mashup community}},
year = {2010}
}
@article{Bilmes1993,
abstract = {This thesis has one main goal: design algorithms that computers can use to produce expressive $\backslash$nsounding rhythmic phrases. First, I describe four elements that can characterize musical rhythm: $\backslash$nmetric structure, tempo variation, deviations, and ametric phrases. The first three elements can be $\backslash$nused successfully to model percussive rhythm. Second, I describe two algorithms: one, an automatic transcription algorithm, extracts stroke attack times and automatically constructs unique stroke types from a percussive performance. The other takes a percussive performance and factors out the metric structure, tempo variation, and deviations. Third, I apply these algorithms to a performance given by the percussion group Los Munequitos de Matanzas. Using both a synthesis of the performance and statistical analysis, I $\backslash$ndemonstrate that timing data represented in this form is not random and is in fact meaningful. In a $\backslash$nsynthesis with tempo variation removed but deviations retained, the original performance's expressive feel is preserved. Therefore, I claim that rhythmic analysis requires the study of both tempo variation and deviations. Finally, because similar quantized rhythmic phrases have similar corresponding deviations, the smoothness assumption necessary for a function approximation approach to learning is satisfied. I describe a multi-stage clustering algorithm that locates sets of similar quantized phrases in a performance, I then describe a machine learning algorithm that can build a mapping between quantized phrases and deviations. This algorithm can be used to apply deviations to new phrases. I claim that deviations are most important for the expressive feel of percussive music. Therefore, I have developed a new drum machine interface, a deviation experimentation program, with $\backslash$nwhich deviations can be explored.$\backslash$n},
author = {Bilmes, J.a.},
file = {:Users/carthach/Documents/Mendeley Desktop/bilmes1993-mit-thesis.pdf:pdf},
journal = {Sciences-New York},
keywords = {bilmes, J. Bilmes, thesis, computational music, rh},
title = {{Timing is of the essence: Perceptual and computational techniques for representing, learning, and reproducing expressive timing in percussive rhythm}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Timing+is+of+the+Essence:+Perceptual+and+Computational+Techniques+for+Representing,+Learning,+and+Reproducing+Expressive+Timing+in+Percussive+Rhythm{\#}0},
year = {1993}
}
@article{Jorda1991,
abstract = {PITEL is a software environment for polyphonic real-time composition and improvisation, based on non-linear recurrence. It is written in the language MAX on the Macintosh II, and can generate, under the control of a mouse conductor or composer, up to eight voices in the form of MIDI data, while optionally listening and reacting to one or more MIDI players.},
author = {Jorda, Sergi},
file = {:Users/carthach/Documents/Mendeley Desktop/A{\_}Real-Time{\_}MIDI{\_}Composer{\_}and{\_}Interactive{\_}Improvis.pdf:pdf},
journal = {International Computer Music Conference},
number = {January 1991},
title = {{A Real-Time Midi Composer And Interactive Improviser By Means Of Feedback Systems}},
year = {1991}
}
@article{Arar2013,
author = {Arar, Raphael and Kapur, Ajay},
file = {:Users/carthach/Documents/Mendeley Desktop/A HISTORY OF SEQUENCERS INTERFACES FOR ORGANIZING PATTERN-BASED MUSIC.pdf:pdf},
isbn = {978-3-8325-3472-1},
journal = {Proceedings of the Sound and Music Computing Conference},
pages = {383--388},
title = {{A History of Sequencers: Interfaces for Organizing Pattern-Based Music}},
url = {http://smcnetwork.org/system/files/A HISTORY OF SEQUENCERS INTERFACES FOR ORGANIZING PATTERN-BASED MUSIC.pdf},
volume = {2},
year = {2013}
}
@article{Foote1999,
author = {Foote, Jonathan},
file = {:Users/carthach/Documents/Mendeley Desktop/Foote - 1999 - Visualizing music and audio using self-similarity.pdf:pdf},
journal = {Proceedings of the seventh ACM international {\ldots}},
title = {{Visualizing music and audio using self-similarity}},
url = {http://dl.acm.org/citation.cfm?id=319472},
year = {1999}
}
@article{Oliver2015,
author = {Oliver, Rowan},
journal = {African American culture and society after Rodney King},
pages = {177--192},
title = {{Breakbeat syncretism: The drum sample in African American popular music}},
year = {2015}
}
@article{Gulati2016,
abstract = {Automatically describing contents of recorded music is crucial for interacting with large volumes of audio recordings, and for developing novel tools to facilitate music pedagogy. Melody is a fundamental facet in most music traditions and, therefore, is an indispensable component in such description. In this thesis, we develop computational approaches for analyzing high-level melodic aspects of music performances in Indian art music (IAM), with which we can describe and interlink large amounts of audio recordings. With its complex melodic framework and well-grounded theory, the description of IAM melody beyond pitch contours offers a very interesting and challenging research topic. We analyze melodies within their tonal context, identify $\backslash$nmelodic patterns, compare them both within and across music pieces, and finally, characterize the specific melodic context of IAM, the r{\{}$\backslash$={\{}a{\}}{\}}gas. All these analyses are done using data-driven methodologies on sizable curated music corpora. Our work paves the way for addressing several interesting research problems in the field of music information research, as well as developing novel applications in the context of music discovery and music pedagogy. $\backslash$n The thesis starts by compiling and structuring largest to date music corpora of the two IAM traditions, Hindustani and Carnatic music, comprising quality audio recordings and the associated metadata. From them we extract the predominant pitch and normalize by the tonic context. An important element to describe melodies is the identification of the meaningful temporal units, for which we propose to detect occurrences of ny{\{}$\backslash$={\{}a{\}}{\}}s svaras in Hindustani music, a landmark that demarcates musically salient melodic patterns. $\backslash$n Utilizing these melodic features, we extract musically relevant recurring melodic patterns. These patterns are the building blocks of melodic structures in both improvisation and composition. Thus, they are fundamental to the description of audio collections in IAM. We propose an unsupervised approach that employs time-series analysis tools to discover melodic patterns in sizable music collections. We first carry out an in-depth supervised analysis of melodic similarity, which is a critical component in pattern discovery. We then improve upon the best possible competing approach by $\backslash$nexploiting peculiar melodic characteristics in IAM. To identify musically meaningful patterns, we exploit the relationships between the discovered patterns by performing a network analysis. Extensive listening tests by professional musicians reveal that the discovered melodic patterns are musically interesting and significant. $\backslash$n Finally, we utilize our results for recognizing r{\{}$\backslash$={\{}a{\}}{\}}gas in recorded performances of IAM. We propose two novel approaches that jointly capture the tonal and the temporal aspects of melody. Our first approach uses melodic patterns, the most prominent cues for r{\{}$\backslash$={\{}a{\}}{\}}ga identification by humans. We utilize the discovered melodic patterns and employ topic modeling techniques, wherein we regard a r{\{}$\backslash$={\{}a{\}}{\}}ga rendition similar to a textual description of a topic. In our second approach, we propose the time delayed melodic surface, a novel feature based on delay coordinates that captures the melodic outline of a r{\{}$\backslash$={\{}a{\}}{\}}ga. With these approaches we demonstrate unprecedented accuracies in r{\{}$\backslash$={\{}a{\}}{\}}ga recognition on the largest datasets ever used for this task. Although our approach is guided by the characteristics of melodies in IAM and the task at hand, we believe our methodology can be easily extended to other melody dominant music traditions. $\backslash$n Overall, we have built novel computational methods for analyzing several melodic aspects of recorded performances in IAM, with which we describe and interlink large amounts of music recordings. In this process we have developed several tools and compiled data that can be used for a number of computational studies in IAM, specifically in characterization of r{\{}$\backslash$={\{}a{\}}{\}}gas, compositions and artists. The technologies resulted from this research work are a part of several applications developed within the CompMusic project for a better description, enhanced listening experience, and pedagogy in IAM. $\backslash$n},
author = {Gulati, Sankalp},
file = {:Users/carthach/Documents/Mendeley Desktop/Gulati - 2016 - Computational Approaches for Melodic Description in Indian Art Music Corpora.pdf:pdf},
keywords = {CompMusic,Indian art music,Melody,Nyas,Time series,Tonic identification,audio analysis,carnatic,chalan,computational analysis,dtw,dynamic time warping,hindustani,melodic description,melodic patterns,motifs,music information retrieval,pakad,pattern network,patterns,phrases,raaga,raga,raga recognition,svara,time delayed melodic surface,vector space modeling},
pages = {305},
title = {{Computational Approaches for Melodic Description in Indian Art Music Corpora}},
year = {2016}
}
@inproceedings{Thompson2014,
author = {Thompson, Lucas and Dixon, Simon and Mauch, Matthias},
booktitle = {International Society for Music Information Retrieval Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Thompson, Dixon, Mauch - 2014 - Drum Transcription via Classification of Bar-Level Rhythmic Patterns.pdf:pdf},
pages = {187--192},
title = {{Drum Transcription via Classification of Bar-Level Rhythmic Patterns}},
year = {2014}
}
@phdthesis{Coleman2015,
author = {Coleman, Graham},
file = {:Users/carthach/Documents/Mendeley Desktop/Coleman - 2015 - Descriptor Control of Sound Transformations and Mosaicing Synthesis.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Descriptor Control of Sound Transformations and Mosaicing Synthesis}},
year = {2015}
}
@article{Bernardes2010,
author = {Bernardes, Gilberto},
file = {:Users/carthach/Documents/Mendeley Desktop/Bernardes - 2010 - Style Emulation of Drum Patterns by Means of Evolutionary Methods and Statistical Analysis.pdf:pdf},
journal = {Proceedings of the Sound and Music Computing Conference},
pages = {1--4},
title = {{Style Emulation of Drum Patterns by Means of Evolutionary Methods and Statistical Analysis}},
url = {http://smcnetwork.org/files/proceedings/2010/26.pdf},
year = {2010}
}
@book{Hatch1987,
abstract = {Denial of the values of such a tradition: for them may mean the of protest we are prepared to argue for a folk-pop continuum as being the most appropriate of our It stated: Folk is the product of a tradition that has been evolved through the},
author = {Hatch, D and Millward, S},
isbn = {0719023491},
pmid = {532379},
publisher = {Manchester University Press},
title = {{From blues to rock: an analytical history of pop music}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=NS7pAAAAIAAJ{\&}oi=fnd{\&}pg=PP11{\&}dq=sound+descriptor+music+musical+analysis+{\&}ots=jey4Ucusmo{\&}sig=ixB32CXpR12BgPgHyKWsT49FzWc},
year = {1987}
}
@article{Milne2016,
abstract = {We present an application XronoMorph for the
algorithmic generation of rhythms in the context of creative composition and
performance, and of musical analysis and education. XronoMorph makes use of
visual and geometrical conceptualizations of rhythms, and allows the user to
smoothly morph between rhythms. Sonification of the user generated geometrical
constructs is possible using a built-in sampler, VST and AU plugins, or
standalone synthesizers via MIDI. The algorithms are based on two underlying
mathematical principles: perfect balance and well-formedness, both of which can
be derived from coefficients of the discrete Fourier transform of the rhythm. The
mathematical background, musical implications, and their implementation in the
software are discussed.},
author = {Milne, Andrew J and Herff, Steffen A and Bulger, David and Sethares, William A and Dean, Roger T},
file = {:Users/carthach/Documents/Mendeley Desktop/Milne et al. - 2016 - XronoMorph Algorithmic Generation of Perfectly Balanced and Well-Formed Rhythms.pdf:pdf},
isbn = {978-1-925455-13-7},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {388--393},
title = {{XronoMorph: Algorithmic Generation of Perfectly Balanced and Well-Formed Rhythms}},
url = {http://www.nime.org/proceedings/2016/nime2016{\_}paper0077.pdf},
volume = {16},
year = {2016}
}
@article{Wanderley2002,
author = {Wanderley, Marcelo Mortensen and Orio, Nicola},
doi = {10.1162/014892602320582981},
file = {:Users/carthach/Documents/Mendeley Desktop/Wanderley, Orio - 2002 - Evaluation of Input Devices for Musical Expression Borrowing Tools from HCI.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
month = {sep},
number = {3},
pages = {62--76},
title = {{Evaluation of Input Devices for Musical Expression: Borrowing Tools from HCI}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/014892602320582981},
volume = {26},
year = {2002}
}
@inproceedings{Sturm2004,
author = {Sturm, Bob L.},
booktitle = {7th International Conference On Digital Audio Effects (DAFx)},
file = {:Users/carthach/Documents/Mendeley Desktop/Sturm - 2004 - Matconcat An Application for Exploring Concatenative Sound Synthesis Using Matlab.pdf:pdf},
pages = {323--326},
title = {{Matconcat: An Application for Exploring Concatenative Sound Synthesis Using Matlab}},
url = {http://dafx04.na.infn.it/WebProc/Proc/P{\_}323.pdf},
year = {2004}
}
@inproceedings{Hockman2015,
author = {Hockman, Jason A. and Davies, Matthew E. P.},
booktitle = {Proc. of the 18th Int. Conference on Digital Audio Effects (DAFx-15)},
file = {:Users/carthach/Documents/Mendeley Desktop/Hockman, Davies - 2015 - Computational Strategies for Breakbeat Classification and Resequencing in Hardcore, Jungle and Drum {\&} Bass.pdf:pdf},
pages = {1--6},
title = {{Computational Strategies for Breakbeat Classification and Resequencing in Hardcore, Jungle and Drum {\&} Bass}},
year = {2015}
}
@article{Conti2013,
abstract = {A review is presented of the book "Everyday Tonality: Towards a Tonal Theory of What Most People Hear," by Philip Tagg.},
author = {Conti, Jacopo},
doi = {10.5429/2079-3871(2013)v3i2.9en},
issn = {20793871},
journal = {IASPM@Journal},
keywords = {ISBN: 97609760188443},
number = {2},
pages = {111--112},
title = {{Everyday Tonality: Towards a Tonal Theory of What Most People Hear. By Philip Tagg}},
url = {http://www.iaspmjournal.net/index.php/IASPM{\_}Journal/article/view/594},
volume = {3},
year = {2013}
}
@book{collins_schedel_wilson_2013,
author = {Collins, Nick and Schedel, Margaret and Wilson, Scott},
doi = {10.1017/CBO9780511820540},
publisher = {Cambridge University Press},
series = {Cambridge Introductions to Music},
title = {{Electronic Music}},
year = {2013}
}
@article{Pachet2011,
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1007/s10601-010-9101-4},
file = {:Users/carthach/Documents/Mendeley Desktop/pachet-09c.pdf:pdf},
issn = {1383-7133},
journal = {Constraints},
keywords = {Computer Science},
number = {2},
pages = {148--172},
publisher = {Springer Netherlands},
title = {{Markov constraints: steerable generation of Markov sequences}},
url = {http://dx.doi.org/10.1007/s10601-010-9101-4},
volume = {16},
year = {2011}
}
@book{Miller2008,
author = {Miller, Paul D.},
publisher = {MIT Press},
title = {{Sound unbound: Sampling digital music and culture}},
year = {2008}
}
@inproceedings{Xu2003,
author = {Xu, Changsheng and Maddage, Namunu C and Shao, Xi and Cao, Fang and Tian, Qi},
booktitle = {Acoustics, Speech, and Signal Processing, 2003 (ICASSP'03)},
file = {:Users/carthach/Documents/Mendeley Desktop/01199998.pdf:pdf},
isbn = {0780376633},
pages = {429--432},
title = {{Musical Gnere Classification Using Support Vector Machines}},
year = {2003}
}
@misc{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, Lawrence R.},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Rabiner - 1989 - A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.pdf:pdf},
isbn = {1558601244},
issn = {15582256},
number = {2},
pages = {257--286},
pmid = {21920608},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
volume = {77},
year = {1989}
}
@article{Johnson-Roberson2017,
author = {Johnson-Roberson, Cora and Sudderth, Erik},
file = {:Users/carthach/Documents/Mendeley Desktop/johnsonroberson.cora.pdf:pdf},
pages = {8},
title = {{Content-Based Genre Classification and Sample Recognition Using Topic Models}},
year = {2017}
}
@article{Puckette2004,
author = {Puckette, Miller},
file = {:Users/carthach/Documents/Mendeley Desktop/icmc04.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {406--408},
title = {{Low-dimensional parameter mapping using spectral envelopes}},
volume = {2004},
year = {2004}
}
@article{Slaney1998,
author = {Slaney, Malcolm},
journal = {Interval Research Corporation, Tech. Rep},
pages = {1998},
title = {{Auditory toolbox}},
volume = {10},
year = {1998}
}
@article{Brazil2002,
abstract = {Collections of sound and music of increasing size and diversity are used both by typical computer users and multimedia designers. Browsing audio collections poses several challenges to the design of effective user interfaces. Recent techniques in audio information retrieval allow the automatic extraction of audio content information. This information can be used to inform and enhance audio browsing tools. In this paper we describe how audio information retrieval can be utilized to create novel user interfaces for browsing of audio collections. More specifically we report on recent work on two system prototypes: the Sonic Browser and Marsyas and our current work on merging the two systems in a common flexible system.},
author = {Brazil, Eoin and Fernstr{\"{o}}m, Mikael},
file = {:Users/carthach/Documents/Mendeley Desktop/BrazilFernstroem2002.pdf:pdf},
journal = {International Conference on Auditory Display},
keywords = {2002,audio information retrieval,ceedings of the 2002,display,enhancing sonic browsing using,eoin brazil,international conference on auditory,japan,july 2-5,kyoto,mikael fernstr{\"{o}}m},
pages = {1--6},
title = {{Enhancing sonic browsing using audio information retrieval}},
url = {http://dev.icad.org/websiteV2.0/Conferences/ICAD2002/proceedings/17{\_}EoinBrazil.pdf},
year = {2002}
}
@article{Kaliakatsos-Papakostas2012,
abstract = {Music composition with algorithms inspired by nature has led to the creation of systems that compose music with rich characteristics. Nevertheless, the complexity imposed by unsupervised algorithms may arguably be considered as undesired, especially when considering the composition of rhythms. This work examines the composition of rhythms through L and Finite L-systems (FL-systems) and presents an interpretation from grammatical to rhythmic entities that expresses the repetitiveness and diversity of the output of these systems. Furthermore, we utilize a supervised training scheme that uses Genetic Algorithms (GA) to evolve the rules of L and FL-systems, so that they may compose rhythms with certain characteristics. Simple rhythmic indicators are introduced that describe the density, pauses, self similarity, symmetry and syncopation of rhythms. With fitness evaluations based on these indicators we assess the performance of L and FL-systems and present results that indicate the superiority of the FL-system in terms of adaptability to certain rhythmic tasks. Copyright 2012 ACM.},
author = {Kaliakatsos-Papakostas, Maximos A. and Floros, Andreas and Kanellopoulos, Nikolaos and Vrahatis, Michael N.},
doi = {10.1145/2330784.2330855},
file = {:Users/carthach/Documents/Mendeley Desktop/Kaliakatsos-Papakostas et al. - 2012 - Genetic evolution of L and FL-systems for the production of rhythmic sequences.pdf:pdf},
isbn = {9781450311786},
journal = {Proceedings of the fourteenth international conference on Genetic and evolutionary computation conference companion - GECCO Companion '12},
keywords = {FL-systems,Genetic Algorithms,L-systems,Rhythm,Rhythmic indicators},
pages = {461},
title = {{Genetic evolution of L and FL-systems for the production of rhythmic sequences}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84864995990{\&}partnerID=tZOtx3y1},
year = {2012}
}
@article{Roads1985,
author = {Roads, Curtis},
publisher = {The MIT Press},
title = {{Granular synthesis of sound}},
year = {1985}
}
@article{Roman2008,
author = {Rom{\'{a}}n, By Dan},
file = {:Users/carthach/Documents/Mendeley Desktop/Rom{\'{a}}n - 2008 - Twelve-Tone Technique A Quick Reference.pdf:pdf},
journal = {Musik{\'{e}}, The online journal of the Conservatory of Music of Puerto Rico},
title = {{Twelve-Tone Technique : A Quick Reference}},
year = {2008}
}
@book{Sethares2007,
abstract = {Anyone interested in the scientific basis of music from psychologists to the designers of electronic musical instruments will be interested in this book.},
author = {Sethares, William A.},
booktitle = {Rhythm and Transforms},
doi = {10.1007/978-1-84628-640-7},
file = {:Users/carthach/Documents/Mendeley Desktop/Sethares - 2007 - Rhythm and transforms.pdf:pdf},
isbn = {9781846286391},
issn = {18650929},
pages = {1--336},
pmid = {1721458},
title = {{Rhythm and transforms}},
year = {2007}
}
@article{Scott2014,
abstract = {This paper briefly examines a number of the demands placed on composers working in the genre of game music. Continual improvements to music software enable composers to have more control of their music but this requires a greater knowledge of software environments and a different compositional skill set. Music composers are a specialized group of people who have the power to manipulate the emotive contexts of individuals through sonic engagement. Composers often communicate to the listener using a standardized musical language but they come from a diverse range of backgrounds and possess different ideologies across a broad range of music genres. Nonetheless, irrespective of the instrumentation chosen, the sound generation used, the textural qualities employed or the notes selected, they are able to produce creative works that are effective and integral to the success of game soundtracks. Playing the right music at the right time is important to the emotive impact and overall satisfaction of gameplay. Writing music to accompany media has it's own peculiarities. Horowitz and Looney [10] indicate that " When left alone with nothing to look at and no deadlines to meet, a composer will, in most cases, write very different music from the music that same},
author = {Scott, Nathan},
doi = {10.1145/2677758.2677792},
file = {:Users/carthach/Documents/Mendeley Desktop/34-scott.pdf:pdf},
isbn = {9781450327909},
journal = {Proceedings of the 2014 Conference on Interactive Entertainment - IE2014},
keywords = {Music General Terms Design Keywords Music,composition,game,software,sound},
pages = {1--3},
title = {{Music to Middleware: The Growing Challenges of the Game Music Composer}},
year = {2014}
}
@book{koza1992genetic,
author = {Koza, John R},
publisher = {MIT press},
title = {{Genetic programming: on the programming of computers by means of natural selection}},
volume = {1},
year = {1992}
}
@incollection{Veltkamp2008,
author = {Veltkamp, Remco C and Wiering, Frans and Typke, Rainer},
booktitle = {Encyclopedia of Multimedia},
pages = {97--98},
publisher = {Springer},
title = {{Content based music retrieval}},
year = {2008}
}
@article{gustafson1987new,
author = {Gustafson, Kjell},
journal = {Nordic Prosody IV},
pages = {105--114},
title = {{A new method for displaying speech rhythm, with illustrations from some Nordic languages}},
year = {1987}
}
@article{Cooper2006,
abstract = {Music information retrieval (MIR) is steadily growing as a research area, as can be evidenced by the international conferences on music information retrieval (ISMIR) series and the increasing number of MIR-related publications. At present, various visualization techniques developed in the context of music information retrieval for representing polyphonic audio signals are already available. The techniques fall into two major categories: techniques for visualizing a single file or piece of music, and techniques for visual collections of pieces. The former includes similariy matrix, beat spectrum and beat spectrogram, beat histograms, real-time audio classification display, and mapping time-varying timbre to color. The latter covers timbre spaces, music similarity via self-organizing maps and smoothed data histograms, and combining different views. Regardless of the category, all of these techniques use sophisticated analysis algorithms to automatically extract content information from music stored in digital audio format.},
author = {Cooper, Matthew and Foote, Jonathan and Pampalk, Elias and Tzanetakis, George},
doi = {10.1162/comj.2006.30.2.42},
file = {:Users/carthach/Documents/Mendeley Desktop/Untitled.pdf:pdf},
isbn = {01489267},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {2},
pages = {42--62},
title = {{Visualization in Audio-Based Music Information Retrieval}},
volume = {30},
year = {2006}
}
@article{Merchant2015,
abstract = {Humans possess an ability to perceive and synchronize movements to the beat in music ('beat perception and synchronization'), and recent neuroscientific data have offered new insights into this beat-finding capacity at multiple neural levels. Here, we review and compare behavioural and neural data on temporal and sequential processing during beat perception and entrainment tasks in macaques (including direct neural recording and local field potential (LFP)) and humans (including fMRI, EEG and MEG). These abilities rest upon a distributed set of circuits that include the motor cortico-basal-ganglia-thalamo-cortical (mCBGT) circuit, where the supplementary motor cortex (SMA) and the putamen are critical cortical and subcortical nodes, respectively. In addition, a cortical loop between motor and auditory areas, connected through delta and beta oscillatory activity, is deeply involved in these behaviours, with motor regions providing the predictive timing needed for the perception of, and entrainment to, musical rhythms. The neural discharge rate and the LFP oscillatory activity in the gamma- and beta-bands in the putamen and SMA of monkeys are tuned to the duration of intervals produced during a beat synchronization-continuation task (SCT). Hence, the tempo during beat synchronization is represented by different interval-tuned cells that are activated depending on the produced interval. In addition, cells in these areas are tuned to the serial-order elements of the SCT. Thus, the underpinnings of beat synchronization are intrinsically linked to the dynamics of cell populations tuned for duration and serial order throughout the mCBGT. We suggest that a cross-species comparison of behaviours and the neural circuits supporting them sets the stage for a new generation of neurally grounded computational models for beat perception and synchronization.},
author = {Merchant, H. and Grahn, J. and Trainor, L. and Rohrmeier, M. and Fitch, W. T.},
doi = {10.1098/rstb.2014.0093},
file = {:Users/carthach/Documents/Mendeley Desktop/20140093.full.pdf:pdf},
isbn = {1471-2970 (Electronic) 0962-8436 (Linking)},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
keywords = {computational biology,neuroscience},
number = {1664},
pages = {20140093--20140093},
pmid = {25646516},
title = {{Finding the beat: a neural perspective across humans and non-human primates}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2014.0093},
volume = {370},
year = {2015}
}
@article{Fonseca2011,
author = {Fonseca, N},
file = {:Users/carthach/Documents/Mendeley Desktop/Fonseca - 2011 - Singing voice resynthesis using concatenative-based techniques.pdf:pdf},
number = {December},
title = {{Singing voice resynthesis using concatenative-based techniques}},
url = {http://iconline.ipleiria.pt/handle/10400.8/540},
year = {2011}
}
@article{Duignan2004,
abstract = {Electronic music production was originally accomplished using a variety of electronic components and conventional analogue recording techniques. Both the electronic components and the recording equipment are now being replaced by computer software. In this paper we present a comparative study of two popular new systems, Reason and Live, concentrating on the role of user-interface metaphors. We compare the two systems, identify the key ways metaphor is used, and describe how it affects usability of the systems focusing on the role that user-interface metaphor play in their design. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
author = {Duignan, M and Noble, J and Barr, P and Biddle, R},
file = {:Users/carthach/Documents/Mendeley Desktop/Metaphors{\_}for{\_}Electronic{\_}Music{\_}Production{\_}in.pdf:pdf},
isbn = {3-540-22312-6},
issn = {03029743},
journal = {Computer Human Interaction},
number = {January},
pages = {111--120},
title = {{Metaphors for electronic music production in Reason and Live}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-27795-8{\_}12},
year = {2004}
}
@article{collins2007amen,
author = {Collins, Steve},
publisher = {QUT Creative Industries},
title = {{Amen to that: sampling and adapting the past}},
year = {2007}
}
@article{Chiarandini2010c,
author = {Chiarandini, Luca and Zanoni, Massimiliano and Sarti, Augusto and Leonardo, Piazza},
file = {:Users/carthach/Documents/Mendeley Desktop/Chiarandini et al. - 2010 - A System for Dynamic Playlist Generation Driven by Multimodal Control Signals and Descriptors.pdf:pdf},
title = {{A System for Dynamic Playlist Generation Driven by Multimodal Control Signals and Descriptors}},
year = {2010}
}
@phdthesis{Creation2014,
author = {Bernardes, Gilberto},
file = {:Users/carthach/Documents/Mendeley Desktop/Bernardes - 2014 - Composing Music by Selection Content-based Algorithmic-Assisted Audio Composition.pdf:pdf},
number = {July},
school = {University of Porto},
title = {{Composing Music by Selection: Content-based Algorithmic-Assisted Audio Composition}},
year = {2014}
}
@article{Smith2013a,
author = {Smith, JBL and Chew, Elaine},
file = {:Users/carthach/Documents/Mendeley Desktop/Smith, Chew - 2013 - A Meta-Analysis of the MIREX Structure Segmentation Task.pdf:pdf},
journal = {Proc. of the 14th International Society for Music {\ldots}},
title = {{A Meta-Analysis of the MIREX Structure Segmentation Task}},
url = {http://www.music.mcgill.ca/{~}jordan/documents/smith2013-ismir-mirex{\_}meta{\_}analysis.pdf},
year = {2013}
}
@article{Schmid2014,
author = {Schmid, Gian-marco},
file = {:Users/carthach/Documents/Mendeley Desktop/Schmid - 2014 - Measuring Musician' s Playing Experience Development of a questionnaire for the evaluation of musical interaction.pdf:pdf},
journal = {Practice-Based Research Workshop at NIME},
keywords = {audio-haptic modalities,digital musical instrument,evaluation,hci,measurements,musical prac-,musician,s playing experience,tice and interaction,user experience},
title = {{Measuring Musician' s Playing Experience : Development of a questionnaire for the evaluation of musical interaction}},
year = {2014}
}
@article{Valle2016,
author = {Valle, Rafael and Donz{\'{e}}, Alexandre and Fremont, Daniel J. and Akkaya, Ilge and Seshia, Sanjit A. and Freed, Adrian and Wessel, David},
file = {:Users/carthach/Documents/Mendeley Desktop/Valle et al. - 2016 - Specification Mining for Machine Improvisation with Formal Specifications RAFAEL.pdf:pdf},
journal = {Computers in Entertainment},
number = {3},
pages = {1--20},
title = {{Specification Mining for Machine Improvisation with Formal Specifications RAFAEL}},
volume = {14},
year = {2016}
}
@article{Papadopoulos2007,
abstract = {This paper dealswith the automatic estimation of chord pro- gression over time of an audio file. From the audio signal, a set of chroma vectors representing the pitch content of the file over time is extracted. From these observations the chord progression is then estimated using hidden Markov models. Several methods are proposed that allow taking into account music theory, perception of key and presence of higher harmonics of pitch notes. The proposed meth- ods are then compared to existing algorithms. A large-scale evaluation on 110 hand-labeled songs from the Beatles al- lows concluding on improvement over the state of the art.},
author = {Papadopoulos, H{\'{e}}l{\`{e}}ne and Peeters, Geoffroy},
doi = {10.1109/CBMI.2007.385392},
file = {:Users/carthach/Documents/Mendeley Desktop/Papadopoulos, Peeters - 2007 - Large-scale study of chord estimation algorithms based on chroma representation and HMM.pdf:pdf},
isbn = {1424410118},
issn = {1-4244-1011-8},
journal = {CBMI'2007 - 2007 International Workshop on Content-Based Multimedia Indexing, Proceedings},
pages = {53--60},
title = {{Large-scale study of chord estimation algorithms based on chroma representation and HMM}},
year = {2007}
}
@article{Frisson2010,
author = {Frisson, Christian and Picard, C{\'{e}}cile and Tardieu, Damien},
file = {:Users/carthach/Documents/Mendeley Desktop/Frisson, Picard, Tardieu - 2010 - Audiogarden Towards a Usable Tool for Composite Audio Creation.pdf:pdf},
journal = {QPSR of the numediart research program},
keywords = {audiogarden,content-based navigation,interfaces,mediacycle,multimedia databases,sound design},
number = {2},
pages = {33--36},
title = {{Audiogarden : Towards a Usable Tool for Composite Audio Creation}},
volume = {3},
year = {2010}
}
@article{Eigenfeldt2009,
author = {Eigenfeldt, Arne},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt - 2009 - The evolution of evolutionary software intelligent rhythm generation in Kinetic Engine.pdf:pdf},
journal = {Applications of Evolutionary Computing},
keywords = {genetic algorithms,realtime systems,recombinance,rhythm generation},
title = {{The evolution of evolutionary software: intelligent rhythm generation in Kinetic Engine}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-01129-0{\_}56},
year = {2009}
}
@article{Stravinsky2000,
author = {Stravinsky, Igor},
file = {:Users/carthach/Documents/Mendeley Desktop/Stravinsky - 2000 - A SYSTEM FOR DATA-DRIVEN CONCATENATIVE SOUND SYNTHESIS Diemo Schwarz IRCAM – Centre Pompidou Analysis – Synthesi.pdf:pdf},
journal = {Database},
pages = {1--6},
title = {{A System}},
year = {2000}
}
@article{Hesmondhalgh1997,
abstract = {In 1990s Britain, dance music is at the very centre of contemporary youth culture. But for many commentators, dance music is despicable, and its recent popularity only makes it more so. Of course, it might be expected that the right would see dance culture as dangerous, or banal, or both. But the left too has failed to engage adequately with it. For an older generation of intellectuals, who located the politics of music in lyrics, or in the public actions of rock stars, dance music's anonymity and hedonism can seem regressive. Yet dance music culture in the 1990s has enormous credibility amongst those still committed to the notion of 'oppositional' popular culture, in a way that would have been unimaginable twenty years ago. So how did dance music culture come to be understood as counter-hegemonic? How do the politics of its production and consumption match up to the sometimes Utopian claims of its proponents?},
author = {Hesmondhalgh, David},
file = {:Users/carthach/Documents/Mendeley Desktop/Hesmondhalgh - 1997 - Politics of dance music.pdf:pdf},
journal = {Soundings},
number = {5},
pages = {167--178},
title = {{Politics of dance music}},
year = {1997}
}
@article{Hennig2014,
abstract = {Though the music produced by an ensemble is influenced by multiple factors, including musical genre, musician skill, and individual interpretation, rhythmic synchronization is at the foundation of musical interaction. Here, we study the statistical nature of the mutual interaction between two humans synchronizing rhythms. We find that the interbeat intervals of both laypeople and professional musicians exhibit scale-free (power law) cross-correlations. Surprisingly, the next beat to be played by one person is dependent on the entire history of the other person's interbeat intervals on timescales up to several minutes. To understand this finding, we propose a general stochastic model for mutually interacting complex systems, which suggests a physiologically motivated explanation for the occurrence of scale-free cross-correlations. We show that the observed long-term memory phenomenon in rhythmic synchronization can be imitated by fractal coupling of separately recorded or synthesized audio tracks and thus applied in electronic music. Though this study provides an understanding of fundamental characteristics of timing and synchronization at the interbrain level, the mutually interacting complex systems model may also be applied to study the dynamics of other complex systems where scale-free cross-correlations have been observed, including econophysics, physiological time series, and collective behavior of animal flocks.},
author = {Hennig, Holger},
doi = {10.1073/pnas.1324142111},
file = {:Users/carthach/Documents/Mendeley Desktop/Hennig - 2014 - Synchronization in human musical rhythms and mutually interacting complex systems.pdf:pdf},
isbn = {1324142111},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {1--6},
pmid = {25114228},
title = {{Synchronization in human musical rhythms and mutually interacting complex systems.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25114228},
volume = {2014},
year = {2014}
}
@book{Weihs2009,
abstract = {I. Music and audio -- II. Methods -- III. Applications -- IV. Implementation.},
author = {Weihs, Claus and Jannach, Dietmar},
booktitle = {Computer},
file = {:Users/carthach/Documents/Mendeley Desktop/Music Data Analysis.pdf:pdf},
isbn = {9781498719568},
title = {{Music Data Analysis: Foundations and Applications}},
year = {2009}
}
@article{Bernardes2013,
author = {Bernardes, Gilberto and Guedes, Carlos and Pennycook, Bruce},
file = {:Users/carthach/Documents/Mendeley Desktop/Bernardes, Guedes, Pennycook - 2013 - EarGram An Application for Interactive Exploration of Concatenative Sound Synthesis in Pure Data.pdf:pdf},
journal = {From Sounds to Music and Emotions},
keywords = {and generative,concatenative sound synthesis,recombination},
pages = {110--129},
title = {{EarGram : An Application for Interactive Exploration of Concatenative Sound Synthesis in Pure Data}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-41248-6{\_}7},
year = {2013}
}
@inproceedings{Schluter2013,
address = {Prague, Czech Republic},
author = {Schl{\"{u}}ter, Jan and B{\"{o}}ck, Sebastian},
booktitle = {6th International Workshop on Machine Learning and Music},
file = {:Users/carthach/Documents/Mendeley Desktop/Schl{\"{u}}ter, B{\"{o}}ck - 2013 - Musical Onset Detection with Convolutional Neural Networks.pdf:pdf},
keywords = {ML,OnsetDetection},
mendeley-tags = {ML,OnsetDetection},
title = {{Musical Onset Detection with Convolutional Neural Networks}},
url = {http://www.ofai.at/{~}jan.schlueter/pubs/2013{\_}mml.pdf},
year = {2013}
}
@article{Morreale2016,
author = {Morreale, Fabio and Angeli, Antonella D E},
doi = {10.1145/2967508},
file = {:Users/carthach/Documents/Mendeley Desktop/Morreale, Angeli - 2016 - Collaborating with an Autonomous Agent to Generate Affective Music.pdf:pdf},
issn = {15443981},
journal = {Computers in Entertainment},
number = {3},
title = {{Collaborating with an Autonomous Agent to Generate Affective Music}},
volume = {14},
year = {2016}
}
@article{Einbond2016,
abstract = {CATORACLE responds to the need to join high-level control of audio timbre with the organization of musical form in time. It is inspired by two powerful existing tools: CataRT for corpus-based concatenative synthesis based on the MUBU for MAX library, and PYORACLE for computer improvisation, combining for the first time audio descriptor analysis and learning and generation of musical structures. Harnessing a user-defined list of audio features, live or prerecorded audio is analyzed to construct an “Audio Oracle” as a basis for improvisation. CATORACLE also extends features of classic concatenative synthesis to include live interactive audio mosaicking and score-based transcription using the BACH library for MAX. The project suggests applications not only to live performance of written and improvised electroacoustic music, but also computer-assisted composition and musical analysis.},
author = {Einbond, Aaron and Schwarz, Diemo and Borghesi, Riccardo and Schnell, Norbert},
file = {:Users/carthach/Documents/Mendeley Desktop/icmc2016-v8.pdf:pdf},
isbn = {0984527451},
pages = {12--16},
title = {{Introducing CatOracle : Corpus-based concatenative improvisation with the Audio Oracle algorithm}},
year = {2016}
}
@book{Gurney1996,
archivePrefix = {arXiv},
arxivId = {arXiv:1508.05133v2},
author = {Gurney, Kevin},
booktitle = {Neural Network World},
eprint = {arXiv:1508.05133v2},
file = {:Users/carthach/Documents/Mendeley Desktop/Gurney{\_}et{\_}al.pdf:pdf},
issn = {12100552},
number = {2},
pmid = {17691284},
publisher = {CRC press},
title = {{Introduction to neural networks}},
volume = {6},
year = {1996}
}
@book{siek2001boost,
author = {Siek, Jeremy G and Lee, Lie-Quan and Lumsdaine, Andrew},
publisher = {Pearson Education},
title = {{The Boost Graph Library: User Guide and Reference Manual}},
year = {2001}
}
@book{Stevens1975,
author = {Stevens, Stanley Smith},
publisher = {Transaction Publishers},
title = {{Psychophysics}},
year = {1975}
}
@inproceedings{Brent2009a,
address = {Montreal, Canada},
author = {Brent, William},
booktitle = {Proceedings of the International Computer Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/cepstrum{\_}paper.pdf:pdf},
isbn = {9780971319271},
pages = {3--6},
title = {{Perceptually based pitch scales in cepstral techniques percussive timbre identification}},
url = {http://www.williambrent.conflations.com/papers/cepstrum{\_}paper.pdf},
year = {2009}
}
@article{Tzanetakis2001a,
abstract = {Most audio editing tools offer limited capabilities for browsing and editing large collections of files. Moreover working with many audio files tends to clutter the limited screen space of a desktop monitor. In this paper we describe MARSYAS3D, a prototype audio browser and editor for large audio collections. A variety of 2D and 3D graphics interfaces for working with collections and/or individual files have been developed. Many of these interfaces are informed by automatic content-based audio analysis tools. Although MARSYAS3D can be used with a desktop monitor it has been specifically designed as an application for the Princeton Scalable Display Wall project. The current Display Wall system has an 8 x 18-foot rear projection screen with a resolution of 4096 x 1536 pixels. For sound a custom-made 16-speaker surround system is used. The users interact with the Wall using a variety of input methods. This immersive display allows for visual and aural presentation of detailed information for browsing and editing large audio collections and supports natural interactive collaborations among multiple simultaneous users.},
author = {Tzanetakis, George and Cook, Perry},
file = {:Users/carthach/Documents/Mendeley Desktop/TzanetakisCook2001.pdf:pdf},
journal = {Proceedings of the 2001 International Conference on Auditory Display},
pages = {1--5},
title = {{Marsyas3D: a Prototype Audio Browser-Editor Using a Large Scale Immersive Visual and Audio Display}},
year = {2001}
}
@article{Hoashi2009,
abstract = {This research presents a formal user evaluation of a typical visualization$\backslash$nmethod for content-based music information retrieval (MIR) systems,$\backslash$nand also proposes a novel interface to improve MIR usability. Numerous$\backslash$ninterfaces to visualize content-based MIR systems have been proposed,$\backslash$nbut reports on user evaluations of such proposed GUIs are scarce.$\backslash$nThis research aims to evaluate the effectiveness of a typical 2-D$\backslash$nvisualization method for content-based MIR systems, by conducting$\backslash$ncomparative user evaluations against the traditional list-based format$\backslash$nto present MIR results to the user. Based on the observations of$\backslash$nthe experimental results, we next propose a 3-D visualization system,$\backslash$nwhich features a function to specify sub-regions of the feature space$\backslash$nbased on genre classification results, and a function which allows$\backslash$nusers to select features that are assigned to the axes of the 3-D$\backslash$nspace. Evaluation of this GUI conclude that the functions of the$\backslash$n3-D system can significantly improve both the efficiency and usability$\backslash$nof MIR systems.},
author = {Hoashi, Keiichiro and Hamawaki, Shuhei and Ishizaki, Hiromi and Takishima, Yasuhiro and Katto, Jiro and Kddi, R and {Keiichiro Hoashi Shuhei Hamawaki}, Hiromi Ishizaki Yasuhiro Takashima Jiro Katto},
file = {:Users/carthach/Documents/Mendeley Desktop/PS2-3.pdf:pdf},
isbn = {9780981353708},
journal = {International Society for Music Information Retrieval Conference (ISMIR'09)},
number = {Ismir},
pages = {207--212},
title = {{Usability evaluation of visualization interfaces for content-based music retrieval systems}},
year = {2009}
}
@article{Srinivas1994,
abstract = {Genetic algorithms provide an alternative to traditional optimization techniques by using directed random searches to locate optimal solutions in complex landscapes. We introduce the art and science of genetic algorithms and survey current issues in GA theory and practice. We do not present a detailed study, instead, we offer a quick guide into the labyrinth of GA research. First, we draw the analogy between genetic algorithms and the search processes in nature. Then we describe the genetic algorithm that Holland introduced in 1975 and the workings of GAs. After a survey of techniques proposed as improvements to Holland's GA and of some radically different approaches, we survey the advances in GA theory related to modeling, dynamics, and deception},
author = {Srinivas, M. and Patnaik, Lalit M.},
doi = {10.1109/2.294849},
file = {:Users/carthach/Documents/Mendeley Desktop/00294849.pdf:pdf},
isbn = {0018-9162},
issn = {00189162},
journal = {Computer},
number = {6},
pages = {17--26},
title = {{Genetic Algorithms: A Survey}},
volume = {27},
year = {1994}
}
@article{Mandel2005,
abstract = {Searching and organizing growing digital music collections requires automatic classification of music. This paper describes a new system, tested on the task of artist identification, that uses support vector machines to classify songs based on features calculated over their entire lengths. Since support vector machines are exemplar-based classifiers, training on and classifying entire songs instead of short-time features makes intuitive sense. On a dataset of 1200 pop songs performed by 18 artists, we showthat this classifier outperforms similar classifiers that use only SVMs or song-level features. We also show that the KL divergence between single Gaussians and Mahalanobis distance between MFCC statistics vectors perform comparably when classifiers are trained and tested on separate albums, but KL divergence outperforms Mahalanobis distance when trained and tested on songs from the same albums. Keywords:},
author = {Mandel, Michael I. and Ellis, Daniel P. W.},
doi = {10.1038/sj.embor.7400483},
file = {:Users/carthach/Documents/Mendeley Desktop/ismir05.pdf:pdf},
isbn = {0955117909},
issn = {1469221X},
journal = {Proceedings of the 6th International Symposium in Music Information Retrieval (ISMIR'05)},
keywords = {artist identification,kernel spaces,song classifica-,support vector machines,tion},
number = {2004},
pages = {594--599},
title = {{Song-level features and support vector machines for music classification}},
year = {2005}
}
@article{Logan2005,
author = {Logan, Beth},
file = {:Users/carthach/Documents/Mendeley Desktop/Logan - 2005 - Nearest-neighbor artist identification.pdf:pdf},
journal = {Proceeding of Music Information Retrieval Evaluation {\ldots}},
keywords = {artist identification,music evaluations},
title = {{Nearest-neighbor artist identification}},
url = {http://www.music-ir.org/mirex/abstracts/2005/logan.pdf},
year = {2005}
}
@conference{Faraldo2017,
abstract = {Key detection in electronic dance music is important for producers and DJ{\{}$\backslash$textquoteright{\}}s who want to mix their tracks harmonically or organise their music collection by tonal content. In this paper, we present an algorithm that improves the performance of an existing method by introducing a system of multiple profiles, addressing difficult minor tracks as well as possibly amodal ones. After the explanation of our method, we use three independent datasets of electronic dance music to evaluate its performance, comparing it to other academic algorithms and commercially available solutions.},
address = {Erlangen, Germany},
author = {Faraldo, {\'{A}}ngel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {AES International Conference on Semantic Audio},
keywords = {Electronic Dance Music,Key Estimation},
title = {{A Multi-Profile Method for Key Estimation in EDM}},
year = {2017}
}
@inproceedings{beutnagel1999t,
author = {Beutnagel, Mark and Conkie, Alistair and Schroeter, Juergen and Stylianou, Yannis and Syrdal, Ann},
booktitle = {Joint meeting of ASA, EAA, and DAGA},
organization = {Berlin, Germany},
pages = {18--24},
title = {{The AT{\&}T next-gen TTS system}},
volume = {1},
year = {1999}
}
@article{Tauscher2013,
author = {Tauscher, JP and Wenger, Stephan and Magnor, Marcus},
file = {:Users/carthach/Documents/Mendeley Desktop/Tauscher, Wenger, Magnor - 2013 - Audio Resynthesis on the Dancefloor A Music Structural Approach.pdf:pdf},
journal = {Vmv},
title = {{Audio Resynthesis on the Dancefloor: A Music Structural Approach.}},
url = {http://europa.cg.cs.tu-bs.de/media/publications/tauscher2013audiotr.pdf},
year = {2013}
}
@inproceedings{Schubert2004,
abstract = {This paper investigates the dependence of perceived timbral brightness on pitch and spectral centroid for single notes and pairs of simultaneous notes. In both cases, brightness is better correlated with the spectral centroid fc than with the ratio of fc to the pitches of the notes.},
address = {Illinois, USA},
author = {Schubert, E. and Wolfe, J. and Tarnopolsky, A.},
booktitle = {Proceedings of the 8th International Conference on Music Perception and Cognition},
file = {:Users/carthach/Documents/Mendeley Desktop/SchWolTarICMPC8.pdf:pdf},
isbn = {1876346507},
keywords = {[Electronic Manuscript]},
number = {August 2004},
pages = {654--657},
title = {{Spectral centroid and timbre in complex, multiple instrumental textures}},
year = {2004}
}
@inproceedings{eigenfeldt2016musebots,
author = {Eigenfeldt, Arne},
booktitle = {Proceedings of 4th International Workshop on Musical Metacreation, Paris, France},
title = {{Musebots at One Year: A Review}},
year = {2016}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/carthach/Documents/Mendeley Desktop/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@misc{Bencina2011,
author = {Bencina, Ross},
title = {{Real-time audio programming 101: time waits for nothing}},
url = {http://www.rossbencina.com/code/real-time-audio-programming-101-time-waits-for-nothing},
urldate = {2015-09-01},
year = {2011}
}
@article{taube1991common,
author = {Taube, Heinrich},
journal = {Computer Music Journal},
number = {2},
pages = {21--32},
publisher = {JSTOR},
title = {{Common Music: A music composition language in Common Lisp and CLOS}},
volume = {15},
year = {1991}
}
@article{Quinn2002,
abstract = {Drum'n'bass is a musical form that expresses the antagonisms of British identity in the 1990s and it also situates itself outside of the dominant terms of African-American expressions of black identity. It speaks of a more productive possibility in the traditional relationship between national and global polarities or public and private histories. Being at once an expression of the crucial significance of place in any characterisation of identity, it also recognises the influence of circumstances that exist outside of the narrow terms of national affiliation. Drum'n'bass represents a metonymic formulation of the long history of race and migration and its (often invisible) effects on the nature of British cultural identity in particular and popular music in general.},
author = {Quinn, Steven},
file = {:Users/carthach/Documents/Mendeley Desktop/Quinn - 2002 - Rumble in the Jungle The Invisible History of Drum'n'Bass.pdf:pdf},
journal = {Transformations},
keywords = {Black cultural identity,British cultural identity,Drum'n'Bass,Harry Beck,London Underground Map,electronic dance music,rave},
number = {3},
pages = {1--12},
title = {{Rumble in the Jungle: The Invisible History of Drum'n'Bass}},
volume = {3},
year = {2002}
}
@misc{ONuanain2009,
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach},
title = {{Darklight X - Festival Performance}},
url = {https://www.youtube.com/watch?v=c6c5ixQTSEc},
urldate = {2014-01-01},
year = {2009}
}
@article{Scaringella2006,
author = {Scaringella, Nicolas and Zoia, Giorgio and Mlynek, Daniel},
file = {:Users/carthach/Documents/Mendeley Desktop/Scaringella, Zoia, Mlynek - 2006 - Automatic Genre Classification of Music Content (A Survey).pdf:pdf},
journal = {Signal Processing Magazine, {\ldots}},
number = {March 2006},
pages = {133--141},
title = {{Automatic Genre Classification of Music Content (A Survey)}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1598089},
year = {2006}
}
@inproceedings{Nuanain2015a,
abstract = {Composing drum patterns and musically developing them through repetition and variation is a typical task in elec- tronic music production. We propose a system that, given an input pattern, automatically creates related patterns us- ing a genetic algorithm. Two distance measures (the Ham- ming distance and directed-swap distance) that relate to rhythmic similarity are shown to derive usable fitness func- tions for the algorithm. A software instrument in the Max for Live environment presents how this can be used in real musical applications. Finally, a user survey was carried out to examine and compare the effectiveness of the fitness metrics in determining rhythmic similarity as well as the usefulness of the instrument for musical creation.},
address = {Maynooth, Ireland},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Herrera, Perfecto and Jordà, Sergi},
booktitle = {Sound and Music Computing Conference (SMC)},
file = {:Users/carthach/Documents/Mendeley Desktop/Nuan{\'{a}}in, Herrera, Jordà - 2015 - Target-Based Rhythmic Pattern Generation and Variation with Genetic Algorithms.pdf:pdf},
isbn = {9780992746629},
title = {{Target-Based Rhythmic Pattern Generation and Variation with Genetic Algorithms}},
year = {2015}
}
@misc{Studios2015,
author = {Urtubia, Hector},
booktitle = {Big Robot Studios},
title = {{Robotic Drums}},
url = {http://bigrobotstudios.com/{\#}roboticdrums},
urldate = {2015-09-28},
year = {2015}
}
@article{Fox2005,
author = {Fox, Charles},
journal = {ReCALL},
keywords = {Proceedings of the Nineteenth International Florid},
pages = {243--247},
title = {{Genetic Hierarchical Music Structures}},
year = {2005}
}
@article{Schuller2009,
author = {Schuller, B and Lehmann, Alexander},
file = {:Users/carthach/Documents/Mendeley Desktop/Schuller, Lehmann - 2009 - Blind Enhancement of the Rhythmic and Harmonic Sections by NMF Does it help.pdf:pdf},
journal = {Proc. of the International Conference on Acoustics},
pages = {361--364},
title = {{Blind Enhancement of the Rhythmic and Harmonic Sections by NMF : Does it help ?}},
url = {http://www.mmk.ei.tum.de/publ/pdf/09/09sch1.pdf},
year = {2009}
}
@article{Bello2005,
author = {Bello, JP and Daudet, Laurent},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello, Daudet - 2005 - A tutorial on onset detection in music signals.pdf:pdf},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {5},
pages = {1035--1047},
title = {{A tutorial on onset detection in music signals}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1495485},
volume = {13},
year = {2005}
}
@book{Books2009,
abstract = {While improvements in computer performance are dramatically changing the computer-generated art industry, scientists in natural computing have teamed up with artists to examine how bioinspired systems can influence art, technology and even aesthetic appreciation.{\{} {\}}This comprehensive book gives an up-to-date survey of the relevant bioinspired computing research fields -- such as evolutionary computation, artificial life, swarm intelligence and ant colony algorithms -- and examines applications in art, music and design.{\{} {\}}The editors and contributors are researchers and artists with deep experience of the related science, tools and applications, and the book includes overviews of historical developments and future perspectives.{\{} {\}}This authoritative book, complete with DVD containing image, video and music samples, as well as source code and demonstrations, offers readers an exhaustive and eye-popping introduction to the area.{\{} {\}}Written for: Researchers, lecturers, students{\{} {\}}Keywords: Artificial evolution, Computer-aided design, Computer-generated art, Computer-generated music, Evolutionary computation, Natural computing},
author = {Books, Search and Cover, Front and Search, Google Product},
booktitle = {Search},
doi = {10.1007/978-3-540-72877-1},
file = {:Users/carthach/Documents/Mendeley Desktop/Books, Cover, Search - 2009 - The Art of Artificial Evolution.pdf:pdf},
isbn = {9783540728764},
pages = {1--289},
title = {{The Art of Artificial Evolution}},
url = {http://link.springer.com/10.1007/978-3-540-72877-1},
year = {2009}
}
@inproceedings{Nuanain2016c,
address = {New York, USA},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Herrera, Perfecto and Jord{\`{a}}, Sergi},
booktitle = {Proceedings of the 17th International Society for Music Information Retrieval Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/{\'{O}} Nuan{\'{a}}in, Herrera, Jord{\`{a}} - 2016 - An Evaluation Framework and Case Study for Rhythmic Concatenative Synthesis.pdf:pdf},
title = {{An Evaluation Framework and Case Study for Rhythmic Concatenative Synthesis}},
year = {2016}
}
@article{Article2011,
author = {Walter, Schulze and van der Merwe, Brink},
file = {:Users/carthach/Documents/Mendeley Desktop/Walter, van der Merwe - 2011 - Music Generation with Markov Models.pdf:pdf},
journal = {IEEE Multimedia},
number = {18},
pages = {78--86},
title = {{Music Generation with Markov Models}},
year = {2011}
}
@inproceedings{Chudy2010,
address = {Cambridge, UK},
author = {Chudy, Magdalena and Dixon, Simon},
booktitle = {International Conference of Students of Systematic Musicology},
file = {:Users/carthach/Documents/Mendeley Desktop/Chudy-Dixon-SysMus-2010.pdf:pdf},
keywords = {performer discrimination,timbre descriptors,timbre dissimilarities},
title = {{Towards Music Performer Recognition Using Timbre Features}},
year = {2010}
}
@inproceedings{Klugel2014,
abstract = {In this contribution, we will discuss a prototype that allows a group of users to design sound collaboratively in real time using a multi-touch tabletop. We make use of a machine learning method to generate a mapping from perceptual audio features to synthesis parameters. This mapping is then used for visualization and interaction. Finally, we discuss the results of a comparative evaluation study.},
address = {London, UK},
archivePrefix = {arXiv},
arxivId = {1406.6012},
author = {Kl{\"{u}}gel, Niklas and Becker, Timo and Groh, Georg},
booktitle = {New Interfaces for Musical Expression},
eprint = {1406.6012},
file = {:Users/carthach/Documents/Mendeley Desktop/nime2014{\_}339.pdf:pdf},
keywords = {collaborative music making,sound design},
pages = {327--330},
title = {{Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis}},
url = {http://arxiv.org/abs/1406.6012},
year = {2014}
}
@book{kaehler2016learning,
author = {Kaehler, Adrian and Bradski, Gary},
publisher = {" O'Reilly Media, Inc."},
title = {{Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library}},
year = {2016}
}
@article{Gomez2007,
abstract = {Rhythmic syncopation is one of the most fundamental fea- tures that can be used to characterize music. Therefore it can be applied in a variety of domains such as mu- sic information retrieval and style analysis. During the past twenty years a score of different formal measures of rhythmic syncopation have been proposed in the mu- sic literature. Here we compare eight of these measures with each other and with human judgements of rhythmic complexity. A data set of 35 rhythms ranked by human subjects was sorted using the eight syncopation measures. A Spearman rank correlation analysis of the rankings was carried out, and phylogenetic trees were calculated to vi- sualize the resulting matrix of coefficients. The main find- ing is that the measures based on perception principles agree well with human judgements and very well with each other. The results also yield several surprises and open problems for further research. 1.},
author = {G{\'{o}}mez, Francisco and Thul, Eric and Toussaint, G},
file = {:Users/carthach/Documents/Mendeley Desktop/Syncopation-2.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
number = {January 2007},
pages = {101--104},
title = {{An experimental comparison of formal measures of rhythmic syncopation}},
url = {http://quod.lib.umich.edu/cgi/p/pod/dod-idx/experimental-comparison-of-formal-measures-of-rhythmic.pdf?c=icmc;idno=bbp2372.2007.023{\%}5Cnhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:AN+EXPERIMENTAL+COMPARISON+OF+FORMAL+MEASURES+OF+RHYTHMIC+},
year = {2007}
}
@inproceedings{Pavlidis2006,
address = {Mytilene},
author = {Pavlidis, G and Tsiafakis, D and Arnaoutoglou, F and Balla, K and Chamzas, C},
booktitle = {Third International Conference of Museology {\&} Annual Conference of AVICOM},
file = {:Users/carthach/Documents/Mendeley Desktop/10.1.1.73.7819.pdf:pdf},
pages = {1--9},
title = {{Musical universe}},
year = {2006}
}
@article{Raffel2014,
abstract = {Central to the field of MIR research is the evaluation of algorithms used to extract information from music data. We present mir{\_}eval, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms. In this paper, we enumerate the metrics implemented by mir{\_}eval and quantitatively compare each to existing implementations. When the scores reported by mir{\_}eval differ substantially from the reference, we detail the differences in implementation. We also provide a brief overview of mir{\_}eval's architecture, design, and intended use.},
author = {Raffel, Colin and Mcfee, Brian and Humphrey, Eric J. and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel P. W.},
file = {:Users/carthach/Documents/Mendeley Desktop/ismir2014mir{\_}eval.pdf:pdf},
journal = {Proc. of the 15th International Society for Music Information Retrieval Conference},
pages = {367--372},
title = {{mir{\_}eval: A Transparent Implementation of Common MIR Metrics}},
year = {2014}
}
@article{Wang2016,
author = {Wang, Cheng-i and Hsu, Jennifer and Dubnov, Shlomo},
file = {:Users/carthach/Documents/Mendeley Desktop/Wang, Hsu, Dubnov - 2016 - Machine Improvisation with Variable Markov Oracle Toward Guided and Structured Improvisation.pdf:pdf},
journal = {Computers in Entertainment},
number = {3},
title = {{Machine Improvisation with Variable Markov Oracle: Toward Guided and Structured Improvisation}},
volume = {14},
year = {2016}
}
@article{Miron2017a,
author = {Miron, Marius},
file = {:Users/carthach/Documents/Mendeley Desktop/51{\_}Paper.pdf:pdf},
number = {October},
title = {{Monaural score-informed source separation for classical music using convolutional neural networks}},
year = {2017}
}
@inproceedings{mandel2005song,
author = {Mandel, Michael I and Ellis, Dan},
booktitle = {ISMIR},
pages = {594--599},
title = {{Song-Level Features and Support Vector Machines for Music Classification.}},
volume = {2005},
year = {2005}
}
@inproceedings{Diakopoulos2009a,
author = {Diakopoulos, Dimitri and Vallis, Owen and Hochenbaum, Jordan and Murphy, Jim and Kapur, Ajay},
booktitle = {International Society for Music Information Retrieval Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Diakopoulos et al. - 2009 - 21st century electronica Mir techniques for classification and performance.pdf:pdf},
keywords = {classification,dj,electronic dance music,electronica,genre,multi-touch,user interfaces},
pages = {465--469},
title = {{21st century electronica: Mir techniques for classification and performance}},
url = {http://mtiid.calarts.edu/sites/default/files/publications/2009{\_}ismir{\_}electronica{\_}final.pdf},
year = {2009}
}
@misc{Road2015,
abstract = {Electronic music evokes new sensations, feelings, and thoughts in both composers and listeners. This book outlines a new theory of composition based on the toolkit of electronic music techniques. The theory consists of a framework of concepts and a vocabulary of terms describing musical materials, their transformation, and their organization. Central to this discourse is the notion of narrative structure in composition—how sounds are born, interact, transform, and die. This text is a guidebook: a tour of facts, history, commentary, opinions, and pointers to interesting ideas and new possibilities to consider and explore.},
author = {Road, Curtis},
doi = {10.1093/acprof:oso/9780195373233.001.0001},
file = {:Users/carthach/Documents/Mendeley Desktop/Composing Electronic Music - A New Aesthetic.pdf:pdf},
isbn = {9780195373233},
keywords = {aesthetics of composition,composition,electronic music composition,multiscale planning,sonic narrative},
title = {{Composing Electronic Music: A New Aesthetic}},
year = {2015}
}
@article{Bello2011,
author = {Bello, Juan P.},
doi = {10.1109/TASL.2011.2108287},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello - 2011 - Measuring Structural Similarity in Music.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
month = {sep},
number = {7},
pages = {2013--2025},
title = {{Measuring Structural Similarity in Music}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5711645},
volume = {19},
year = {2011}
}
@inproceedings{Ob1995,
abstract = {Concatenating units of natural speech is one method of speech synthesis 1 . Most such systems use an inventory of fixed length units, typically diphones or triphones with one instance of each type. An alternative is to use more varied, non-uniform units extracted from large speech databases containing multiple instances of each. The greater variability in such natural speech segments allows closer modeling of naturalness and differences in speaking styles, and eliminates the need for specially-recorded, single-use databases. However, with the greater variability comes the problem of how to select between the many instances of units in the database. This paper addresses that issue and presents a general method for unit selection. 1. INTRODUCTION The ATR -talk system for Japanese [4] efficiently selects non-uniform units from a large speech database but is specific to Japanese. English has more phonemes and more varying prosody, so a simple translation of the -talk system to English ...},
author = {Black, Alan W and Campbell, Nick},
booktitle = {Proc. Eurospeech},
file = {:Users/carthach/Documents/Mendeley Desktop/Black{\_}1995{\_}b.pdf:pdf},
keywords = {International Speech Communication Association,search,unit selection},
number = {0},
pages = {581--584},
title = {{Optimising selection of units from speech databases for concatenative synthesis.}},
url = {https://www.cs.cmu.edu/{~}awb/papers/eurosp95.ps},
volume = {2},
year = {1995}
}
@misc{JohnWalden2009,
author = {{John Walden}},
booktitle = {Sound On Sound},
title = {{Great Loops With LoopMash}},
url = {http://www.soundonsound.com/sos/oct09/articles/cubasetech{\_}1009.htm},
year = {2009}
}
@book{Winer2012,
author = {Winer, Ethan},
publisher = {CRC Press},
title = {{The audio expert: everything you need to know about audio}},
year = {2012}
}
@article{Blashill2002,
author = {Blashill, Pat},
journal = {Wired Magazine},
title = {{Six Machines That Changed the Music World}},
volume = {10},
year = {2002}
}
@book{reynolds2009rip,
author = {Reynolds, Simon},
publisher = {Faber {\&} Faber},
title = {{Rip it up and start again: Postpunk 1978-1984}},
year = {2009}
}
@book{Nierhaus2009a,
abstract = {Algorithmic composition – composing by means of formalizable methods – has a century old tradition not only in occidental music history. This is the first book to provide a detailed overview of prominent procedures of algorithmic composition in a pragmatic way rather than by treating formalizable aspects in single works. In addition to an historic overview, each chapter presents a specific class of algorithm in a compositional context by providing a general introduction to its development and theoretical basis and describes different musical applications. Each chapter outlines the strengths, weaknesses and possible aesthetical implications resulting from the application of the treated approaches. Topics covered are: markov models, generative grammars, transition networks, chaos and self-similarity, genetic algorithms, cellular automata, neural networks and artificial intelligence are covered. The comprehensive bibliography makes this work ideal for the musician and the researcher alike.},
archivePrefix = {arXiv},
arxivId = {1011.1669},
author = {Nierhaus, Gerhard},
booktitle = {Springer Science {\&} Business Media},
doi = {10.1007/978-3-211-75540-2},
eprint = {1011.1669},
isbn = {9783211755396},
issn = {01489267},
pages = {1--287},
pmid = {25246403},
title = {{Algorithmic composition: Paradigms of automated music generation}},
year = {2009}
}
@book{Appel2002,
author = {Appel, Alfred},
publisher = {Knopf},
title = {{Jazz modernism: from Ellington and Armstrong to Matisse and Joyce}},
year = {2002}
}
@article{Micallef2014,
abstract = {Venn diagrams with three curves are used extensively in various medical and scientific disciplines to visualize relationships between data sets and facilitate data analysis. The area of the regions formed by the overlapping curves is often directly proportional to the cardinality of the depicted set relation or any other related quantitative data. Drawing these diagrams manually is difficult and current automatic drawing methods do not always produce appropriate diagrams. Most methods depict the data sets as circles, as they perceptually pop out as complete distinct objects due to their smoothness and regularity. However, circles cannot draw accurate diagrams for most 3-set data and so the generated diagrams often have misleading region areas. Other methods use polygons to draw accurate diagrams. However, polygons are non-smooth and non-symmetric, so the curves are not easily distinguishable and the diagrams are difficult to comprehend. Ellipses are more flexible than circles and are similarly smooth, but none of the current automatic drawing methods use ellipses. We present eulerAPE as the first method and software that uses ellipses for automatically drawing accurate area-proportional Venn diagrams for 3-set data. We describe the drawing method adopted by eulerAPE and we discuss our evaluation of the effectiveness of eulerAPE and ellipses for drawing random 3-set data. We compare eulerAPE and various other methods that are currently available and we discuss differences between their generated diagrams in terms of accuracy and ease of understanding for real world data.},
author = {Micallef, Luana and Rodgers, Peter},
doi = {10.1371/journal.pone.0101717},
file = {:Users/carthach/Documents/Mendeley Desktop/Micallef, Rodgers - 2014 - euler APE Drawing area-proportional 3-Venn diagrams using ellipses.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pmid = {25032825},
title = {{euler APE: Drawing area-proportional 3-Venn diagrams using ellipses}},
volume = {9},
year = {2014}
}
@article{Eigenfeldt1996,
author = {Eigenfeldt, Arne},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt - 1996 - Generating Structure – Towards Large-Scale Formal Generation.pdf:pdf},
keywords = {AAAI Technical Report WS-14-18},
pages = {2--9},
title = {{Generating Structure – Towards Large-Scale Formal Generation}},
year = {1996}
}
@article{Dannenberg2002,
author = {Dannenberg, RB and Hu, N},
file = {:Users/carthach/Documents/Mendeley Desktop/Dannenberg, Hu - 2002 - Discovering musical structure in audio recordings.pdf:pdf},
journal = {Music and Artificial Intelligence},
title = {{Discovering musical structure in audio recordings}},
url = {http://link.springer.com/chapter/10.1007/3-540-45722-4{\_}6},
year = {2002}
}
@article{Gueguen2005,
abstract = {Sarment is a package of Python modules for easy building and manipulation of sequence segmentations. It provides efficient implementation of usual algorithms for hidden Markov Model computation, as well as for maximal predictive partitioning. Owing to its very large variety of criteria for computing segmentations, Sarment can handle many kinds of models. Because of object-oriented programming, the results of the segmentation are very easy tomanipulate.},
author = {Gu{\'{e}}guen, Laurent},
doi = {10.1093/bioinformatics/bti533},
file = {:Users/carthach/Documents/Mendeley Desktop/Gu{\'{e}}guen - 2005 - Sarment Python modules for HMM analysis and partitioning of sequences.pdf:pdf},
issn = {13674803},
journal = {Bioinformatics},
number = {16},
pages = {3427--3428},
pmid = {15947017},
title = {{Sarment: Python modules for HMM analysis and partitioning of sequences}},
volume = {21},
year = {2005}
}
@article{Ames1990a,
author = {Ames, Charles},
file = {:Users/carthach/Documents/Mendeley Desktop/Ames - 1990 - Statistics and compositional balance.pdf:pdf},
journal = {Perspectives of new music},
number = {1},
pages = {80--111},
title = {{Statistics and compositional balance}},
url = {http://www.jstor.org/stable/10.2307/833345},
volume = {28},
year = {1990}
}
@inproceedings{Jorda2005,
address = {Barcelona},
author = {Jord{\`{a}}, Sergi and Kaltenbrunner, Martin and Geiger, G{\"{u}}nter and Bencina, Ross},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}} et al. - 2005 - The reactable.pdf:pdf},
pages = {2--5},
title = {{The reactable*}},
url = {http://www.mtg.upf.es/system/files/publications/reacTableKickOff2003.pdf},
year = {2005}
}
@misc{Media2015,
author = {Media, Techn{\'{e}}},
title = {{Different Drummer — Advanced rhythm {\&} music generator}},
url = {https://itunes.apple.com/us/app/different-drummer/id525272276?mt=8},
urldate = {2015-09-28},
year = {2015}
}
@article{Toussaint,
author = {Toussaint, Godfried},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint - Unknown - Point Pattern Matching in One Dimension Applications to Music Information Retrieval.pdf:pdf},
title = {{Point Pattern Matching in One Dimension : Applications to Music Information Retrieval}}
}
@article{Eigenfeldt2013,
author = {Eigenfeldt, Arne and Pasquier, Philippe},
doi = {10.1145/2463372.2463415},
file = {:Users/carthach/Documents/Mendeley Desktop/Eigenfeldt, Pasquier - 2013 - Evolving structures for electronic dance music.pdf:pdf},
isbn = {9781450319638},
journal = {Proceeding of the fifteenth annual conference {\ldots}},
keywords = {1,22,computational creativity,electronic dance music,evolutionary art,generative music,introduction and motivations,is the idea of,or metacreation},
pages = {319},
title = {{Evolving structures for electronic dance music}},
url = {http://dl.acm.org/citation.cfm?doid=2463372.2463415{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2463415},
year = {2013}
}
@article{Parncutt1994,
author = {Parncutt, Richard},
file = {:Users/carthach/Documents/Mendeley Desktop/Parncutt - 1994 - A perceptual model of pulse salience and metrical accent in musical rhythms.pdf:pdf},
journal = {Music Perception},
title = {{A perceptual model of pulse salience and metrical accent in musical rhythms}},
url = {http://www.jstor.org/stable/40285633},
year = {1994}
}
@article{Foote2002,
author = {Foote, Jonathan and Cooper, ML and Nam, U},
file = {:Users/carthach/Documents/Mendeley Desktop/Foote, Cooper, Nam - 2002 - Audio Retrieval by Rhythmic Similarity.pdf:pdf},
journal = {ISMIR},
title = {{Audio Retrieval by Rhythmic Similarity.}},
url = {http://www.fxpal.com/publications/FXPAL-PR-02-172.pdf},
year = {2002}
}
@article{Maestre2009,
abstract = {Here we describe an approach to the expressive synthesis of jazz saxophone melodies that reuses audio recordings and carefully concatenates note samples. The aim is to generate an expressive audio sequence from the analysis of an arbitrary input score using a previously induced performance model and an annotated saxophone note database extracted from real performances. We push the idea of using the same corpus for both inducing an expressive performance model and synthesizing sound by concatenating samples in the corpus. Therefore, a connection between the performers instrument sound and performance characteristics is kept during the synthesis process.},
author = {Maestre, Esteban and Ram{\'{i}}rez, Rafael and Kersten, Stefan and Serra, Xavier},
doi = {10.1162/comj.2009.33.4.23},
file = {:Users/carthach/Documents/Mendeley Desktop/Maestre et al. - 2009 - Expressive Concatenative Synthesis by Reusing Samples from Real Performance Recordings.pdf:pdf},
issn = {0148-9267},
journal = {Computer Music Journal},
pages = {23--42},
title = {{Expressive Concatenative Synthesis by Reusing Samples from Real Performance Recordings}},
volume = {33},
year = {2009}
}
@article{Miller2010,
abstract = {The console gaming industry is experiencing a revolution in terms of user control, and a large part being played. to Nintendo's introduction of the Wii remote. The online open source development community has embraced the Wii remote, integrating the inexpensive technology into numerous applications. Some of the more interesting applications demonstrate how the remote hardware can be leveraged for nonstandard uses. In this paper we describe a new way of interacting with the Wii remote and sensor bar to produce music. The Wiiolin is a virtual instrument which can mimic a violin or cello. Sensor bar motion relative to the Wii remote and button presses are analyzed in real-time to generate notes. Our design is novel in that it involves the remote's infrared camera and sensor bar as an integral part of music production, allowing users to change notes by simply altering the angle of their wrist, and henceforth, bow. The Wiiolin introduces a more realistic way of instrument interaction than other attempts that rely on button presses and accelerometer data alone.},
author = {Miller, Jace and Hammond, Tracy},
file = {:Users/carthach/Documents/Mendeley Desktop/P497{\_}Miller.pdf:pdf},
journal = {Proceedings of the 2010 Conference on New Interfaces for Musical Expression (NIME 2010)},
keywords = {cello,figure 1,gesture recognition,human computer interaction,motion,recognition,the orientation of the,violin,virtual instrument,wii remote,wii remote determines},
number = {June},
pages = {15--18},
title = {{Wiiolin : a virtual instrument using the Wii remote}},
year = {2010}
}
@article{Herrera-Boyer2003,
abstract = {We present an exhaustive review of research on automatic classification of sounds from musical instruments. Two dif- ferent but complementary approaches are examined, the per- ceptual approach and the taxonomic approach. The former is targeted to derive perceptual similarity functions in order to use them for timbre clustering and for searching and retriev- ing sounds by timbral similarity. The latter is targeted to derive indexes for labeling sounds after culture- or user- biased taxonomies. We review the relevant features that have been used in the two areas and then we present and discuss different techniques for similarity-based clustering of sounds and for classification into pre-defined instrumental categories.},
author = {Herrera-Boyer, Perfecto and Peeters, Geoffroy and Dubnov, Shlomo},
doi = {10.1076/jnmr.32.1.3.16798},
file = {:Users/carthach/Documents/Mendeley Desktop/Herrera-Peeters-Dubnov-JNMR-2003.pdf:pdf},
isbn = {9780387306674},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1},
pages = {3--21},
title = {{Automatic Classification of Musical Instrument Sounds}},
url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.1.3.16798},
volume = {32},
year = {2003}
}
@article{Martin1999,
author = {Martin, Daniel},
file = {:Users/carthach/Documents/Mendeley Desktop/j.0022-3840.1999.00077.x.pdf:pdf},
journal = {The Journal of Popular Culture},
number = {4},
pages = {77--99},
publisher = {Wiley Online Library},
title = {{Power play and party politics: The significance of raving}},
volume = {32},
year = {1999}
}
@article{fels2002mapping,
author = {Fels, Sidney and Gadd, Ashley and Mulder, Axel},
file = {:Users/carthach/Documents/Mendeley Desktop/2d91c84e4d2f5dbf33ccdf77219007c45f67.pdf:pdf},
journal = {Organised Sound},
number = {2},
pages = {109--126},
publisher = {Cambridge University Press},
title = {{Mapping transparency through metaphor: towards more expressive musical instruments}},
volume = {7},
year = {2002}
}
@article{Holzapfel2012,
abstract = {In this paper, we propose a method that can identify challenging music samples for beat tracking without ground truth. Our method, motivated by the machine learning method {\#}x201C;selective sampling, {\#}x201D; is based on the measurement of mutual agreement between beat sequences. In calculating this mutual agreement we show the critical influence of different evaluation measures. Using our approach we demonstrate how to compile a new evaluation dataset comprised of difficult excerpts for beat tracking and examine this difficulty in the context of perceptual and musical properties. Based on tag analysis we indicate the musical properties where future advances in beat tracking research would be most profitable and where beat tracking is too difficult to be attempted. Finally, we demonstrate how our mutual agreement method can be used to improve beat tracking accuracy on large music collections.},
author = {Holzapfel, Andr{\'{e}} and Davies, Matthew E P and Zapata, Jos{\'{e}} R. and Oliveira, Jo{\~{a}}o Lobato and Gouyon, Fabien},
doi = {10.1109/TASL.2012.2205244},
file = {:Users/carthach/Documents/Mendeley Desktop/Holzapfel et al. - 2012 - Selective sampling for beat tracking evaluation.pdf:pdf},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Beat tracking,evaluation,ground truth annotation,selective sampling},
number = {9},
pages = {2539--2548},
title = {{Selective sampling for beat tracking evaluation}},
volume = {20},
year = {2012}
}
@article{Frisson2014a,
author = {Frisson, Christian and Dupont, St{\'{e}}phane and Yvart, Willy and Riche, Nicolas and Siebert, Xavier and Dutoit, Thierry},
doi = {10.1145/2636879.2636880},
file = {:Users/carthach/Documents/Mendeley Desktop/audiomostly2014{\_}audiometro.pdf:pdf},
isbn = {9781450330329},
journal = {Proceedings of the 9th Audio Mostly on A Conference on Interaction With Sound - AM '14},
keywords = {all or part of,content-based similarity,known-item search,mation retrieval,media browsers,music infor-,or hard copies of,permission to make digital,sound effects,this work for per-,visual variables},
pages = {1--8},
title = {{AudioMetro}},
url = {http://dl.acm.org/citation.cfm?doid=2636879.2636880},
year = {2014}
}
@inproceedings{Schwarz2012,
author = {Schwarz, Diemo and Hackbarth, Benjamin},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Schwarz, Hackbarth - 2012 - Navigating variation composing for audio mosaicing.pdf:pdf;:Users/carthach/Documents/Mendeley Desktop/variation-explorer-icmc2012.pdf:pdf},
isbn = {9780984527410},
pages = {1--4},
title = {{Navigating variation: composing for audio mosaicing}},
url = {http://articles.ircam.fr/textes/Schwarz12b/index.pdf},
year = {2012}
}
@article{pachet2001finite,
address = {Barcelona, Spain},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre and Barbieri, Gabriele and Paris, Sony C S L},
journal = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence},
number = {1/3},
title = {{Finite-length Markov processes with constraints}},
volume = {6},
year = {2011}
}
@article{Masood2015,
author = {Masood, Sarfaraz and Gupta, Shubham and Khan, Shadab},
doi = {10.1109/INDICON.2015.7443497},
file = {:Users/carthach/Documents/Mendeley Desktop/07443497.pdf:pdf},
isbn = {978-1-4673-7399-9},
journal = {2015 Annual IEEE India Conference (INDICON)},
pages = {1--5},
title = {{Novel approach for musical instrument identification using neural network}},
url = {http://ieeexplore.ieee.org/document/7443497/},
year = {2015}
}
@article{Gunderson204,
author = {Gunderson, Philip A},
journal = {Postmodern culture},
number = {1},
publisher = {The Johns Hopkins University Press},
title = {{Danger Mouse's Grey Album, mash-ups, and the age of composition}},
volume = {15},
year = {2004}
}
@book{Roads1996,
abstract = {The Computer Music Tutorial is a comprehensive text and reference that covers all aspects of computer music, including digital audio, synthesis techniques, signal processing, musical input devices, performance software, editing systems, algorithmic composition, MIDI, synthesizer architecture, system interconnection, and psychoacoustics. A special effort has been made to impart an appreciation for the rich history behind current activities in the field.The Computer Music Tutorial provides a step-by-step introduction to the entire field of computer music techniques. Written for nontechnical as well as technical readers, it uses hundreds of charts, diagrams, screen images, and photographs as well as clear explanations to present basic concepts and terms. Mathematical notation and program code examples are used only when absolutely necessary. Explanations are not tied to any specific software or hardware.Computer Music Journal for more than a decade and is a recognized authority in the field. The material in this book was compiled and refined over a period of several years of teaching in classes at Harvard University, Oberlin Conservatory, the University of Naples, IRCAM, Les Ateliers UPIC, and in seminars and workshops in North America, Europe, and Asia.},
author = {Roads, Curtis},
doi = {10.1016/S0898-1221(96)90229-1},
isbn = {0262680823},
issn = {08981221},
number = {6},
pages = {133},
pmid = {669394},
publisher = {MIT Press},
title = {{The computer music tutorial}},
volume = {32},
year = {1996}
}
@article{McAdams1979,
abstract = {No abstract},
author = {McAdams, Stephen and Bregman, Albert S},
doi = {10.2307/4617866},
file = {:Users/carthach/Documents/Mendeley Desktop/McAdams-Bregman-streams.pdf:pdf},
isbn = {01489267},
issn = {01489267},
journal = {Computer Music Journal},
number = {4},
pages = {26--43},
title = {{Hearing Musical Streams}},
volume = {3},
year = {1979}
}
@phdthesis{Tindale2009,
author = {Tindale, Adam},
file = {:Users/carthach/Documents/Mendeley Desktop/thesis.pdf:pdf},
pages = {205},
school = {University of Victoria},
title = {{Advancing the Art of Electronic Percussion}},
year = {2009}
}
@article{Song2015,
author = {Song, Chunyang and Pearce, Marcus and Harte, Christopher},
file = {:Users/carthach/Documents/Mendeley Desktop/syncopation{\_}toolkit.pdf:pdf},
isbn = {9780992746629},
journal = {Proceedings of the 12th International Conference on Sound and Music Computing (SMC-15)},
pages = {295--300},
title = {{SYNPY: a python toolkit for syncopation modelling}},
year = {2015}
}
@article{Grey1977,
abstract = {Two experiments were performed to evaluate the perceptual relationships between 16 music instrument tones. The stimuli were computer synthesized based upon an analysis of actual instrument tones, and they were perceptually equalized for loudness,pitch, and duration. Experiment 1 evaluated the tones with respect to perceptual similarities, and the results were treated with multidimensional scaling techniques and hierarchic clusteringanalysis. A three‐dimensional scaling solution, well matching the clusteringanalysis, was found to be interpretable in terms of (1) the spectral energy distribution; (2) the presence of synchronicity in the transients of the higher harmonics, along with the closely related amount of spectral fluctuation within the the tone through time; and (3) the presence of low‐amplitude, high‐frequency energy in the initial attack segment; an alternate interpretation of the latter two dimensions viewed the cylindrical distribution of clusters of stimulus points about the spectral energy distribution, grouping on the basis of musical instrument family (with two exceptions). Experiment 2 was a learning task of a set of labels for the 16 tones. Confusions were examined in light of the similarity structure for the tones from experiment 1, and one of the family‐grouping exceptions was found to be reflected in the difficulty of learning the labels.},
author = {Grey, J M},
doi = {10.1121/1.381428},
file = {:Users/carthach/Documents/Mendeley Desktop/07e4aa1fdbed5c24b3decd0ecd45b0d9fcf7.pdf:pdf},
isbn = {0001-4966},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {5},
pages = {1270--1277},
pmid = {560400},
title = {{Multidimensional perceptual scaling of musical timbres.}},
volume = {61},
year = {1977}
}
@inproceedings{Bock2011,
abstract = {A system for the synthesis of backing vocals by pitch shifting of a lead vocal signal is presented. The harmonization of the backing vocals is based on the chords which are retrieved from an accom- panying instrument. The system operates completely autonomous without the need to provide the key of the performed song. This simplifies the handling of the harmonization effect. The system is designed to have realtime capability to be used as live sound effect.},
author = {B{\"{o}}ck, Sebastian and Schedl, Markus},
booktitle = {14th International Conference on Digital Audio Effects (DAFx-11)},
file = {:Users/carthach/Documents/Mendeley Desktop/31{\_}e.pdf:pdf},
isbn = {9782954035109},
pages = {301--306},
title = {{Enhanced Beat Tracking With Context-Aware Neural Networks}},
year = {2011}
}
@article{Gillet2006,
abstract = {One of the main bottlenecks in the progress of the Music Information Retrieval (MIR) research field is the limited access to common, large and annotated audio databases that could serve for technology development and/or evaluation. The aim of this paper is to present in detail the ENST-Drums database, emphasizing on both the content and the recording process. This audiovisual database of drum performances by three professional drummers was recorded on 8 audio channels and 2 video channels. The drum sequences are fully annotated and will be, for a large part, freely distributed for research purposes. The large variety in its content should serve research in various domains of audio signal processing involving drums, ranging from single drum event classification to complex multimodal drum track transcription and extraction from polyphonic music.},
author = {Gillet, Olivier and Ga{\"{e}}l, Richard},
file = {:Users/carthach/Documents/Mendeley Desktop/Gillet, Ga{\"{e}}l - 2006 - Enst-drums an extensive audio-visual database for drum signals processing.pdf:pdf},
isbn = {1-55058-349-2},
journal = {In Proceedings of the 7th International Symposium on Music Information Retrieval (ISMIR 2006},
keywords = {aration,automatic drum transcrip-,campaign,drum event detection in,for example,multimodal music transcription,polyphonic music,research database,source sep-,the database used for,the mami,tion},
pages = {156--159},
title = {{Enst-drums: an extensive audio-visual database for drum signals processing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.134.4704},
year = {2006}
}
@article{Hunt1996,
abstract = {One approach to the generation of natural-sounding synthesized speech waveforms is to select and concatenate units from a large speech database. Units (in the current work, phonemes) are selected to produce a natural realisation of a target phoneme sequence predicted from text which is annotated with prosodic and phonetic context information. We propose that the units in a synthesis database can be considered as a state transition network in which the state occupancy cost is the distance between a database unit and a target, and the transition cost is an estimate of the quality of concatenation of two consecutive units. This framework has many similarities to HMM-based speech recognition. A pruned Viterbi search is used to select the best units for synthesis from the database. This approach to waveform synthesis permits training from natural speech: two methods for training from speech are presented which provide weights which produce more natural speech than can be obtained by hand-tuning},
author = {Hunt, Andrew J. and Black, Alan W.},
doi = {10.1109/ICASSP.1996.541110},
file = {:Users/carthach/Documents/Mendeley Desktop/Hunt, Black - 1996 - Unit selection in a concatenative speech synthesis system using a large speech database.pdf:pdf},
isbn = {0-7803-3192-3},
issn = {1520-6149},
journal = {1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings},
pages = {373--376},
title = {{Unit selection in a concatenative speech synthesis system using a large speech database}},
volume = {1},
year = {1996}
}
@article{Vogl2016,
author = {Vogl, Richard and Leimeister, Matthias and {{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Jord{\`{a}}, Sergi and Hlatky, Michael and Knees, Peter},
file = {:Users/carthach/Documents/Mendeley Desktop/Vogl et al. - 2016 - An Intelligent Interface for Drum Pattern Variation.pdf:pdf},
journal = {Journal of the Audio Engineering Society},
number = {7},
pages = {503--513},
title = {{An Intelligent Interface for Drum Pattern Variation and Comparative Evaluation of Algorithms}},
volume = {64},
year = {2016}
}
@article{Dong2006,
author = {Dong, M and Lua, KT and Li, H},
file = {:Users/carthach/Documents/Mendeley Desktop/406f07c805b1f69a26ad50124e20d2207207.pdf:pdf},
journal = {Journal of Chinese Language and Computing},
keywords = {prosody parameter,speech synthesis,text-to-speech,unit selection},
number = {1},
pages = {1--10},
title = {{A Unit Selection-based Speech Synthesis Approach for Mandarin Chinese}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Unit+Selection-based+Speech+Synthesis+Approach+for+Mandarin+Chinese{\#}1},
volume = {16},
year = {2006}
}
@inproceedings{kim2006towards,
author = {Kim, Youngmoo E and Williamson, Donald S and Pilli, Sridhar},
booktitle = {ISMIR},
pages = {393--394},
title = {{Towards Quantifying the" Album Effect" in Artist Identification.}},
year = {2006}
}
@article{Rodgers2003,
abstract = {Most scholars writing on the use of samplers express anxiety over the dissolution of boundaries between human-generated and automated musical expression, and focus on the copyright infringement issues surrounding sampling practices without adequately exploring samplists' musical and political goals. Drawing on musical examples from various underground electronic music genres and on interviews with electronic musicians, this essay addresses such questions as: What is a sampler, and how does the sampling process resonate with or diverge from other traditions of instrument-playing? How do electronic musicians use the 'automated' mechanisms of digital instruments to achieve nuanced musical expression and cultural commentary? What are some political implications of presenting sampled and processed sounds in a reconfigured compositional environment? By exploring these issues, I hope to counter the over-simplified, uninformed critical claims that sampling is a process of 'theft' and 'automation', and instead offer insight into the myriad and complex musical and political dimensions of sampling in electronic music production.},
author = {Rodgers, Tara},
doi = {10.1017/S1355771803000293},
file = {:Users/carthach/Documents/Mendeley Desktop/Rodgers - 2003 - On the process and aesthetics of sampling in electronic music production.pdf:pdf},
isbn = {1355771803},
issn = {1355-7718},
journal = {Organised Sound},
number = {03},
pages = {313--320},
title = {{On the process and aesthetics of sampling in electronic music production}},
volume = {8},
year = {2003}
}
@article{Beller2005,
abstract = {In this paper, we describe a concatenative synthesis system$\backslash$nwhich was first designed for a realistic synthesis of$\backslash$nmelodic phrases. It has since been augmented to become$\backslash$nan experimental TTS (Text-to-Speech) synthesizer. Today,$\backslash$nit is able to realize hybrid synthesis involving speech$\backslash$nsegments and musical excerpts coming from any recording$\backslash$nimported in its database. The system can also synthesize$\backslash$nsentences with different voices, sentences with musical$\backslash$nsounds, melodic phrases with speech segments and$\backslash$ngenerate compositional material from specific intonation$\backslash$npatterns using a prosodic pattern extractor.},
author = {Beller, Gr{\'{e}}gory and Schwarz, Diemo and Hueber, Thomas and Rodet, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Beller et al. - 2005 - A hybrid concatenative synthesis system on the intersection of music and speech.pdf:pdf},
journal = {Proceedings of 2005 Joun{\'{e}}es d'Informatique Musicale},
pages = {41--45},
title = {{A hybrid concatenative synthesis system on the intersection of music and speech}},
year = {2005}
}
@article{Somerville2008,
abstract = {The automatic identification of musical instrument timbres occurring in a recording of music has many applications, including music search by timbre, music recommender systems and transcribers. A major difficulty is that most music is multitimbral, making it difficult to identify the individual timbres present. One approach is to classify music based on specific groups of musical instruments. In this paper we report on our experiments that classify musical instrument timbres based on specific groups that are often found in commercial recordings. Classification using the k-nearest neighbour classifier, with audio features such as Mel frequency cepstral coefficients, on a set of 160 samples from commercial recordings, gave an accuracy of 80{\%}. Some of the difficulties arose from distinguishing similar instrument groups, such as those only differing by the inclusion or exclusion ofa voice. However, when these were examined in isolation, greater accuracy was achieved, suggesting that a hierarchical approach may be helpful.},
author = {Somerville, Peter and Uitdenbogerd, Alexandra L.},
doi = {10.1109/CSA.2008.67},
file = {:Users/carthach/Documents/Mendeley Desktop/04654099.pdf:pdf},
isbn = {9780769534282},
journal = {Proceedings - International Symposium on Computer Science and Its Applications, CSA 2008},
pages = {269--274},
title = {{Multitimbral musical instrument classification}},
year = {2008}
}
@inproceedings{balestri1999choose,
author = {Balestri, Marcello and Pacchiotti, Alberto and Quazza, Silvia and Salza, Pier Luigi and Sandri, Stefano},
booktitle = {Sixth European Conference on Speech Communication and Technology},
title = {{Choose the best to modify the least: a new generation concatenative synthesis system}},
year = {1999}
}
@inproceedings{Mangani2006,
abstract = {While quotation in general has recently become a main topic of the musicological research, nquotation in jazzz, as David Metzer states, nhas received little attentionz (Quotation and Cultural Meaning in Twentieth-Century Music, Cambridge 2003, p. 50 n. 10). In this paper we focus on the use of quotation on the part of the jazz soloist in order to integrate improvisation. This practice reveals in the first instance some specific aspects of the musical background of a jazz player: the case of Charlie Parker, whose quotations range from traditional tunes to Stravinsky, is well known and has recently been reappraised (for instance, in the interesting, even if discontinuous, book written by Gianfranco Salvatore, Viterbo 2005), while the cognitive implications of Parkers method were faced several years ago by Perlman and Greenblatt (Miles Davis Meets Noam Chomsky..., in The Sign in Music and Language, ed. W. Steiner, Austin 1981). Parker, anyway, was by no means the only jazz player who had recourse to quotation; and quotation is not to be considered as a mere indicator of the soloists musical background. We are convinced that in several jazz styles quotation represents a downright structural factor of the improvisational path; and that, as such, it has several cognitive implications. In order to properly evaluate the role of such quotations, the simple aural/mnemonic individuation of their sources is not enough: We need instead a systematic description of their location (in what Chorus?; to what formal part of the Standard does the quotation correspond?), their melodic and harmonic features and even their role in terms of performance (studio or live?; at the end of a piece to increase applauses? and so on). To this end, we propose here a pattern of database and show some interesting results concerning three emblematic cases: Sidney Bechet, Duke Ellington and Ella Fitzgerald.},
author = {Mangani, Marco and Baldizzone, Roberta and Nobile, Gianni},
booktitle = {Proceedings of the 9th International Conference on Music Perception and Cognition},
isbn = {8873951554},
pages = {286},
title = {{Quotation in jazz improvisation: A database and some examples}},
year = {2006}
}
@article{Nye2013,
author = {Nye, Sean},
doi = {10.1111/jpms.12032},
file = {:Users/carthach/Documents/Mendeley Desktop/Minimal{\_}Understandings{\_}The{\_}Berlin{\_}Decade.pdf:pdf},
issn = {15242226},
journal = {Journal of Popular Music Studies},
number = {2},
pages = {154--184},
title = {{Minimal understandings: The Berlin decade, the minimal continuum, and debates on the legacy of German techno}},
volume = {25},
year = {2013}
}
@article{dijkstra1959note,
author = {Dijkstra, Edsger W},
journal = {Numerische mathematik},
number = {1},
pages = {269--271},
publisher = {Springer},
title = {{A note on two problems in connexion with graphs}},
volume = {1},
year = {1959}
}
@article{Goto2003a,
author = {Goto, M},
file = {:Users/carthach/Documents/Mendeley Desktop/Goto - 2003 - A CHORUS-SECTION DETECTING METHOD FOR MUSICAL AUDIO SIGNALS.pdf:pdf},
isbn = {0780376633},
journal = {Acoustics, Speech, and Signal Processing, 2003. {\ldots}},
number = {April},
pages = {437--440},
title = {{A CHORUS-SECTION DETECTING METHOD FOR MUSICAL AUDIO SIGNALS}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1200000},
volume = {2003},
year = {2003}
}
@book{Siek2002,
abstract = {The Boost Graph Library (BGL) is the first C++ library to apply the principles of generic programming to the construction of the advanced data structures and algorithms used in graph computations. Problems in such diverse areas as Internet packet routing, molecular biology, scientific computing, and telephone network design can be solved by using graph theory. This book presents an in-depth description of the BGL and provides working examples designed to illustrate the application of BGL to these real-world problems. Written by the BGL developers, The Boost Graph Library: User Guide and Reference Manual gives you all the information you need to take advantage of this powerful new library. Part I is a complete user guide that begins by introducing graph concepts, terminology, and generic graph algorithms. This guide also takes the reader on a tour through the major features of the BGL; all motivated with example problems. Part II is a comprehensive reference manual that provides complete documentation of all BGL concepts, algorithms, and classes.},
author = {Siek, Jeremy G. and Lee, Lie-Quan and Lumsdaine, Andrew},
booktitle = {ist.tugraz.at},
file = {:Users/carthach/Documents/Mendeley Desktop/Siek, Lee, Lumsdaine - 2002 - The Boost Graph Library.pdf:pdf},
isbn = {0-201-72914-8},
issn = {00983500},
pages = {352},
pmid = {10828931},
title = {{The Boost Graph Library}},
url = {http://www.informit.com/store/product.aspx?isbn=0201729148},
year = {2002}
}
@article{Flexer2015,
abstract = {Visualizations of music databases are a popular form of interface allowing intuitive exploration of music catalogs. They are often based on lower dimensional projections of high dimensional music similarity spaces. Such similarity spaces have already been shown to be negatively impacted by so-called hubs and anti-hubs. These are points that ap- pear very close or very far to many other data points due to a problem of measuring distances in high-dimensional spaces. We present an empirical study on how this phe- nomenon impacts three popular approaches to compute two- dimensional visualizations of music databases. We also show how the negative impact of hubs and anti-hubs can be reduced by re-scaling the high dimensional spaces be- fore low dimensional projection. 1.},
author = {Flexer, Arthur},
file = {:Users/carthach/Documents/Mendeley Desktop/Flexer - 2015 - Improving visualization of high-dimensional music similarity spaces.pdf:pdf},
journal = {International Society for Music Information Retrieval Conference (ISMIR)},
pages = {547--553},
title = {{Improving visualization of high-dimensional music similarity spaces}},
year = {2015}
}
@article{Hamasaki2013,
author = {Hamasaki, Masahiro and Goto, Masataka},
doi = {10.1145/2491055.2491059},
file = {:Users/carthach/Documents/Mendeley Desktop/a4-hamasaki.pdf:pdf},
isbn = {9781450318525},
journal = {Proceedings of the 9th International Symposium on Open Collaboration - WikiSym '13},
keywords = {massive open collaboration,music interface,or hard copies of,part or all of,permission to make digital,social tagging,this work for,user-generated content,visualization,web service},
pages = {1--10},
title = {{Songrium}},
url = {http://dl.acm.org/citation.cfm?doid=2491055.2491059},
year = {2013}
}
@article{Bock2013,
author = {B{\"{o}}ck, S and Widmer, G},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck, Widmer - 2013 - Maximum filter vibrato suppression for onset detection.pdf:pdf},
journal = {Proc. of the 16th Int. Conference on Digital Audio Effects},
pages = {1--7},
title = {{Maximum filter vibrato suppression for onset detection}},
url = {http://dafx13.nuim.ie/papers/09.dafx2013{\_}submission{\_}12.pdf},
year = {2013}
}
@inproceedings{Paiement2008,
abstract = {Modeling long-term dependencies in time series has proved very difficult to achieve with traditional machine learning methods. This problem occurs when considering music data. In this paper, we introduce a model for rhythms based on the distributions of distances between subsequences. A specific implementation of the model when considering Hamming distances over a simple rhythm representation is described. The proposed model consistently outperforms a standard Hidden Markov Model in terms of conditional prediction accuracy on two different music databases.},
author = {Paiement, Jean-Francois and Grandvalet, Yves and Bengio, Samy},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
doi = {10.1145/1390156.1390249},
file = {:Users/carthach/Documents/Mendeley Desktop/Paiement, Grandvalet, Bengio - 2008 - A Distance Model for Rhythms.pdf:pdf},
isbn = {9781605582054},
keywords = {Theory {\&} Algorithms},
title = {{A Distance Model for Rhythms}},
url = {http://eprints.pascal-network.org/archive/00004731/},
year = {2008}
}
@article{Davies2013,
author = {Davies, Matthew E. P. and Hamel, Philippe and Yoshii, Kazuyoshi and Goto, Masataka},
file = {:Users/carthach/Documents/Mendeley Desktop/Davies et al. - 2013 - AutoMashUpper An Automatic Multi-Song Mashup System.pdf:pdf},
isbn = {978-0-615-90065-0},
journal = {Proceedings of the 14th International Society for Music Information Retrieval Conference, ISMIR 2013},
pages = {575----580},
title = {{AutoMashUpper: An Automatic Multi-Song Mashup System.}},
url = {http://130.54.20.150/members/yoshii/papers/ismir-2013-davies.pdf},
year = {2013}
}
@phdthesis{Marti2015,
author = {Marti, Umbert},
file = {:Users/carthach/Documents/Mendeley Desktop/Marti - 2015 - Expression Control of Singing Voice Synthesis Modeling Pitch and Dynamics with Unit Selection and Statistical Approaches.pdf:pdf},
school = {Universitat Pompeu Fabra},
title = {{Expression Control of Singing Voice Synthesis: Modeling Pitch and Dynamics with Unit Selection and Statistical Approaches}},
year = {2015}
}
@article{Wilkinson2012,
abstract = {Scientists conducting microarray and other experiments use circular Venn and Euler diagrams to analyze and illustrate their results. As one solution to this problem, this paper introduces a statistical model for fitting area-proportional Venn and Euler diagrams to observed data. The statistical model outlined in this paper includes a statistical loss function and a minimization procedure that enables formal estimation of the Venn/Euler area-proportional model for the first time. A significance test of the null hypothesis is computed for the solution. Residuals from the model are available for inspection. As a result, this algorithm can be used for both exploration and inference on real data sets. A Java program implementing this algorithm is available under the Mozilla Public License. An R function venneuler() is available as a package in CRAN and a plugin is available in Cytoscape.},
author = {Wilkinson, Leland},
doi = {10.1109/TVCG.2011.56},
file = {:Users/carthach/Documents/Mendeley Desktop/Wilkinson - 2012 - Exact and approximate area-proportional circular venn and euler diagrams.pdf:pdf},
isbn = {1941-0506 (Electronic)$\backslash$n1077-2626 (Linking)},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Visualization,bioinformatics,statistical graphics},
number = {2},
pages = {321--331},
pmid = {21383412},
title = {{Exact and approximate area-proportional circular venn and euler diagrams}},
volume = {18},
year = {2012}
}
@inproceedings{Lazier2003,
abstract = {The process of creating an audio mosaic consists of the concatenation$\backslash$nof segments of sound. Segments are chosen to correspond$\backslash$nbest with a description of a target sound specified by the desired$\backslash$nfeatures of the final mosaic. Current audio mosaicing techniques$\backslash$ntake advantage of the description of future target units in order$\backslash$nto make more intelligent decisions when choosing individual segments.$\backslash$nIn this paper, we investigate ways to expand mosaicing$\backslash$ntechniques in order to use the mosaicing process as an interactive$\backslash$nmeans of musical expression in real time.$\backslash$n$\backslash$nIn our system, the user can interactively choose the specification$\backslash$nof the target as well as the source signals from which the$\backslash$nmosaic is composed. These means of control are incorporated into$\backslash$nMoSievius, a framework intended for the rapid implementation of$\backslash$ndifferent interactive mosaicing techniques. Its integral means of$\backslash$ncontrol, the Sound Sieve, provides real-time control over the source$\backslash$nselection process when creating an audio mosaic. We discuss a$\backslash$nnumber of new real-time effects that can be achieved through use$\backslash$nof the Sound Sieve.},
author = {Lazier, Ari and Cook, Perry R.},
booktitle = {Proc. of the 6th Int. Conference on Digital Audio Effects (DAFx-03)},
pages = {1--6},
title = {{Mosievius: Feature Driven Interactive Audio Mosaicing}},
year = {2003}
}
@article{Hockman2007,
author = {Hockman, JA},
file = {:Users/carthach/Documents/Mendeley Desktop/Hockman - 2007 - Automatic Timbre Mutation of Drum Loops.pdf:pdf},
title = {{Automatic Timbre Mutation of Drum Loops}},
url = {http://steinhardt.nyu.edu/scmsAdmin/uploads/002/919/jhockman{\_}thesis.pdf},
year = {2007}
}
@article{McAdams1999,
abstract = {The perceptual salience of several outstanding features of quasiharmonic, time-variant spectra was investigated in musical instrument sounds. Spectral analyses of sounds from seven musical instruments (clarinet, flute, oboe, trumpet, violin, harpsichord, and marimba) produced time-varying harmonic amplitude and frequency data. Six basic data simplifications and five combinations of them were applied to the reference tones: amplitude-variation smoothing, coherent variation of amplitudes over time, spectral-envelope smoothing, forced harmonic-frequency variation, frequency-variation smoothing, and harmonic-frequency flattening. Listeners were asked to discriminate sounds resynthesized with simplified data from reference sounds resynthesized with the full data. Averaged over the seven instruments, the discrimination was very good for spectral envelope smoothing and amplitude envelope coherence, but was moderate to poor in decreasing order for forced harmonic frequency variation, frequency variation smoothing, frequency flattening, and amplitude variation smoothing. Discrimination of combinations of simplifications was equivalent to that of the most potent constituent simplification. Objective measurements were made on the spectral data for harmonic amplitude, harmonic frequency, and spectral centroid changes resulting from simplifications. These measures were found to correlate well with discrimination results, indicating that listeners have access to a relatively fine-grained sensory representation of musical instrument sounds.},
author = {McAdams, S and Beauchamp, J W and Meneguzzi, S},
doi = {10.1121/1.426277},
file = {:Users/carthach/Documents/Mendeley Desktop/mcadams{\_}1999{\_}jasa.pdf:pdf},
isbn = {0001-4966 (Print)},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2 Pt 1},
pages = {882--897},
pmid = {9972573},
title = {{Discrimination of musical instrument sounds resynthesized with simplified spectrotemporal parameters.}},
volume = {105},
year = {1999}
}
@book{Bregman1994,
author = {Bregman, Albert S},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {MIT press},
title = {{Auditory scene analysis: The perceptual organization of sound}},
year = {1994}
}
@article{Wiggins2008,
author = {Wiggins, Geraint A.},
doi = {10.1093/llc/fqm025},
file = {:Users/carthach/Documents/Mendeley Desktop/Wiggins - 2008 - Computer models of musical creativity A review of computer models of musical creativity by David Cope.pdf:pdf},
isbn = {0268-1145},
issn = {02681145},
journal = {Literary and Linguistic Computing},
number = {1},
pages = {109--116},
title = {{Computer models of musical creativity: A review of computer models of musical creativity by David Cope}},
volume = {23},
year = {2008}
}
@article{Cope2000,
author = {Cope, David},
publisher = {Waveland Press},
title = {{New directions in music}},
year = {2000}
}
@article{Sewell2014,
author = {Sewell, Amanda},
journal = {Journal of the Society for American Music},
title = {{Paul's Boutique and Fear of a Black Planet: Digital Sampling and Musical Style in Hip Hop}},
year = {2014}
}
@article{Heusser2017,
abstract = {Data visualizations can reveal trends and patterns that are not otherwise obvious from the raw data or summary statistics. While visualizing low-dimensional data is relatively straightforward (for example, plotting the change in a variable over time as (x,y) coordinates on a graph), it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way. Here we present HypeTools, a Python toolbox for visualizing and manipulating large, high-dimensional datasets. Our primary approach is to use dimensionality reduction techniques (Pearson, 1901; Tipping {\&} Bishop, 1999) to embed high-dimensional datasets in a lower-dimensional space, and plot the data using a simple (yet powerful) API with many options for data manipulation [e.g. hyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot styling. The toolbox is designed around the notion of data trajectories and point clouds. Just as the position of an object moving through space can be visualized as a 3D trajectory, HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories may be plotted as interactive static plots or visualized as animations. These same dimensionality reduction and alignment algorithms can also reveal structure in static datasets (e.g. collections of observations or attributes). We present several examples showcasing how using our toolbox to explore data through trajectories and low-dimensional embeddings can reveal deep insights into datasets across a wide variety of domains.},
archivePrefix = {arXiv},
arxivId = {1701.08290},
author = {Heusser, Andrew C. and Ziman, Kirsten and Owen, Lucy L. W. and Manning, Jeremy R.},
eprint = {1701.08290},
file = {:Users/carthach/Documents/Mendeley Desktop/1701.08290.pdf:pdf},
pages = {1--22},
title = {{HyperTools: A Python toolbox for visualizing and manipulating high-dimensional data}},
url = {http://arxiv.org/abs/1701.08290},
year = {2017}
}
@book{Beauchamp2007,
abstract = {This book contains a complete and accurate mathematical treatment of the sounds of music with an emphasis on musical timbre. The book spans the range from tutorial introduction to advanced research and application to speculative assessment of its various techniques. All the contributors use a generalized additive sine wave model for describing musical timbre which gives a conceptual unity, but is of sufficient utility to be adapted to many different tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Beauchamp, James W},
doi = {10.1007/978-0-387-32576-7},
eprint = {arXiv:1011.1669v3},
file = {:Users/carthach/Documents/Mendeley Desktop/Analysis, Synthesis, and Perception of Musical Sounds.pdf:pdf},
isbn = {978-0-387-32496-8},
issn = {00014966},
pmid = {4990342},
publisher = {Springer},
title = {{Analysis, Synthesis, and Perception of Musical Sounds}},
url = {http://link.springer.com/10.1007/978-0-387-32576-7},
year = {2007}
}
@article{Brown2017,
author = {Brown, Andrew R. and Gifford, Toby and Voltz, Bradley},
doi = {10.1145/2991146},
file = {:Users/carthach/Documents/Mendeley Desktop/Brown, Gifford, Voltz - 2017 - Stimulating Creative Partnerships in Human-Agent Musical Interaction.pdf:pdf},
issn = {15443574},
journal = {Computers in Entertainment},
number = {2},
pages = {1--17},
title = {{Stimulating Creative Partnerships in Human-Agent Musical Interaction}},
url = {http://dl.acm.org/citation.cfm?doid=3023311.2991146},
volume = {14},
year = {2017}
}
@article{Davies2014a,
abstract = {In this paper we present Improvasher a real-time musical accompaniment system which creates an automatic mashup to accompany live musical input. Improvasher is built around two music processing modules, the first, a performance following technique, makes beat-synchronous predictions of chroma features from a live musical input. The second, a music mashup system, determines the compatibility between beat-synchronous chromagrams from different pieces of music. Through the combination of these two techniques, a real-time time predict mashup can be generated towards a new form of automatic accompaniment for interactive musical performance.},
author = {Davies, Matthew and Stark, Adam and Gouyon, Fabien and Goto, Masataka},
file = {:Users/carthach/Documents/Mendeley Desktop/nime2014{\_}405.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
pages = {541--544},
title = {{Improvasher: A Real-Time Mashup System for Live Musical Input}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}405.pdf},
year = {2014}
}
@inproceedings{ONuanain2015,
address = {M{\'{a}}laga, Spain},
author = {{{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and Hermant, Martin and Faraldo, {\'{A}}ngel and G{\'{o}}mez, Daniel},
booktitle = {Late-Breaking Demo Session of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/carthach/Documents/Mendeley Desktop/LBD29.pdf:pdf},
title = {{The EEEAR: Building a Real-Time MIR-Based Instrument from a Hack}},
year = {2015}
}
@article{Cho2010,
abstract = {Most automatic chord recognition systems followa standard approach combining chroma feature extraction, filtering and pattern matching. However, despite much research, there is little understanding about the interaction between these different components, and the optimal parameterization of their variables. In this paper we perform a systematic evalu- ation including the most common variations in the literature. The goal is to gain insight into the potential and limitations of the standard approach, thus contributing to the identifi- cation of areas for future development in automatic chord recognition. In our study we find that filtering has a signifi- cant impact on performance, with self-transition penalties being the most important parameter; and that the benefits of using complex models are mostly, but not entirely, offset by an appropriate choice of filtering strategies.},
author = {Cho, Taemin and Weiss, Ron J and Bello, Juan P},
file = {:Users/carthach/Documents/Mendeley Desktop/Cho, Weiss, Bello - 2010 - Exploring Common Variations in State of the Art Chord Recognition Systems.pdf:pdf},
journal = {Sound and Music Computing},
number = {January},
pages = {11--22},
title = {{Exploring Common Variations in State of the Art Chord Recognition Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.167.8011{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {2010}
}
@article{Hove2014,
author = {Hove, Michael J and Marie, C{\'{e}}line and Bruce, Ian C and Trainor, Laurel J},
journal = {Proceedings of the National Academy of Sciences},
number = {28},
pages = {10383--10388},
publisher = {National Acad Sciences},
title = {{Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms}},
volume = {111},
year = {2014}
}
@inproceedings{Bock2011a,
abstract = {We present two new beat tracking algorithms based on the auto-correlation analysis, which showed state-of-the-art performance in the MIREX 2010 beat tracking contest. Unlike the traditional ap-proach of processing a list of onsets, we propose to use a bidirec-tional Long Short-Term Memory recurrent neural network to per-form a frame by frame beat classification of the signal. As inputs to the network the spectral features of the audio signal and their relative differences are used. The network transforms the signal directly into a beat activation function. An autocorrelation func-tion is then used to determine the predominant tempo to eliminate the erroneously detected -or complement the missing -beats. The first algorithm is tuned for music with constant tempo, whereas the second algorithm is further capable to follow changes in tempo and time signature.},
address = {Paris, France},
author = {B{\"{o}}ck, Sebastian and Sched},
booktitle = {14th Int. Conference on Digital Audio Effects (DAFx-11)},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck, Schedl - Unknown - ENHANCED BEAT TRACKING WITH CONTEXT-AWARE NEURAL NETWORKS.pdf:pdf},
isbn = {9782954035109},
pages = {1--5},
title = {{Enchanced Beat Tracking with Context-Aware Neural Networks}},
url = {http://www.cp.jku.at/research/papers/Boeck{\_}Schedl{\_}DAFx{\_}2011.pdf},
year = {2011}
}
@article{McDermott2006,
author = {McDermott, J and O'Neill, M and Griffith, NJL},
file = {:Users/carthach/Documents/Mendeley Desktop/McDermott, O'Neill, Griffith - 2006 - Target-driven genetic algorithms for synthesizer control.pdf:pdf},
journal = {9th Int. Conference on Digital Audio Effects},
pages = {1--15},
title = {{Target-driven genetic algorithms for synthesizer control}},
url = {http://jmmcd.skynet.ie/papers/target{\_}driven{\_}genetic{\_}algorithms{\_}for{\_}synthesizer{\_}control.pdf},
year = {2006}
}
@inproceedings{Assayag2006,
abstract = {We describe a multi-agent architecture for an improvization oriented musician-machine interaction system that learns in real time from human performers. The improvization kernel is based on sequence modeling and statistical learning. The working system involves a hybrid architecture using two popular composition/perfomance environments, Max and OpenMusic, that are put to work and communicate together, each one handling the process at a different time/memory scale. The system is capable of processing real-time audio/video as well as MIDI. After discussing the general cognitive background of improvization practices, the statistical modeling tools and the concurrent agent architecture are presented. Finally, a prospective Reinforcement Learning scheme for enhancing the system's realism is described. },
author = {Assayag, G and Bloch, G and Chemillier, M and Cont, A and Dubnov., S},
booktitle = {ACM Multimedia Workshop on Audio and Music Computing for Multimedia},
file = {:Users/carthach/Documents/Mendeley Desktop/Assayag et al. - 2006 - Omax brothers A dynamic topology of agents for improvisation learning.pdf:pdf},
title = {{Omax brothers: A dynamic topology of agents for improvisation learning}},
year = {2006}
}
@article{Conkie1999,
author = {Conkie, Alistair},
doi = {10.1121/1.425343},
file = {:Users/carthach/Documents/Mendeley Desktop/halfphones.pdf:pdf},
issn = {00014966},
journal = {The Journal of the Acoustical Society of America},
number = {2},
pages = {978},
title = {{A robust unit selection system for speech synthesis}},
url = {http://link.aip.org/link/JASMAN/v105/i2/p978/s2{\&}Agg=doi{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.9094{\&}rep=rep1{\&}type=pdf},
volume = {105},
year = {1999}
}
@article{Bello2004,
author = {Bello, Juan Pablo and Duxbury, Chris and Davies, Mike and Sandler, Mark},
journal = {IEEE Signal Processing Letters},
number = {6},
pages = {553--556},
publisher = {IEEE},
title = {{On the use of phase and energy for musical onset detection in the complex domain}},
volume = {11},
year = {2004}
}
@article{Chang2011,
author = {Chang, Chih-Chung and Lin, Chih-Jen},
journal = {ACM transactions on intelligent systems and technology (TIST)},
number = {3},
pages = {27},
publisher = {Acm},
title = {{LIBSVM: a library for support vector machines}},
volume = {2},
year = {2011}
}
@article{Eppstein1994,
abstract = {We give algorithms for finding the k shortest paths (not required to be simple) connecting a pair of vertices in a digraph. Our algorithms output an implicit representation of these paths in a digraph with n vertices and m edges, in time O(m+n log n+k). We can also find the k shortest paths from a given source s to each vertex in the graph, in total time O(m+n log n+kn). We describe applications to dynamic programming problems including the knapsack problem, sequence alignment, and maximum inscribed polygons},
author = {Eppstein, David},
doi = {10.1109/SFCS.1994.365697},
file = {:Users/carthach/Documents/Mendeley Desktop/Eppstein - 1994 - Finding the k shortest paths.pdf:pdf},
isbn = {0-8186-6580-7},
issn = {0097-5397},
journal = {35th Annual Symposium on Foundations of Computer Science},
keywords = {Biology computing,Computer science,Dynamic programming,Joining processes,Motion planning,Path planning,Road transportation,Robot motion,Routing,Shortest path problem,algorithms,computational geometry,digraph,dynamic programming,dynamic programming problems,implicit representation,k shortest paths,knapsack problem,maximum inscribed polygons,operations research,sequence alignment,vertices},
pages = {154--165},
title = {{Finding the k shortest paths}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=365697},
year = {1994}
}
@inproceedings{schroder2001emotional,
author = {Schr{\"{o}}der, Marc},
booktitle = {Seventh European Conference on Speech Communication and Technology},
title = {{Emotional speech synthesis: A review}},
year = {2001}
}
@article{Bernardes2017,
author = {Bernardes, Gilberto and Cocharro, Diogo and Guedes, Carlos and Davies, Matthew E. P.},
doi = {10.1145/2991145},
file = {:Users/carthach/Documents/Mendeley Desktop/Bernardes et al. - 2017 - Harmony Generation Driven by a Perceptually Motivated Tonal Interval Space.pdf:pdf},
issn = {15443574},
journal = {Computers in Entertainment},
number = {2},
pages = {1--21},
title = {{Harmony Generation Driven by a Perceptually Motivated Tonal Interval Space}},
url = {http://dl.acm.org/citation.cfm?doid=3023311.2991145},
volume = {14},
year = {2017}
}
@book{Forman2004,
author = {Forman, Murray and Neal, Mark Anthony},
file = {:Users/carthach/Documents/Mendeley Desktop/FormanNeal-Thats{\_}the{\_}Joint{\_}The{\_}Hip{\_}Hop{\_}Studies{\_}Readerbook.pdf:pdf},
isbn = {0415969182},
keywords = {hi,rap (music) - history and criticism,rap(music)-social aspects},
pages = {1--628},
publisher = {Psychology Press},
title = {{That's the joint!: the hip hop studies reader}},
year = {2004}
}
@article{Middleton2009,
author = {Middleton, Richard},
file = {:Users/carthach/Documents/Mendeley Desktop/853102.pdf:pdf},
journal = {Popular Music},
number = {1983},
pages = {235--270},
title = {{'Play It Again Sam': Some Notes on the Productivity of Repetition in Popular Music}},
volume = {3},
year = {2009}
}
@article{Ehmann2011,
author = {Ehmann, AF and Bay, Mert and Downie, JS and Fujinaga, Ichiro and Roure, David De},
file = {:Users/carthach/Documents/Mendeley Desktop/Ehmann et al. - 2011 - MUSIC STRUCTURE SEGMENTATION ALGORITHM EVALUATION EXPANDING ON MIREX 2010 ANALYSES AND DATASETS.pdf:pdf},
journal = {ISMIR},
number = {Ismir},
pages = {561--566},
title = {{MUSIC STRUCTURE SEGMENTATION ALGORITHM EVALUATION : EXPANDING ON MIREX 2010 ANALYSES AND DATASETS}},
url = {http://ismir2011.ismir.net/papers/PS4-15.pdf},
year = {2011}
}
@article{Collins2003,
author = {Collins, Nick},
doi = {10.1080/0749446032000156919},
file = {:Users/carthach/Documents/Mendeley Desktop/Collins - 2003 - Generative music and laptop performance.pdf:pdf},
issn = {0749-4467},
journal = {Contemporary Music Review},
month = {dec},
number = {4},
pages = {67--79},
title = {{Generative music and laptop performance}},
url = {http://www.tandfonline.com/doi/abs/10.1080/0749446032000156919},
volume = {22},
year = {2003}
}
@article{Pasquier2017,
author = {Pasquier, Philippe and Eigenfeldt, Arne and Bown, Oliver and Dubnov, Shlomo},
doi = {10.1145/2930672},
file = {:Users/carthach/Documents/Mendeley Desktop/Pasquier et al. - 2017 - An Introduction to Musical Metacreation.pdf:pdf},
isbn = {009031591X},
issn = {15443574},
journal = {Computers in Entertainment},
number = {2},
pages = {1--14},
title = {{An Introduction to Musical Metacreation}},
url = {http://dl.acm.org/citation.cfm?doid=3023311.2930672},
volume = {14},
year = {2017}
}
@misc{Skau2012,
author = {Skau, Drew},
booktitle = {visual.ly},
title = {{Euler and Venn Diagrams: They Aren't Just for Fun}},
url = {http://blog.visual.ly/euler-and-venn-diagrams/},
urldate = {2015-09-25},
year = {2012}
}
@article{Serr2010a,
author = {Serr, Joan and Meinard, M and Grosche, Peter and Arcos, Josep Ll},
file = {:Users/carthach/Documents/Mendeley Desktop/Serr et al. - 2010 - Unsupervised Detection of Music Boundaries by Time Series Structure Features.pdf:pdf},
number = {2009},
pages = {1613--1619},
title = {{Unsupervised Detection of Music Boundaries by Time Series Structure Features}},
year = {2010}
}
@article{Deng2008,
author = {Deng, JD},
file = {:Users/carthach/Documents/Mendeley Desktop/Deng - 2008 - A Study on Feature Analysis for Musical Instrument Classification.pdf:pdf},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {2},
pages = {429--438},
title = {{A Study on Feature Analysis for Musical Instrument Classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4436069},
volume = {38},
year = {2008}
}
@article{Grekow2017,
author = {Grekow, Jacek},
doi = {10.1109/INISTA.2017.8001129},
file = {:Users/carthach/Documents/Mendeley Desktop/08001129.pdf:pdf},
isbn = {978-1-5090-5795-5},
journal = {2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},
keywords = {audio features,features,music emotion detection},
pages = {40--44},
title = {{Audio features dedicated to the detection of arousal and valence in music recordings}},
url = {http://ieeexplore.ieee.org/document/8001129/},
year = {2017}
}
@misc{Schloss1985,
abstract = {This dissertation is concerned with the use of a computer to analyze and understand rhythm in music. The research focuses on the development of a program that automatically transcribes percussive music, investigating issues of timing and rhythmic complexity in a rich musical setting. Beginning with a recording of an improvised performance, the intent is to be able to produce a score of the performance, to be able to resynthesize the performance in various ways, and also to make inferencesabout rhythmic structure and style. In order to segment percussive sound from the given acoustic waveform, automatic slope-detection algorithms have been developed and implemented. Initially, a simple amplitude envelope is found by tracing the peaks of the waveform. This provides a data reduction of about 200:1 and is useful for obtaining an overview of the musical material. The data are then segmented by repeatedly performing a linear regression over a small moving window of the envelope data, moving the window one point at a time over the envelope. The linear regressions create a sequence of line segments that "float" over the data and allow segmentation by carefully set slope thresholds. The slope threshold determines the attacks. Once the attacks are determined, the decay time-constant, tau, is determined by fitting a one-pole model to the amplitude envelope. From the value of tau, a decision can be made as to whether a given stroke is damped or undamped. This corresponds to the method of striking the drum. Once the damped/undamped decision is made, a portion of the original time waveform is sent to a "stroke-detector" that determines how the drum was struck in greater detail. At this point, enough information about the performance has been obtained to begin a higher-level analysis. Given the timing information and the patterns of strokes, it is possible to track tempo automatically, and to try to make inferences about the meter. These two issues are in fact quite deep, and are the focus of a body of work that involves detection of "macro-periodicity", that is a repetition rate over longer periods of time than signal processing would normally yield. Also included in this thesis is an historical and theoretical overview of research on rhythm, from several perspectives.},
author = {Schloss, A. W.},
booktitle = {Proceedings of the 18th International Congress on Acoustics},
file = {:Users/carthach/Documents/Mendeley Desktop/Schloss - 1985 - On the Automatic Transcription of Percussive Music - From Acoustic Signal to High-Level Analysis.pdf:pdf},
number = {27},
title = {{On the Automatic Transcription of Percussive Music - From Acoustic Signal to High-Level Analysis}},
url = {http://ccrma.stanford.edu/STANM/stanms/stanm27/stanm27.pdf},
year = {1985}
}
@article{Tomas2014,
abstract = {Tangible Scores are a new paradigm for musical instrument design with a physical configuration inspired by graphic scores. In this paper we will focus on the design aspects of this new interface as well as on some of the related technical details. Creating an intuitive, modular and expressive instrument for textural music was the primary driving force. Following these criteria, we literally incorporated a musical score onto the surface of the instrument as a way of continuously controlling several parameters of the sound synthesis. Tangible Scores are played with both hands and they can adopt multiple physical forms. Complex and expressive sound textures can be easily played over a variety of timbres, enabling precise control in a natural manner.},
author = {Tom{\'{a}}s, Enrique and Kaltenbrunner, Martin},
file = {:Users/carthach/Documents/Mendeley Desktop/Tangible{\_}Scores{\_}Shaping{\_}the{\_}Inherent{\_}Ins.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {composition,nime,notation,scores,tangible interface},
pages = {609--614},
title = {{Tangible Scores: Shaping the Inherent Instrument Score}},
url = {http://www.nime.org/proceedings/2014/nime2014{\_}352.pdf},
year = {2014}
}
@article{Bischof2008,
author = {Bischof, Markus and Conradi, Bettina and Lachenmaier, Peter},
file = {:Users/carthach/Documents/Mendeley Desktop/Bischof, Conradi, Lachenmaier - 2008 - Xenakis combining tangible interaction with probability-based musical composition.pdf:pdf},
isbn = {9781605580043},
journal = {{\ldots} embedded interaction},
keywords = {and rhythm can be,controlled by simply moving,it generates music,like striking certain keys,modifying this model while,of a piano,on the surface,pitch,tangibles,tone},
pages = {121--124},
title = {{Xenakis: combining tangible interaction with probability-based musical composition}},
url = {http://dl.acm.org/citation.cfm?id=1347416},
year = {2008}
}
@article{C2016,
abstract = {Protein-protein interactions play a central role in the biological processes of cells. Accurate prediction of the interacting residues in protein-protein interactions enhances understanding of the interaction mechanisms and enables in silico mutagenesis, which can help facilitate drug design and deepen our understanding of the inner workings of cells. Correlations have been found among interacting residues as a result of selection pressure to retain the interaction during evolution. In previous work, incorporation of such correlations in the interaction profile hidden Markov models with a special decoding algorithm (ETB-Viterbi) has led to improvement in prediction accuracy. In this work, we first demonstrated the sub-optimality of the ETB-Viterbi algorithm, and then reformulated the optimality of decoding paths to include correlations between interacting residues. To identify optimal decoding paths, we propose a post-decoding re-ranking algorithm based on a genetic algorithm with simulated annealing and show that the new method gains an increase of near 14{\%} in prediction accuracy over the ETB-Viterbi algorithm.},
author = {C, Kern and L, Liao},
doi = {10.1016/j.compbiolchem.2016.09.015},
file = {:Users/carthach/Documents/Mendeley Desktop/C, L - 2016 - A post-decoding re-ranking algorithm for predicting interacting residues in proteins with hidden Markov models incorporati.pdf:pdf},
issn = {1476-928X},
journal = {Computational Biology and Chemistry},
keywords = {protein,protein interaction},
pages = {21--28},
pmid = {27718452},
publisher = {Elsevier Ltd},
title = {{A post-decoding re-ranking algorithm for predicting interacting residues in proteins with hidden Markov models incorporating long-distance information}},
url = {http://dx.doi.org/10.1016/j.compbiolchem.2016.09.015},
volume = {65},
year = {2016}
}
@article{McAdams1995,
abstract = {To study the perceptual structure of musical timbre and the effects of musical training, timbral dissimilarities of synthesized instrument sounds were rated by professional musicians, amateur musicians, and nonmusicians. The data were analyzed with an extended version of the multidimensional scaling algorithm CLASCAL (Winsberg {\&} De Soete, 1993), which estimates the number of latent classes of subjects, the coordinates of each timbre on common Euclidean dimensions, a specificity value of unique attributes for each timbre, and a separate weight for each latent class on each of the common dimensions and the set of specificities. Five latent classes were found for a three-dimensional spatial model with specificities. Common dimensions were quantified psychophysically in terms of log-rise time, spectral centroid, and degree of spectral variation. The results further suggest that musical timbres possess specific attributes not accounted for by these shared perceptual dimensions. Weight patterns indicate that perceptual salience of dimensions and specificities varied across classes. A comparison of class structure with biographical factors associated with degree of musical training and activity was not clearly related to the class structure, though musicians gave more precise and coherent judgments than did non-musicians or amateurs. The model with latent classes and specificities gave a better fit to the data and made the acoustic correlates of the common dimensions more interpretable.},
author = {McAdams, Stephen and Winsberg, Suzanne and Donnadieu, Sophie and {De Soete}, Geert and Krimphoff, Jochen},
doi = {10.1007/BF00419633},
file = {:Users/carthach/Documents/Mendeley Desktop/McAdams{\_}1995{\_}PsycholRes.pdf:pdf},
isbn = {0340-0727 (Print) 0340-0727 (Linking)},
issn = {03400727},
journal = {Psychological Research},
number = {3},
pages = {177--192},
pmid = {8570786},
title = {{Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes}},
volume = {58},
year = {1995}
}
@article{Cascone2000,
author = {Cascone, Kim},
journal = {Computer Music Journal},
number = {4},
pages = {12--18},
publisher = {MIT Press},
title = {{The aesthetics of failure:``Post-digital'' tendencies in contemporary computer music}},
volume = {24},
year = {2000}
}
@article{King2010,
abstract = {Statistical parametric speech synthesis, based on HMM-like models, has become competitive with established concatenative techniques over the last few years. This paper offers a non-mathematical introduction to this method of speech synthesis. It is intended to be complementary to the wide range of excellent technical publications already available. Rather than offer a comprehensive literature review, this paper instead gives a small number of carefully chosen references which are good starting points for further reading.},
author = {King, Simon},
file = {:Users/carthach/Documents/Mendeley Desktop/King - 2010 - A beginners' guide to statistical parametric speech synthesis.pdf:pdf},
journal = {Synthesis},
pages = {1--16},
title = {{A beginners' guide to statistical parametric speech synthesis}},
year = {2010}
}
@phdthesis{Submitted2009,
author = {Loughran, R{\'{o}}is{\'{i}}n and Walker, Jacqueline and Griffith, Niall},
file = {:Users/carthach/Documents/Mendeley Desktop/LoughranThesis.pdf:pdf},
number = {September},
school = {University of Limerick},
title = {{Musical Instrument Identification with Feature Selection Using Evolutionary Methods}},
year = {2009}
}
@article{Humphrey2013,
author = {Humphrey, Eric J and Turnbull, Douglas and Collins, Tom},
file = {:Users/carthach/Documents/Mendeley Desktop/Humphrey, Turnbull, Collins - 2013 - A brief review of creative MIR.pdf:pdf},
journal = {International Society for Music Information Retrieval},
title = {{A brief review of creative MIR}},
year = {2013}
}
@article{davis1991handbook,
author = {Davis, Lawrence},
title = {{Handbook of genetic algorithms}},
year = {1991}
}
@article{Chiarandini2010b,
author = {Chiarandini, Luca and Zanoni, Dott Massimiliano and Juan, Prof and Martin, Carlos De},
file = {:Users/carthach/Documents/Mendeley Desktop/Chiarandini et al. - 2010 - Automatic Audio Compositing System based on Music Information Retrieval.pdf:pdf},
title = {{Automatic Audio Compositing System based on Music Information Retrieval}},
year = {2010}
}
@article{Albiez2005,
abstract = {Techno is a globally successful genre of electronic dance music that can trace its origins to Detroit in the early 1980s, but it has been generally overlooked in academic, historical and critical analyses of 1980s African American music in the United States. This study will consider the cultural context of early Techno, its relationship to European electronic music, the birth of the „Techno‟ genre in the UK, the founding myths and histories of Techno, and its potential for helping us understand transformations in African American cultural politics in the 'post-soul‟ era.},
author = {Albiez, Sean},
doi = {10.1386/ejac.24.2.131/1},
file = {:Users/carthach/Documents/Mendeley Desktop/socidoc.com{\_}post-soul-futurama-early-detroit-techno-and-african-american-cultural-politics.pdf:pdf},
isbn = {1466-0407},
issn = {14660407},
journal = {European Journal of American Culture},
keywords = {african-american,cultural,detroit,identity,music,techno},
number = {2},
pages = {131--152},
title = {{Post-soul futurama: African American cultural politics and early Detroit techno}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1466-0407{\&}volume=24{\&}issue=2{\&}spage=131},
volume = {24},
year = {2005}
}
@article{Nill1995,
abstract = {The Viterbi Algorithm (VA) is the optimum decoding algorithm for a convolutional code. Improvements in the performance of a concatenated coding system that uses VA decoding (inner decoder) can be obtained when, in addition to the standard output, an indicator of the reliability of the VA decision is delivered to the outer stage of processing. Two different approaches of extending the VA are considered. In the first approach, the VA is extended with a Soft Output unit (SOVA) that calculates, based on the difference between the cumulative metrics of the two paths merging at each time instant and state, reliability values for each of the decoded information symbols. In the second approach, coding gains are obtained by delivering, in addition to the best path, the next L = 1 best estimates of the transmitted data sequence. Here, the output format is a list of size L. This is a List VA (LVA). In this work, we evaluate LVA and SOVA in comparison to each other and attain extended versions of LVA and SOVA with low complexity that implement the other algorithm. We construct and evaluate a List-SOVA using the reliability information of the SOVA to generate a list of size L and that also has a lower complexity than the LVA for a long list size. Further, we introduce a low complexity algorithm that accepts the list output of the LVA and calculates for each of the decoded information bits a reliability value. The complexity and the performance of this Soft-LVA is a function of the list size L. The performances of Soft-LVA and SOVA are compared in concatenated coding systems.},
author = {Nill, Christiane and Sundberg, Carl Erik W.},
doi = {10.1109/26.380046},
file = {:Users/carthach/Documents/Mendeley Desktop/Nill, Sundberg - 1995 - List and Soft Symbol Output Viterbi Algorithms Extensions and Comparisons.pdf:pdf;:Users/carthach/Documents/Mendeley Desktop/00380046.pdf:pdf},
isbn = {0780308786 (ISBN)},
issn = {00906778},
journal = {IEEE Transactions on Communications},
number = {234},
pages = {277--287},
title = {{List and Soft Symbol Output Viterbi Algorithms: Extensions and Comparisons}},
volume = {43},
year = {1995}
}
@conference{Faraldo2017b,
abstract = {In this paper, we present a study of tonal practises in Electronic Dance Music (EDM), an umbrella term referring to a number of subgenres such as house, techno or jungle, originating in the 1980{\{}$\backslash$textquoteright{\}}s and intended for dancing at nightclubs and raves. EDM is produced mainly with electronic equipment and has a strong presence of percussive elements and a steady beat. Based on corpus analysis of audio and midi files, we en-quire what the role of commonplace digital production techniques {\{}$\backslash$textendash{\}}such as layering and looping{\{}$\backslash$textendash{\}} could be in defining some tonal practises in Electronic Dance Music.},
address = {Strasbourg, France},
author = {Faraldo, {\'{A}}ngel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {Ninth European Music Analysis Conference},
title = {{A Study of Tonal Practises In Electronic Dance Music}},
year = {2017}
}
@inproceedings{Grill2012,
abstract = {This paper describes the construction of computable audio descriptors capable of modeling relevant high-level perceptual qualities of textural sounds. These qualities – all metaphoric bipolar and continuous constructs – have been identified in previous research: high–low, ordered– chaotic, smooth–coarse, tonal–noisy, and homogeneous– heterogeneous, covering timbral, temporal and structural properties of sound. We detail the construction of the descriptors and demonstrate the effects of tuning with respect to individual accuracy or mutual independence. The descriptors are evaluated on a corpus of 100 textural sounds against respective measures of human perception that have been retrieved by use of an online survey. Potential future use of perceptual audio descriptors in music creation is illustrated by a prototypic sound browser application.},
author = {Grill, Thomas},
booktitle = {9th Sound and Music Computing Conference 2012},
file = {:Users/carthach/Documents/Mendeley Desktop/smc2012-235.pdf:pdf},
isbn = {9783832531805},
pages = {235},
title = {{Constructing high-level perceptual audio descriptors for textural sounds}},
url = {http://smcnetwork.org/system/files/smc2012-235.pdf},
year = {2012}
}
@article{Gouyon2000,
abstract = {We address the issue of automatically extracting rhythm descriptors from audio signals, to be eventually used in content-based musical applications such as in the context of MPEG7. Our aim is to approach the comprehension of auditory scenes in raw polyphonic audio signals without preliminary source separation. As a first step towards the automatic extraction of rhythmic structures out of signals taken from the popular music repertoire, we propose an approach for automatically extracting time indexes of occurrences of different percussive timbres in an audio signal. Within this framework, we found that a particular issue lies in the classification of percussive sounds. In this paper, we report on the method currently used to deal with this problem.},
author = {Gouyon, Fabien and Pachet, Fran{\c{c}}ois and Delerue, Olivier},
file = {:Users/carthach/Documents/Mendeley Desktop/Gouyon, Pachet, Delerue - 2000 - On the use of Zero-Crossing rate for an application of classification of percussive sounds.pdf:pdf},
journal = {Dafx},
pages = {3--8},
title = {{On the use of Zero-Crossing rate for an application of classification of percussive sounds}},
year = {2000}
}
@article{Kaltenbrunner2007,
author = {Kaltenbrunner, Martin and Bencina, Ross},
file = {:Users/carthach/Documents/Mendeley Desktop/Kaltenbrunner, Bencina - 2007 - reacTIVision a computer-vision framework for table-based tangible interaction.pdf:pdf},
journal = {{\ldots} on Tangible and embedded interaction},
title = {{reacTIVision: a computer-vision framework for table-based tangible interaction}},
url = {http://dl.acm.org/citation.cfm?id=1226983},
year = {2007}
}
@article{Lee2015,
abstract = {In this paper, we proposed a system to effectively create music mashups – a kind of re-created music that is made by mixing parts of multiple existing music pieces. Unlike previous studies which merely generate mashups by over-laying music segments on one single base track, the pro-posed system creates mashups with multiple background (e.g. instrumental) and lead (e.g. vocal) track segments. So, besides the suitability between the vertically overlaid tracks (i.e. vertical mashability) used in previous studies, we proposed to further consider the suitability between the horizontally connected consecutive music segments (i.e. horizontal mashability) when searching for proper music segments to be combined. On the vertical side, two new factors: " harmonic change balance " and " volume weight " have been considered. On the horizontal side, the meth-ods used in the studies of medley creation are incorporated. Combining vertical and horizontal mashabilities together, we defined four levels of mashability that may be encoun-tered and found the proper solution to each of them. Sub-jective evaluations showed that the proposed four levels of mashability can appropriately reflect the degrees of listen-ing enjoyment. Besides, by taking the newly proposed ver-tical mashability measurement into account, the improve-ment in user satisfaction is statistically significant.},
author = {Lee, Chuan-lung and Lin, Yin-Tzu and Yao, Zun-Ren and Lee, Feng-Yi and Wu, Ja-Ling},
file = {:Users/carthach/Documents/Mendeley Desktop/302{\_}Paper.pdf:pdf},
journal = {Proc. International Society for Music Information Retrieval Conference (ISMIR)},
pages = {399--405},
title = {{Automatic Mashup Creation By Considering Both Vertical And Horizontal Mashabilities}},
year = {2015}
}
@inproceedings{Faraldo2015,
address = {M{\'{a}}laga, Spain},
author = {Faraldo, {\'{A}}ngel and {{\'{O}} Nuan{\'{a}}in}, C{\'{a}}rthach and G{\'{o}}mez, Daniel and Herrera, Perfecto and Jord{\`{a}}, Sergi},
booktitle = {Late-Breaking Demo Session of the International Society for Music Information Retrieval Conference (ISMIR)},
file = {:Users/carthach/Documents/Mendeley Desktop/LBD38.pdf:pdf},
title = {{Making Electronic Music with Expert Musical Agents}},
year = {2015}
}
@article{Marin2015,
abstract = {Measuring rhythm similarity is relevant for the analysis and generation of music. Existing similarity metrics tend to consider our perception of rhythms as homogeneous in time without discriminating the importance of some re- gions from others. On a previously reported experiment we observed that measures of similarity may differ given the presence or absence of a pulse inducing sound and the importance of those measures is not constant along the pat- tern. These results are now reinterpreted by refining the previously proposed metrics. We consider that the percep- tual contribution of each beat to the measured similarity is non-homogeneous but might indeed depend on the tem- poral positions of the beat along the bar. We show that with these improvements, the correlation between the pre- viously evaluated experimental similarity and predictions based on our metrics increases substantially. We conclude by discussing a possible new methodology for evaluating rhythmic similarity between audio loops.},
author = {G{\'{o}}mez-Mar{\'{i}}n, Daniel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
file = {:Users/carthach/Documents/Mendeley Desktop/Gomez-Marin, Jordà, Herrera - 2015 - Pad and Sad Two Awareness-Weighted Rhythmic Similarity Distances.pdf:pdf},
journal = {16th International Society for Music Information Retrieval Conference},
title = {{Pad and Sad: Two Awareness-Weighted Rhythmic Similarity Distances}},
year = {2015}
}
@inproceedings{Zils2002,
author = {Zils, Aymeric and Pachet, Fran{\c{c}}ois and Delerue, Olivier and Gouyon, Fabien},
booktitle = {Web Delivering of Music, 2002. WEDELMUSIC 2002. Proceedings. Second International Conference on},
file = {:Users/carthach/Documents/Mendeley Desktop/01176209.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {179--183},
title = {{Automatic extraction of drum tracks from polyphonic music signals}},
year = {2002}
}
@phdthesis{Malmberg2010,
author = {Malmberg, Viljo},
file = {:Users/carthach/Documents/Mendeley Desktop/optika{\_}id{\_}1057{\_}malmberg{\_}viljo{\_}2010.pdf:pdf},
school = {Aalto University},
title = {{Iris: A Circular Polyrhythmic Music Sequencer}},
year = {2010}
}
@article{Neill2002,
abstract = {The division between high-art electronic music and pop electronic music is best defined in terms of rhythmic content. Pop electronic music uses repetitive beats, primarily in 4/4 time, but a new generation of composers is working within that structure to create what is essentially new art music. This phenomenon is an outgrowth of such historical currents as minimalism and postmodernism, along with the continuing development of a global technoculture; it is part of a larger cultural shift in which art is becoming more connected with society rather than being created by and for specialists. This positive development is being accelerated by the rapid evolution of new technologies for producing and reproducing music today, as well as by new possibilities for distribution and dissemination of music electornically.},
author = {Neill, Ben},
doi = {10.1162/096112102762295052},
file = {:Users/carthach/Documents/Mendeley Desktop/1513341.pdf:pdf},
isbn = {0961-1215},
issn = {0961-1215},
journal = {Leonardo},
number = {2002},
pages = {3--6},
title = {{Pleasure Beats: Rhythm and the Aesthetics of Current Electronic Music}},
volume = {12},
year = {2002}
}
@article{Loughran2004,
author = {Loughran, R{\'{o}}is{\'{i}}n and Walker, Jacqueline and O'Neill, Michael and O'Farrell, Marion},
file = {:Users/carthach/Documents/Mendeley Desktop/Loughran et al. - 2004 - The Use of Mel-frequency Cepstral Coefficients in Musical Instrument Identification.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {42--43},
title = {{The Use of Mel-frequency Cepstral Coefficients in Musical Instrument Identification}},
year = {2004}
}
@book{Helander2014,
author = {Helander, Martin G},
publisher = {Elsevier},
title = {{Handbook of human-computer interaction}},
year = {2014}
}
@article{Zhang2007,
author = {Zhang, Xin and Ras, ZW},
file = {:Users/carthach/Documents/Mendeley Desktop/Zhang, Ras - 2007 - Analysis of sound features for music timbre recognition.pdf:pdf},
journal = {{\ldots} and Ubiquitous Engineering, 2007. MUE'07 {\ldots}},
title = {{Analysis of sound features for music timbre recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4197241},
year = {2007}
}
@article{Carabias-Orti2015,
abstract = {In this paper, we present an audio to score alignment framework based on spectral factorization and online Dy-namic Time Warping (DTW). The proposed framework has two separated stages: preprocessing and alignment. In the first stage, we use Non-negative Matrix Factoriza-tion (NMF) to learn spectral patterns (i.e. basis functions) associated to each combination of concurrent notes in the score. In the second stage, a low latency signal decomposi-tion method with fixed spectral patterns per combination of notes is used over the magnitude spectrogram of the input signal resulting in a divergence matrix that can be inter-preted as the cost of the matching for each combination of notes at each frame. Finally, a Dynamic Time Warping (DTW) approach has been used to find the path with the minimum cost and then determine the relation between the performance and the musical score times. Our framework have been evaluated using a dataset of baroque-era pieces and compared to other systems, yielding solid results and performance.},
author = {Carabias-Orti, J J and Rodriguez-Serrano, F.J. and Vera-Candeas, P and Ruiz-Reyes, N and Canadas-Quesada, F. J.},
file = {:Users/carthach/Documents/Mendeley Desktop/Carabias-Orti et al. - 2015 - An audio to score alignment framework using spectral factorization and dynamic time warping.pdf:pdf},
journal = {ISMIR: proceedings of the International Conference of Music Information Retrieval},
pages = {742--748},
title = {{An audio to score alignment framework using spectral factorization and dynamic time warping}},
year = {2015}
}
@article{Holm-Hudson1997,
abstract = {Although the music industry (not without inconsistency) has insisted that digital sampling is "theft," it is perhaps better viewed in historical and theoretical context as timbral quotation. Often the sample functions as a quote that is recontextualized but that nevertheless bears the weight of its original context. The author compares some attempts at creating a taxonomy for sampling use and concludes with a discussion of the controversial "plunderphonic" work of composer John Oswald. Oswald's work reveals an often overlooked aspect of contemporary popular music in the age of mechanical reproduction: his samples refer not only to specific songs (where they are recognizable), but also to the timbres associated with entire genres.},
author = {Holm-Hudson, Kevin},
file = {:Users/carthach/Documents/Mendeley Desktop/1513241.pdf:pdf},
issn = {09611215, 15314812},
journal = {Leonardo Music Journal},
number = {May},
pages = {17--25},
title = {{Quotation and Context: Sampling and John Oswald's Plunderphonics}},
url = {http://www.jstor.org.pitt.idm.oclc.org/stable/1513241?seq=4{\&}Search=yes{\&}searchText=plunderphonics{\&}list=hide{\&}searchUri=/action/doBasicSearch?Query=plunderphonics{\&}acc=on{\&}wc=on{\&}fc=off{\&}prevSearch={\&}item=1{\&}ttl=32{\&}returnArticleService=showFullText{\&}resultsServiceN},
volume = {7},
year = {1997}
}
@article{Wenger2012,
author = {Wenger, Stephan and Magnor, Marcus},
doi = {10.1145/2393347.2396292},
file = {:Users/carthach/Documents/Mendeley Desktop/Wenger, Magnor - 2012 - A genetic algorithm for audio retargeting.pdf:pdf},
isbn = {9781450310895},
journal = {Proceedings of the 20th ACM international conference on Multimedia - MM '12},
keywords = {audio retargeting,audio synthesis,example-based synthesis},
pages = {705},
title = {{A genetic algorithm for audio retargeting}},
url = {http://dl.acm.org/citation.cfm?doid=2393347.2396292},
year = {2012}
}
@book{Chang2007,
author = {Chang, Jeff},
publisher = {St. Martin's Press},
title = {{Can't stop won't stop: A history of the hip-hop generation}},
year = {2007}
}
@inproceedings{Sandvold2004,
abstract = {This paper deals with automatic percussion in polyphonic audio recordings, focusing on kick, snare and cymbal sounds. We present a feature-based modeling approach that combines general, prior knowl-},
author = {Sandvold, Vegard and Gouyon, Fabien and Herrera, P},
booktitle = {5th International Conference on Music Information Retrieval (ISMIR'04)},
file = {:Users/carthach/Documents/Mendeley Desktop/Sandvold, Gouyon, Herrera - 2004 - Percussion Classification in Polyphonic Audio Recordings Using Localized Sound Models.pdf:pdf},
isbn = {84-88042-44-2},
pages = {2--5},
title = {{Percussion Classification in Polyphonic Audio Recordings Using Localized Sound Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.9869{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@book{Reich2011,
abstract = {In the mid-1960s, Steve Reich radically renewed the musical landscape with a back-to-basics sound that came to be called Minimalism. These early works, characterized by a relentless pulse and static harmony, focused single-mindedly on the process of gradual rhythmic change. Throughout his career, Reich has continued to reinvigorate the music world, drawing from a wide array of classical, popular, sacred, and non-western idioms. His works reflect the steady evolution of an original musical mind. Writings on Music documents the creative journey of this thoughtful, groundbreaking composer. These 64 short pieces include Reich's 1968 essay "Music as a Gradual Process," widely considered one of the most influential pieces of music theory in the second half of the 20th century. Subsequent essays, articles, and interviews treat Reich's early work with tape and phase shifting, showing its development into more recent work with speech melody and instrumental music. Other essays recount his exposure to non-western music--African drumming, Balinese gamelan, Hebrew cantillation--and the influence of these musics as structures and not as sounds. The writings include Reich's reactions to and appreciations of the works of his contemporaries (John Cage, Luciano Berio, Morton Feldman, Gyorgy Ligeti) and older influences (Kurt Weill, Schoenberg). Each major work of the composer's career is also explored through notes written for performances and recordings. Paul Hillier, himself a respected figure in the early music and new music worlds, has revisited these texts, working with the author to clarify their central narrative: the aesthetic and intellectual development of an influential composer. For long-time listeners and young musicians recently introduced to his work, this book provides an opportunity to get to know Reich's music in greater depth and perspective.},
author = {Reich, Steve and Hillier, Paul},
booktitle = {Writings on Music 1965-2000: 1965-2000},
doi = {10.1093/acprof:oso/9780195151152.001.0001},
isbn = {9780199850044},
keywords = {African drumming,Balinese gamelan,Early work,Harmony,Hebrew cantillation,Influences,Minimalism,Rhythmic change,Steve Reich,Structures},
pages = {1--270},
title = {{Writings on Music 1965-2000: 1965-2000}},
year = {2011}
}
@misc{Bjork2014,
author = {Guardian, The},
booktitle = {The Guardian},
title = {{Bjork: Biophilia App}},
url = {http://www.theguardian.com/music/musicblog/2011/jul/20/bjork-biophilia-app},
year = {2011}
}
@article{Bretan2016,
abstract = {Several methods exist for a computer to generate music based on data including Markov chains, recurrent neural networks, recombinancy, and grammars. We explore the use of unit selection and concatenation as a means of generating music using a procedure based on ranking, where, we consider a unit to be a variable length number of measures of music. We first examine whether a unit selection method, that is restricted to a finite size unit library, can be sufficient for encompassing a wide spectrum of music. We do this by developing a deep autoencoder that encodes a musical input and reconstructs the input by selecting from the library. We then describe a generative model that combines a deep structured semantic model (DSSM) with an LSTM to predict the next unit, where units consist of four, two, and one measures of music. We evaluate the generative model using objective metrics including mean rank and accuracy and with a subjective listening test in which expert musicians are asked to complete a forced-choiced ranking task. We compare our model to a note-level generative baseline that consists of a stacked LSTM trained to predict forward by one note.},
archivePrefix = {arXiv},
arxivId = {1612.03789},
author = {Bretan, Mason and Weinberg, Gil and Heck, Larry},
eprint = {1612.03789},
file = {:Users/carthach/Documents/Mendeley Desktop/1612.03789.pdf:pdf},
pages = {1--13},
title = {{A Unit Selection Methodology for Music Generation Using Deep Neural Networks}},
url = {http://arxiv.org/abs/1612.03789},
year = {2016}
}
@article{Seshadri1994,
abstract = {A list Viterbi decoding algorithm (LVA) produces a rank ordered list of the L globally best candidates after a trellis search. The authors present two such algorithms, (i) a parallel LVA that simultaneously produces the L best candidates and (ii) a serial LVA that iteratively produces the kth best candidate based on knowledge of the previously found k-1 best paths. The application of LVA to a concatenated communication system consisting of an inner convolutional code and an outer error detecting code is considered in detail. Analysis as well as simulation results show that significant improvement in error performance is obtained when the inner decoder, which is conventionally based on the Viterbi algorithm (VA), is replaced by the LVA. An improvement of up to 3 dB is obtained for the additive white Gaussian noise (AWGN) channel due to an increase in the minimum Euclidean distance. Ever larger gains are obtained for the Rayleigh fading channel due to an increase in the time diversity. It is also shown that a 10{\%} improvement in throughput is obtained along with significantly reduced probability of a decoding failure for a hybrid FEC/ARQ scheme with the inner code being a rate compatible punctured convolutional (RCPC) code},
author = {Seshadri, N. and Sundberg, C.-E.W.},
doi = {10.1109/TCOMM.1994.577040},
file = {:Users/carthach/Documents/Mendeley Desktop/Seshadri, Sundberg - 1994 - List Viterbi decoding algorithms with applications.pdf:pdf},
isbn = {0090-6778 VO  - 42},
issn = {0090-6778},
journal = {IEEE Transactions on Communications},
keywords = {AWGN,AWGN channel,Additive white noise,Algorithm design and analysis,Analytical models,Concatenated codes,Convolutional codes,Iterative algorithms,Iterative decoding,Performance analysis,Rayleigh fading channel,Viterbi algorithm,additive white Gaussian noise,coding errors,concatenated communication system,convolutional codes,data communication systems,decoding,decoding failure probability,error detection codes,error performance,globally best candidates,hybrid FEC/ARQ,inner convolutional code,inner decoder,iterative algorithm,list Viterbi decoding algorithm,maximum likelihood estimation,minimum Euclidean distance,outer error detecting code,parallel LVA,parallel algorithms,probability,random noise,rank ordered list,rate compatible punctured convolutional code,serial LVA,simulation results,throughput,time diversity,trellis search,white noise},
number = {2/3/4},
pages = {313--323},
title = {{List Viterbi decoding algorithms with applications}},
volume = {42},
year = {1994}
}
@article{Dannenberg2006,
author = {Dannenberg, Roger B},
file = {:Users/carthach/Documents/Mendeley Desktop/Concatenative-Synthesis-ICMC-2006.pdf:pdf},
journal = {International Computer Music Conference},
pages = {352--355},
title = {{Concatenative Synthesis Using Score-Aligned Transcriptions Music Analysis and Segmentation}},
year = {2006}
}
@inproceedings{Kaliakatsos-Papakostas2013,
author = {Kaliakatsos-Papakostas, Maximos a. and Floros, Andreas and Vrahatis, Michael N.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-36955-1_3},
file = {:Users/carthach/Documents/Mendeley Desktop/Kaliakatsos-Papakostas, Floros, Vrahatis - 2013 - evoDrummer Deriving rhythmic patterns through interactive genetic algorithms.pdf:pdf},
isbn = {9783642369544},
issn = {03029743},
pages = {25--36},
title = {{evoDrummer: Deriving rhythmic patterns through interactive genetic algorithms}},
volume = {7834 LNCS},
year = {2013}
}
@article{Chai2001,
abstract = {Automatic music classification is essential for implementing efficient music information retrieval systems; meanwhile, it may shed light on the process of human's music perception. This paper describes our work on the classification of folk music from different countries based on their monophonic melodies using hidden Markov models. Music corpora of Irish, German and Austrian folk music in various symbolic formats were used as the data set. Different representations and HMM structures were tested and compared. The classification performances achieved 75{\%}, 77{\%} and 66{\%} for 2-way classifications and 63{\%} for 3-way classification using 6-state left-right HMM with the interval representation in the experiment. This shows that the melodies of folk music do carry some statistical features to distinguish them. We expect that the result will improve if we use a more discriminable data set and the approach should be applicable to other music classification tasks and acoustic musical signals. Furthermore, the results suggest to us a new way to think about musical style similarity.},
author = {Chai, W and Vercoe, B},
doi = {10.1.1.68.206},
file = {:Users/carthach/Documents/Mendeley Desktop/Chai, Vercoe - 2001 - Folk music classification using hidden Markov models.pdf:pdf},
journal = {Proceedings of International Conference on Artificial Intelligence},
keywords = {hidden markov model,music classification,music perception},
number = {6.4},
title = {{Folk music classification using hidden Markov models}},
volume = {6},
year = {2001}
}
@article{Toussaint2010,
author = {Toussaint, Gt},
file = {:Users/carthach/Documents/Mendeley Desktop/Toussaint - 2010 - Generating “good” musical rhythms algorithmically.pdf:pdf},
journal = {Proceedings of the 8th International Conference on Arts and Humanities},
pages = {1--18},
title = {{Generating “good” musical rhythms algorithmically}},
url = {http://cgm.cs.mcgill.ca/{~}godfried/teaching/mir-reading-assignments/Rhythm-Generation.pdf},
year = {2010}
}
@book{Hiller1979,
author = {Hiller, Lejaren Arthur and Isaacson, Leonard M},
publisher = {Greenwood Publishing Group Inc.},
title = {{Experimental Music; Composition with an electronic computer}},
year = {1979}
}
@inproceedings{Wasca,
author = {{Waschka II}, Rodney},
booktitle = {International Computer Music Conference Proceedings},
file = {:Users/carthach/Documents/Mendeley Desktop/Waschka II - 1999 - Avoiding the Fitness Bottleneck Using Genetic Algorithms to Compose Orchestral Music.pdf:pdf},
title = {{Avoiding the Fitness "Bottleneck": Using Genetic Algorithms to Compose Orchestral Music}},
year = {1999}
}
@book{Holmes2008,
abstract = {Electronic Experimental Music is a second edition of a well-known text on the history of electronic music. Holmes' original book, first published in 1985, was a good beginner's introduction both to the theories of electronic sound and sound production and to the history of some of the earliest experiments in instrument building and composition.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Holmes, Thom},
booktitle = {Routledge},
doi = {10.2307/3680269},
eprint = {arXiv:1011.1669v3},
isbn = {0203929594},
issn = {01489267},
pages = {481},
pmid = {25246403},
title = {{Electronic and Experimental Music}},
url = {http://www.jstor.org/stable/3680269?origin=crossref},
year = {2008}
}
@article{McLeod2001,
abstract = {[none]},
author = {McLeod, Kembrew},
doi = {10.1111/j.1533-1598.2001.tb00013.x},
file = {:Users/carthach/Documents/Mendeley Desktop/McLeod - 2001 - Genres, Subgenres, Sub-Subgenres and More Musical and Social Differentiation Within ElectronicDance Music Communities.pdf:pdf},
issn = {1524-2226},
journal = {Journal of Popular Music Studies},
pages = {59--75},
title = {{Genres, Subgenres, Sub-Subgenres and More: Musical and Social Differentiation Within Electronic/Dance Music Communities}},
year = {2001}
}
@article{Monteiro2011,
author = {Monteiro, Adriano and Manzolli, J{\^{o}}natas},
file = {:Users/carthach/Documents/Mendeley Desktop/A{\_}Framework{\_}for{\_}Real-time{\_}Instrumental{\_}Sound{\_}Segme.pdf:pdf},
journal = {Proceedings of IV International Conference of Pure data--Weimar},
keywords = {assisted-improvisation,automatic transcription,musical information,retrieval},
number = {June 2014},
title = {{A Framework for Real-time Instrumental Sound Segmentation and Labeling}},
year = {2011}
}
@article{Jehan2005,
abstract = {Machines have the power and potential to make expressive music on their own. This thesis aims to computationally model the process of creating music using experience from listening to examples. Our unbiased signal-based solution mod- els the life cycle of listening, composing, and performing, turning the machine into an active musician, instead of simply an instrument. We accomplish this through an analysis-synthesis technique by combined perceptual and structural modeling of the musical surface, which leads to a minimal data representation. We introduce a music cognition framework that results from the interaction of psychoacoustically grounded causal listening, a time-lag embedded feature representation, and perceptual similarity clustering. Our bottom-up analysis in- tends to be generic and uniform by recursively revealing metrical hierarchies and structures of pitch, rhythm, and timbre. Training is suggested for top-down un- biased supervision, and is demonstrated with the prediction of downbeat. This musical intelligence enables a range of original manipulations including song alignment, music restoration, cross-synthesis or song morphing, and ultimately the synthesis of original pieces.},
author = {Jehan, T},
file = {:Users/carthach/Documents/Mendeley Desktop/Jehan - 2005 - Creating music by listening.pdf:pdf},
issn = {15410064},
journal = {Media Arts and Sciences},
title = {{Creating music by listening}},
volume = {PhD},
year = {2005}
}
@article{Cont2007a,
abstract = {In this article, a method is proposed for fast and$\backslash$nautomatic retrieval of factors of audio content in a large$\backslash$naudio database based on user's audio query. The proposed$\backslash$nmethod, unlike most existing systems, takes explicit$\backslash$nconsiderations of temporal morphology of audio content.$\backslash$nThis work touches upon several existing approaches and$\backslash$ntechnologies for sound manipulations, such as sound texture$\backslash$nsynthesis, music and audio mosaicing on the synthesis side,$\backslash$nand audio matching, query by audio and audio structure$\backslash$ndiscovery on the analysis side. Destined for creative$\backslash$napplications, the proposed method is modular by allowing$\backslash$ninteractive choice of search criteria. The analysis side of$\backslash$nthe proposed model features a new audio structure discovery$\backslash$nalgorithm called Audio Oracle that describes the temporal$\backslash$nmorphologies of the underlying sound as a compact$\backslash$nstate-space model. The search engine, and the main focus of$\backslash$nthis paper, features a fast and novel algorithm based on$\backslash$ndynamic programming called Guidage that is capable of$\backslash$nreassembling the query audio by concatenating subclips of$\backslash$ntarget audio files. Demonstrated results suggest a degree$\backslash$nof semantic-driven control for query guided applications.$\backslash$nThe article concludes with examples of two immediate$\backslash$napplications of audio matching using Guidage on music,$\backslash$nspeech and natural sounds and a discussion on further$\backslash$ndevelopment and use of such methods in interactive and$\backslash$ncreative environments.},
author = {Cont, Arshia and Dubnov, Shlomo and Assayag, G{\'{e}}rard},
file = {:Users/carthach/Documents/Mendeley Desktop/Cont, Dubnov, Assayag - 2007 - GUIDAGE A Fast Audio Query Guided Assemblage.pdf:pdf},
journal = {Proceedings of the International Computer Music Conference},
pages = {252--259},
title = {{GUIDAGE: A Fast Audio Query Guided Assemblage}},
url = {http://cosmal.ucsd.edu/arshia/papers/ArshiaCont{\_}Guidage{\_}ICMC07.pdf},
year = {2007}
}
@article{Agres2016,
author = {Agres, Kathleen and Forth, Jamie and Wiggins, Geraint A.},
doi = {10.1145/2967506},
file = {:Users/carthach/Documents/Mendeley Desktop/Agres, Forth, Wiggins - 2016 - Evaluation of musical creativity and musical metacreation systems.pdf:pdf},
isbn = {8135841450},
issn = {15443981},
journal = {Computers in Entertainment},
keywords = {22901,375 greenbrier drive,charlottesville,scholarone,va},
number = {3},
pages = {1--33},
title = {{Evaluation of musical creativity and musical metacreation systems}},
volume = {14},
year = {2016}
}
@article{Hiraga2004,
author = {Hiraga, Rumi and Bresin, Roberto and Hirata, Keiji and Katayose, Haruhiro},
file = {:Users/carthach/Documents/Mendeley Desktop/Hiraga et al. - 2004 - Rencon 2004 Turing Test for Musical Expression.pdf:pdf},
journal = {Proceedings of the International Conference on New Interfaces for Musical Expression},
keywords = {musical expression,performance ren-,rencon,turing test},
pages = {120--123},
title = {{Rencon 2004: Turing Test for Musical Expression}},
url = {http://www.nime.org/proceedings/2004/nime2004{\_}120.pdf},
year = {2004}
}
@inproceedings{Gomez-Marin2017,
address = {Matosinhos, Portugal},
author = {G{\'{o}}mez-Mar{\'{i}}n, Daniel and Jord{\`{a}}, Sergi and Herrera, Perfecto},
booktitle = {13th International Symposium on Computer Music Multidisciplinary Research},
file = {:Users/carthach/Documents/Mendeley Desktop/7{\_}CMMR{\_}2017{\_}paper{\_}83.pdf:pdf},
keywords = {conceptual maps,drum pat-,edm,electronic dance music,music cognition,rhythm representations,rhythm space,terns},
title = {{Drum rhythm spaces : from global models to style-specifc maps}},
year = {2017}
}
@book{Mitchell1998,
author = {Mitchell, Melanie},
publisher = {MIT press},
title = {{An introduction to genetic algorithms}},
year = {1998}
}
@article{Guastavino2008,
author = {Guastavino, Catherine and Toussaint, G and G{\'{o}}mez, F. and Marandola, F. and Absar, R.},
file = {:Users/carthach/Documents/Mendeley Desktop/Guastavino et al. - 2008 - Rhythmic similarity in Flamenco music Comparing psychological and mathematical measures.pdf:pdf},
journal = {Proceedings of the fourth Conference on Interdisciplinary Musicology},
pages = {76},
title = {{Rhythmic similarity in Flamenco music: Comparing psychological and mathematical measures}},
url = {http://mil.mcgill.ca/docs/GuastavinoToussaintCIM2008.pdf http://cim08.web.auth.gr/cim08{\_}abstracts/CIM08 Abstracts Proceedings.pdf{\#}page=76},
year = {2008}
}
@article{Zhao2013,
abstract = {Automatic speaker recognition can achieve a high level of performance in matched training and testing conditions. However, such performance drops significantly in mis- matched noisy conditions. Recent research indicates that a new speaker feature, gammatone frequency cepstral coeffi- cients (GFCC), exhibits superior noise robustness to com- monly used mel-frequency cepstral coefficients (MFCC). To gain a deep understanding of the intrinsic robustness of GFCC relative to MFCC, we design speaker identification experiments to systematically analyze their differences and similarities. This study reveals that the nonlinear rectifica- tion accounts for the noise robustness differences primarily. Moreover, this study suggests how to enhance MFCC ro- bustness, and further improve GFCC robustness by adopting a different time-frequency representation.},
author = {Zhao, Xiaojia and Wang, Deliang},
doi = {10.1109/ICASSP.2013.6639061},
file = {:Users/carthach/Documents/Mendeley Desktop/4d435cdbda6f41a436ad35205d8a2062603c.pdf:pdf},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {GFCC,MFCC,noise robustness,speaker features,speaker identification},
pages = {7204--7208},
title = {{Analyzing noise robustness of MFCC and GFCC features in speaker identification}},
year = {2013}
}
@article{Wilson2008,
abstract = {This paper presents an implementation for dynamic two or three dimensional spatial distribution of granulated sound (or granular synthesis) over an arbitrary loudspeaker system.},
author = {Wilson, Scott},
file = {:Users/carthach/Documents/Mendeley Desktop/cr1690.pdf:pdf},
journal = {Proceedings of the 2008 International Computer Music Conference},
pages = {4--7},
title = {{Spatial Swarm Granulation}},
year = {2008}
}
@book{Kim2006,
abstract = {"Essential reading for practising electronics and communications engineers designing and implementing MPEG-7 compliant systems, this book will also be a useful reference for researchers and graduate students working with multimedia database technology."-BOOK JACKET.},
author = {Kim, Hyoung Gook and Moreau, Nicolas and Sikora, Thomas},
booktitle = {Communication},
doi = {10.1002/0470093366},
file = {:Users/carthach/Documents/Mendeley Desktop/[Hyoung-Gook{\_}{\_}Kim{\_}Nicolas{\_}{\_}Moreau{\_}Thomas{\_}{\_}Sikora]{\_}(BookSee.org).pdf:pdf},
isbn = {047009334X},
pmid = {13941350},
publisher = {John Wiley {\&} Sons},
title = {{MPEG-7 audio and beyond: Audio content indexing and retrieval}},
year = {2006}
}
@book{Wright2009,
author = {Wright, David},
doi = {10.1007/978-3-642-24497-1_8},
file = {:Users/carthach/Documents/Mendeley Desktop/Mathematics and Music.pdf:pdf},
isbn = {9783642244964},
issn = {21905622},
publisher = {American Mathematical Society},
title = {{Mathematics and Music}},
year = {2009}
}
@article{Truax1998,
author = {Truax, Barry},
journal = {Computer Music Journal},
number = {2},
pages = {14--26},
publisher = {JSTOR},
title = {{Real-time granular synthesis with a digital signal processor}},
volume = {12},
year = {1988}
}
@article{Kenmochi2007,
author = {Kenmochi, Hideki and Ohshita, Hayato},
file = {:Users/carthach/Documents/Mendeley Desktop/Kenmochi, Ohshita - 2007 - VOCALOID-commercial singing synthesizer based on sample concatenation.pdf:pdf},
journal = {Interspeech},
number = {August},
pages = {3--4},
title = {{VOCALOID-commercial singing synthesizer based on sample concatenation.}},
url = {http://www.mirlab.org/conference{\_}papers/International{\_}Conference/Eurospeech 2007/NOREVIEW/PDF/AUTHOR/NORV1312.PDF},
year = {2007}
}
@book{Morgan2015,
author = {Morgan, Nigel and Legard, PHil},
publisher = {Tonality Systems Press},
title = {{Parametric composition : computer-assisted strategies for human performance}},
year = {2015}
}
@article{Silver2016,
abstract = {Recent work in the sociology of music suggests a declining importance of genre categories. Yet other work in this research stream and in the sociology of classification argues for the continued prevalence of genres as a meaningful tool through which creators, critics and consumers focus their attention in the topology of available works. Building from work in the study of categories and categorization we examine how boundary strength and internal differentiation structure the genre pairings of some 3 million musicians and groups. Using a range of network-based and statistical techniques, we uncover three musical "complexes," which are collectively constituted by 16 smaller genre communities. Our analysis shows that the musical universe is not monolithically organized but rather composed of multiple worlds that are differently structured-i.e., uncentered, single-centered, and multi-centered. {\textcopyright} 2016 Silver et al.This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
author = {Silver, Daniel and Lee, Monica and Childress, C. Clayton},
doi = {10.1371/journal.pone.0155471},
file = {:Users/carthach/Documents/Mendeley Desktop/Silver, Lee, Childress - 2016 - Genre Complexes in Popular Music.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
number = {5},
pages = {1--23},
title = {{Genre Complexes in Popular Music}},
volume = {11},
year = {2016}
}
@article{assayag2006computer,
author = {Assayag, G{\'{e}}rard and Rueda, Camilo and Laurson, Mikael and Agon, Carlos and Delerue, Olivier},
journal = {Computer},
number = {3},
publisher = {MIT Press},
title = {{Computer-assisted composition at IRCAM: From PatchWork to OpenMusic}},
volume = {23},
year = {2006}
}
@article{GouyonFabien;Dixon2005,
author = {{Gouyon, Fabien; Dixon}, Simon},
file = {:Users/carthach/Documents/Mendeley Desktop/Gouyon, Fabien Dixon - 2005 - A review of automatic rhythm description systems.pdf:pdf},
journal = {Computer music journal},
title = {{A review of automatic rhythm description systems}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/comj.2005.29.1.34},
year = {2005}
}
@misc{Vog,
author = {Vogel, Christian},
title = {{Rhythmic Computation Lab}},
url = {http://www.cristianvogel.com/neverenginelabs/product/rhythmiccomputationlab},
urldate = {2015-09-28},
year = {2015}
}
@article{Boone2012,
abstract = {Abstract: This article provides information for Extension professionals on the correct analysis of Likert data. The analyses of Likert-type and Likert scale data require unique data analysis procedures, and as a result, misuses and/or mistakes often occur. This article discusses the differences between Likert-type and Likert scale data and provides recommendations for descriptive statistics to be used during the analysis. Once a researcher understands the difference between Likert-type and Likert scale data, the decision on appropriate statistical procedures will be apparent. Introduction},
author = {Boone, Harry N and Boone, Deborah A.},
file = {:Users/carthach/Documents/Mendeley Desktop/Likert Scale Analysis.pdf:pdf},
journal = {Journal of Extension},
number = {2},
title = {{Analyzing Likert Data Likert-Type Versus Likert Scales}},
url = {http://www.joe.org/joe/2012april/tt2p.shtml[8/20/2012 9:07:48 AM]},
volume = {50},
year = {2012}
}
@book{Rogers2011,
author = {Rogers, Yvonne and Sharp, Helen and Preece, Jenny},
publisher = {John Wiley {\&} Sons},
title = {{Interaction design: beyond human-computer interaction}},
year = {2011}
}
@article{Parncutt1994b,
author = {Parncutt, Richard},
file = {:Users/carthach/Documents/Mendeley Desktop/Parncutt - 1994 - A perceptual model of pulse salience and metrical accent in musical rhythms.pdf:pdf},
journal = {Music Perception},
title = {{A perceptual model of pulse salience and metrical accent in musical rhythms}},
url = {http://www.jstor.org/stable/40285633},
year = {1994}
}
@article{Piszczalski:1979,
author = {Piszczalski, Martin and Galler, Bernard A},
journal = {The Journal of the Acoustical Society of America},
keywords = {Melody extraction,Source Separation,pattern matching,pitch,spectral pitch},
mendeley-tags = {Melody extraction,Source Separation,pattern matching,pitch,spectral pitch},
pages = {710},
title = {{Predicting musical pitch from component frequency ratios}},
volume = {66},
year = {1979}
}
@article{Puckette:1998,
author = {Puckette, Miller S and Brown, Judith C},
journal = {Speech and Audio Processing, IEEE Transactions on},
keywords = {Melody extraction,Source Separation},
mendeley-tags = {Melody extraction,Source Separation},
number = {2},
pages = {166--176},
publisher = {IEEE},
title = {{Accuracy of frequency estimates using the phase vocoder}},
volume = {6},
year = {1998}
}
@article{Hove2014,
author = {Hove, Michael and Marie, Celine and Bruce, Ian and Trainor, Laurel},
file = {::},
journal = {Proc Natl Acad Sci U S A},
pages = {10383 -- 8},
title = {{Superior time perception for lower musical pitch explains why bass-ranged instruments lay down musical rhythms.}},
volume = {111},
year = {2014}
}
@article{Bock2013,
author = {B{\"{o}}ck, S and Widmer, G},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck, Widmer - 2013 - Maximum filter vibrato suppression for onset detection.pdf:pdf},
journal = {Proc. of the 16th Int. Conf. on Digital Audio {\ldots}},
pages = {1--7},
title = {{Maximum filter vibrato suppression for onset detection}},
url = {http://dafx13.nuim.ie/papers/09.dafx2013{\_}submission{\_}12.pdf},
year = {2013}
}
@inproceedings{heittola2009musical,
author = {Heittola, Toni and Klapuri, Anssi and Virtanen, Tuomas},
booktitle = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {327--332},
title = {{Musical Instrument Recognition in Polyphonic Audio Using Source-Filter Model for Sound Separation.}},
url = {http://www.cs.tut.fi/sgn/arg/klap/ismir09-heittola.pdf},
year = {2009}
}
@article{Arora:2012,
author = {Arora, Vipul and Behera, Laxmidhar},
journal = {Extended abstract submission to the Music Information Retrieval Evaluation eXchange (MIREX)},
keywords = {Melody extraction,Re,Source Separation},
mendeley-tags = {Melody extraction,Re,Source Separation},
publisher = {MIREX},
title = {{Online melody extraction: Mirex 2012}},
year = {2012}
}
@article{Tindale2004,
author = {Tindale, Adam and Kapur, Ajay and Fujinaga, Ichiro},
issn = {2223-3881},
journal = {International Computer Music Conference Proceedings},
month = {jan},
publisher = {Michigan Publishing, University of Michigan Library},
title = {{Towards Timbre Recognition of Percussive Sounds}},
url = {http://quod.lib.umich.edu/i/icmc/bbp2372.2004.157?rgn=main;view=fulltext},
volume = {2004},
year = {2004}
}
@article{gillet2008transcription,
author = {Gillet, Olivier and Richard, Ga{\"{e}}l},
file = {::},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {3},
pages = {529--540},
publisher = {IEEE},
title = {{Transcription and separation of drum signals from polyphonic music}},
volume = {16},
year = {2008}
}
@incollection{rodriguez2012multiple,
author = {Rodriguez-Serrano, Francisco J and Carabias-Orti, Julio J and Vera-Candeas, Pedro and Virtanen, Tuomas and Ruiz-Reyes, Nicolas},
booktitle = {Latent Variable Analysis and Signal Separation},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {380--387},
publisher = {Springer},
title = {{Multiple instrument mixtures source separation evaluation using instrument-dependent NMF models}},
year = {2012}
}
@article{Levy2008c,
author = {Levy, Mark and Sandler, Mark},
file = {:Users/carthach/Documents/Mendeley Desktop/Levy, Sandler - 2008 - Structural segmentation of musical audio by constrained clustering.pdf:pdf},
journal = {Audio, Speech, and Language Processing, {\ldots}},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {2},
pages = {318--326},
title = {{Structural segmentation of musical audio by constrained clustering}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4432648},
volume = {16},
year = {2008}
}
@book{bregman1994auditory,
author = {Bregman, Albert S},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {MIT press},
title = {{Auditory scene analysis: The perceptual organization of sound}},
year = {1994}
}
@article{Klapuri1999,
abstract = {A system was designed, which is able to detect the perceptual
onsets of sounds in acoustic signals. The system is general in regard to
the sounds involved and was found to be robust for different kinds of
signals. This was achieved without assuming regularities in the
positions of the onsets. In this paper, a method is first proposed that
can determine the beginnings of sounds that exhibit onset imperfections,
i.e., the amplitude envelope of which does not rise monothinically. Then
the mentioned system is described, which utilizes band-wise processing
and a psychoacoustic model of intensity coding to combine the results
from the separate frequency bands. The performance of the system was
validated by applying it to the detection of onsets in musical signals
ranging from rock to classical and big band recordings},
author = {Klapuri, A.},
doi = {10.1109/ICASSP.1999.757494},
isbn = {0-7803-5041-3},
issn = {1520-6149},
journal = {1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)},
title = {{Sound onset detection by applying psychoacoustic knowledge}},
volume = {6},
year = {1999}
}
@book{Cope2004,
author = {Cope, David},
publisher = {MIT press},
title = {{Virtual music: computer synthesis of musical style}},
year = {2004}
}
@misc{Wavedna2015,
author = {Wavedna},
booktitle = {Liquid Music for Live},
title = {{Liquid Music for Live}},
url = {https://www.wavedna.com/liquid-music/ableton-live-plugin-max-for-live/},
urldate = {2012-05-20},
year = {2015}
}
@book{McCormack2012,
address = {Berlin, Heidelberg},
editor = {McCormack, Jon and D'Inverno, Mark},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {Springer Berlin Heidelberg},
title = {{Computers and Creativity}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-31727-9},
year = {2012}
}
@inproceedings{benetos2011multiple,
author = {Benetos, Emmanouil and Dixon, Simon},
booktitle = {8th Sound and Music Computing Conference},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {19--24},
title = {{Multiple-instrument polyphonic music transcription using a convolutive probabilistic model}},
url = {http://www.eecs.qmul.ac.uk/{~}simond/pub/2011/BenetosDixonSMC2011.pdf},
year = {2011}
}
@article{Ehmann2011a,
author = {Ehmann, AF F and Bay, Mert and Downie, JS S and Fujinaga, Ichiro and Roure, David De},
file = {:Users/carthach/Documents/Mendeley Desktop/Ehmann et al. - 2011 - MUSIC STRUCTURE SEGMENTATION ALGORITHM EVALUATION EXPANDING ON MIREX 2010 ANALYSES AND DATASETS.pdf:pdf},
journal = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {Ismir},
pages = {561--566},
title = {{MUSIC STRUCTURE SEGMENTATION ALGORITHM EVALUATION : EXPANDING ON MIREX 2010 ANALYSES AND DATASETS}},
url = {http://ismir2011.ismir.net/papers/PS4-15.pdf},
year = {2011}
}
@article{Chai2006,
author = {Chai, Wei},
file = {:Users/carthach/Documents/Mendeley Desktop/Chai - 2006 - Semantic Segmentation and Summarization of Music - Methods based on tonality and recurrent structure.pdf:pdf},
journal = {Signal Processing Magazine, IEEE},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {March},
pages = {124--132},
title = {{Semantic Segmentation and Summarization of Music - Methods based on tonality and recurrent structure}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1598088},
year = {2006}
}
@inproceedings{Klapuri:2000,
author = {Klapuri, Anssi and Virtanen, Tuomas and Holm, Jan-Markus},
booktitle = {Proc. COST-G6 Conference on Digital Audio Effects},
keywords = {Melody extraction,Source Separation,pitch,pitch polyphonic,pitch spectral},
mendeley-tags = {Melody extraction,Source Separation,pitch,pitch polyphonic,pitch spectral},
pages = {233--236},
title = {{Robust multipitch estimation for the analysis and manipulation of polyphonic musical signals}},
year = {2000}
}
@incollection{Sturm2014,
abstract = {Much work is focused upon music genre recognition (MGR) from audio recordings, symbolic data, and other modalities. While reviews have been written of some of this work before, no survey has been made of the approaches to evaluating approaches to MGR. This paper compiles a bibliography of work in MGR, and analyzes three aspects of evaluation: experimental designs, datasets, and figures of merit.},
author = {Sturm, Bob L.},
booktitle = {Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation},
doi = {10.1007/978-3-319-12093-5},
isbn = {978-3-319-12092-8},
pages = {29--66},
title = {{A survey of evaluation in music genre reconigtion}},
url = {http://link.springer.com/10.1007/978-3-319-12093-5},
volume = {8382},
year = {2014}
}
@article{Pope2004a,
author = {Pope, Stephen Travis and Holm, Frode and Kouznetsov, Alexandre},
file = {:Users/carthach/Documents/Mendeley Desktop/Pope, Holm, Kouznetsov - 2004 - Feature Extraction and Database Design for Music Software Dimensions of MDB Applications.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {1--8},
title = {{Feature Extraction and Database Design for Music Software * Dimensions of MDB Applications}},
year = {2004}
}
@book{Nierhaus2009,
abstract = {Algorithmic composition – composing by means of formalizable methods – has a century old tradition not only in occidental music history. This is the first book to provide a detailed overview of prominent procedures of algorithmic composition in a pragmatic way rather than by treating formalizable aspects in single works. In addition to an historic overview, each chapter presents a specific class of algorithm in a compositional context by providing a general introduction to its development and theoretical basis and describes different musical applications. Each chapter outlines the strengths, weaknesses and possible aesthetical implications resulting from the application of the treated approaches. Topics covered are: markov models, generative grammars, transition networks, chaos and self-similarity, genetic algorithms, cellular automata, neural networks and artificial intelligence are covered. The comprehensive bibliography makes this work ideal for the musician and the researcher alike.},
author = {Nierhaus, Gerhard},
booktitle = {Algorithmic Composition: Paradigms of Automated Music Generation},
doi = {10.1007/978-3-211-75540-2},
isbn = {9783211755396},
issn = {01489267},
pages = {1--287},
title = {{Algorithmic composition: Paradigms of automated music generation}},
year = {2009}
}
@misc{Lee2006,
abstract = {In this paper, we propose a novel method for obtaining la- beled training data to estimate the parameters in a super- vised learning model for automatic chord recognition. To this end, we perform harmonic analysis on symbolic data to generate label files. In parallel, we generate audio data from the same symbolic data, which are then provided to a machine learning algorithm along with label files to esti- mate model parameters. Experimental results show higher performance in frame-level chord recognition than the pre- vious approaches.},
author = {Lee, Kyogu and Slaney, Malcolm},
booktitle = {International Society for Music Information Retrieval},
doi = {10.1145/1178723.1178726},
file = {:Users/carthach/Documents/Mendeley Desktop/Lee, Slaney - 2006 - Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.pdf:pdf},
isbn = {1595935010},
issn = {10413200},
keywords = {chord recognition,hidden markov model},
pages = {2--6},
title = {{Automatic Chord Recognition from Audio Using a HMM with Supervised Learning.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.9311{\&}rep=rep1{\&}type=pdf{\%}5Cnhttps://ccrma.stanford.edu/{~}kglee/pubs/klee-ismir06.pdf},
year = {2006}
}
@book{Davismoon2010,
address = {Berlin, Heidelberg},
author = {Davismoon, Stephen and Eccles, John},
booktitle = {European Conference on the Applications of Evolutionary Computation},
editor = {{Di Chio}, Cecilia and Brabazon, Anthony and {Di Caro}, Gianni A. and Ebner, Marc and Farooq, Muddassar and Fink, Andreas and Grahl, J{\"{o}}rn and Greenfield, Gary and Machado, Penousal and O'Neill, Michael and Tarantino, Ernesto and Urquhart, Neil},
language = {en},
month = {apr},
pages = {361--370},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Applications of Evolutionary Computation}},
url = {http://link.springer.com.sare.upf.edu/chapter/10.1007/978-3-642-12242-2{\_}37},
volume = {6025},
year = {2010}
}
@book{Witten2005,
author = {Witten, IH H and Frank, E},
booktitle = {Vasa},
file = {::},
isbn = {0120884070},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Data Mining: Practical machine learning tools and techniques}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://books.google.com/books?hl=en{\&}lr={\&}id=QTnOcZJzlUoC{\&}oi=fnd{\&}pg=PR17{\&}dq=Data+Mining:+practical+machine+learning+tools+and+techniques{\&}ots=3goD8nWhOd{\&}sig=5Tbsrx5szVr84PMIt{\_}q2Xc0UBV4},
year = {2005}
}
@article{Paulusa,
author = {Paulus, Jouni},
file = {:Users/carthach/Documents/Mendeley Desktop/Paulus - Unknown - AUDIO-BASED MUSIC STRUCTURE ANALYSIS.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{AUDIO-BASED MUSIC STRUCTURE ANALYSIS}}
}
@phdthesis{Salamon:2013,
abstract = {Music was the first mass-market industry to be completely restructured by digital technology, and today we can have access to thousands of tracks stored locally on our smartphone and millions of tracks through cloud-based music services. Given the vast quantity of music at our fingertips, we now require novel ways of describing, indexing, searching and interacting with musical content. In this thesis we focus on a technology that opens the door to a wide range of such applications: automatically estimating the pitch sequence of the melody directly from the audio signal of a polyphonic music recording, also referred to as melody extraction. Whilst identifying the pitch of the melody is something human listeners can do quite well, doing this automatically is highly challenging. We present a novel method for melody extraction based on the tracking and characterisation of the pitch contours that form the melodic line of a piece. We show how different contour characteristics can be exploited in combination with auditory streaming cues to identify the melody out of all the pitch content in a music recording using both heuristic and model-based approaches. The performance of our method is assessed in an international evaluation campaign where it is shown to obtain state-of-the-art results. In fact, it achieves the highest mean overall accuracy obtained by any algorithm that has participated in the campaign to date. We demonstrate the applicability of our method both for research and end-user applications by developing systems that exploit the extracted melody pitch sequence for similarity-based music retrieval (version identification and query-by-humming), genre classification, automatic transcription and computational music analysis. The thesis also provides a comprehensive comparative analysis and review of the current state-of-the-art in melody extraction and a first of its kind analysis of melody extraction evaluation methodology.},
address = {Barcelona},
author = {Salamon, J},
file = {::},
keywords = {Genre classification,Source Separation,auditory scene analysis,melody extraction,pitch,pitch polyphonic},
mendeley-tags = {Genre classification,Source Separation,auditory scene analysis,melody extraction,pitch,pitch polyphonic},
school = {Universitat Pompeu Fabra},
title = {{Melody Extraction from Polyphonic Music Signals}},
url = {files/publications/jsalamon{\_}phdthesis.pdf},
year = {2013}
}
@inproceedings{Duxbury2002,
abstract = {Common problems with current methods of musical note onset detection are detection of fast passages of musical audio, detection of all onsets within a passage with a strong dynamic range and detection of onsets of varying types, such as multi-instrumental music. We present a method that uses a subband decomposition approach to onset detection. An energy-based detector is used on the upper subbands to detect strong transient events. This yields precision in the time resolution of the onsets, but does not detect softer or weaker onsets. A frequency based distance measure is formulated for use with the lower subbands, improving detection accuracy of softer onsets. We also present a method for improving the detection function, by using a smoothed difference metric. Finally, we show that the detection threshold may be set automatically from analysis of the statistics of the detection function, with results comparable in most places to manual setting of thresholds.},
author = {Duxbury, Chris and Sandler, Mark and Davies, Mike},
booktitle = {Computer},
pages = {33--38},
title = {{A hybrid approach to musical note onset detection}},
url = {http://www.unibw-hamburg.de/EWEB/ANT/dafx2002/papers/DAFX02{\_}Duxbury{\_}Sandler{\_}Davis{\_}note{\_}onset{\_}detection.pdf},
year = {2002}
}
@article{Peeters2002,
author = {Peeters, Geoffroy and Burthe, A La and Rodet, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Peeters, Burthe, Rodet - 2002 - Toward Automatic Music Audio Summary Generation from Signal Analysis.pdf:pdf},
journal = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Toward Automatic Music Audio Summary Generation from Signal Analysis.}},
url = {http://users.cis.fiu.edu/{~}lli003/Music/ms/5.pdf},
year = {2002}
}
@inproceedings{robel2009onset,
author = {R{\"{o}}bel, A},
booktitle = {Proceedings of MIREX as part of the 10th International Conference on Music Information Retrieval (ISMIR)},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {2},
title = {{Onset detection by means of transient peak classification in harmonic bands}},
year = {2009}
}
@inproceedings{Biles:1994,
abstract = {This paper describes GenJam, a genetic algorithm-based model of a novice jazz musician learning to improvise. GenJam maintains hierarchically related populations of melodic ideas that are mapped to specific notes through scales suggested by the chord progression being played. As GenJam plays its solos over the accompaniment of a standard rhythm section, a human mentor gives real-time feedback, which is used to derive fitness values for the individual measures and phrases. GenJam then applies various genetic operators to the populations to breed improved generations of ideas},
author = {Biles, John A},
booktitle = {International Computer Music Conference},
file = {:Users/carthach/Documents/Mendeley Desktop/Biles - 1994 - GenJam A Genetic Algorithm for Generating Jazz Solos.pdf:pdf},
issn = {1026-1087},
keywords = {BSc Thesis,Folder - IAT 811: Metacreation,Source Separation,artificial intelligence,automated composition,emotions,generative music,genetic algorithm,genetic algorithms,jazz,music,research},
mendeley-tags = {BSc Thesis,Folder - IAT 811: Metacreation,Source Separation,artificial intelligence,automated composition,emotions,generative music,genetic algorithm,genetic algorithms,jazz,music,research},
pages = {131--137},
title = {{GenJam : A Genetic Algorithm for Generating Jazz Solos}},
year = {1994}
}
@inproceedings{zils2002automatic,
author = {Zils, Aymeric and Pachet, Fran{\c{c}}ois and Delerue, Olivier and Gouyon, Fabien},
booktitle = {Web Delivering of Music, 2002. WEDELMUSIC 2002. Proceedings. Second International Conference on},
file = {:Users/carthach/Documents/Mendeley Desktop/01176209.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {179--183},
title = {{Automatic extraction of drum tracks from polyphonic music signals}},
year = {2002}
}
@article{Bello2011a,
author = {Bello, Juan P.},
doi = {10.1109/TASL.2011.2108287},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello - 2011 - Measuring Structural Similarity in Music.pdf:pdf},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
month = {sep},
number = {7},
pages = {2013--2025},
title = {{Measuring Structural Similarity in Music}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5711645},
volume = {19},
year = {2011}
}
@article{PierreLeveau,
author = {{Pierre Leveau}, Laurent Daudet},
file = {::},
keywords = {Onset detection},
mendeley-tags = {Onset detection},
title = {{Methodology and Tools for the evaluation of automatic onset detection algorithms in music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.7442}
}
@article{Song:2013,
author = {Song, Liming and Li, Ming},
journal = {Extended abstract submission to the Music Information Retrieval Evaluation eXchange (MIREX), 2013.},
keywords = {Melody extraction,Source Separation},
mendeley-tags = {Melody extraction,Source Separation},
title = {{Bayesian framework-based vocal melody extraction for MIREX 2013}},
year = {2013}
}
@phdthesis{McVicar2013,
author = {McVicar, Matt},
file = {::},
school = {University of Bristol},
title = {{A Machine Learning Approach to Automatic Chord Extraction}},
year = {2013}
}
@article{Zicarelli1987,
author = {Zicarelli, David},
journal = {Computer Music journal},
number = {1},
pages = {13 -- 29},
title = {{M and Jam Factory}},
volume = {dec},
year = {1987}
}
@article{carabias2011musical,
author = {Carabias-Orti, Julio J and Virtanen, Tuomas and Vera-Candeas, Pedro and Ruiz-Reyes, Nicol{\'{a}}s and Ca{\~{n}}adas-Quesada, Francisco J},
journal = {Selected Topics in Signal Processing, IEEE Journal of},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {6},
pages = {1144--1158},
publisher = {IEEE},
title = {{Musical instrument sound multi-excitation model for non-negative spectrogram factorization}},
volume = {5},
year = {2011}
}
@article{McFee2012,
author = {McFee, Brian and Barrington, Luke and Lanckriet, Gert},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Audio retrieval and recommendation,Collaboration,Equations,Histograms,Measurement,Mel frequency cepstral coefficient,Source Separation,Training,Vectors,answer queries,audio content,audio signal processing,collaborative filter data,collaborative filter methods,collaborative filter techniques,collaborative filtering,collaborative filters (CFs),content-based recommendation techniques,content-based retrieval,content-based similarity optimization,historical data,learning content similarity,music,music information retrieval,music recommendation,musical items,online radio,optimized content-based similarity metric,query-by-example,query-by-example setting,structured prediction},
language = {English},
mendeley-tags = {Source Separation},
month = {oct},
number = {8},
pages = {2207--2218},
publisher = {IEEE},
title = {{Learning Content Similarity for Music Recommendation}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6213086},
volume = {20},
year = {2012}
}
@article{Serra1997,
author = {Serra, Xavier},
file = {:Users/carthach/Documents/Mendeley Desktop/Serra - 1997 - Musical sound modeling with sinusoids plus noise.pdf:pdf},
journal = {Musical signal processing},
pages = {1--25},
title = {{Musical sound modeling with sinusoids plus noise}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=RJ9lAgAAQBAJ{\&}oi=fnd{\&}pg=PT14{\&}dq=Musical+Sound+Modeling+with+Sinusoids+plus+Noise{\&}ots=ZeWz1mGX3K{\&}sig=EBJaVNkS9HI1jz9zebRhYhe4DVc},
year = {1997}
}
@inproceedings{smaragdis2011polyphonic,
author = {Smaragdis, Paris},
booktitle = {Applications of Signal Processing to Audio and Acoustics (WASPAA), 2011 IEEE Workshop on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {125--128},
title = {{Polyphonic pitch tracking by example}},
url = {http://www.cs.illinois.edu/{~}paris/pubs/smaragdis-waspaa2011.pdf},
year = {2011}
}
@phdthesis{Ewert:2012,
annote = {From Duplicate 2 ( 


Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation


- Ewert, Sebastian )



chap 8 : source separation},
author = {Ewert, Sebastian},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
school = {Universit{\{}{\"{a}}{\}}ts-und Landesbibliothek Bonn},
title = {{Signal Processing Methods for Music Synchronization, Audio Matching, and Source Separation}},
year = {2012}
}
@article{Goto2003,
author = {Goto, M},
file = {:Users/carthach/Documents/Mendeley Desktop/Goto - 2003 - A CHORUS-SECTION DETECTING METHOD FOR MUSICAL AUDIO SIGNALS.pdf:pdf},
isbn = {0780376633},
journal = {Acoustics, Speech, and Signal Processing, 2003. {\ldots}},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {April},
pages = {437--440},
title = {{A CHORUS-SECTION DETECTING METHOD FOR MUSICAL AUDIO SIGNALS}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1200000},
volume = {2003},
year = {2003}
}
@misc{Brossier2006,
author = {Brossier, PM},
file = {:Users/carthach/Documents/Mendeley Desktop/Brossier - 2006 - Automatic annotation of musical audio for interactive applications.pdf:pdf},
number = {August},
title = {{Automatic annotation of musical audio for interactive applications}},
url = {http://qmro.qmul.ac.uk/jspui/handle/123456789/3809},
year = {2006}
}
@article{hu2004monaural,
author = {Hu, Guoning and Wang, DeLiang},
journal = {Neural Networks, IEEE Transactions on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {5},
pages = {1135--1150},
publisher = {IEEE},
title = {{Monaural speech segregation based on pitch tracking and amplitude modulation}},
volume = {15},
year = {2004}
}
@article{PierreLeveaua,
author = {{Pierre Leveau}, Laurent Daudet},
file = {::},
keywords = {Onset detection},
mendeley-tags = {Onset detection},
title = {{Methodology and Tools for the evaluation of automatic onset detection algorithms in music}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.105.7442}
}
@incollection{Cont2007,
abstract = {The role of expectation in listening and composing music has drawn much attention in music cognition since about half a century ago. In this paper, we provide a first attempt to model some aspects of musical expectation specifically pertained to short-time and working memories, in an anticipatory framework. In our proposition anticipation is the mental realization of possible predicted actions and their effect on the perception of the world at an instant in time. We demonstrate the model in applications to automatic improvisation and style imitation. The proposed model, based on cognitive foundations of musical expectation, is an active model using reinforcement learning techniques with multiple agents that learn competitively and in collaboration. We show that compared to similar models, this anticipatory framework needs little training data and demonstrates complex musical behavior such as long-term planning and formal shapes as a result of the anticipatory architecture. We provide sample results and discuss further research.},
author = {Cont, Arshia and Dubnov, Shlomo and Assayag, G{\'{e}}rard},
booktitle = {Anticipatory Behavior in Adaptive Learning Systems},
doi = {10.1007/978-3-540-74262-3_16},
isbn = {978–3−540–74261–6},
issn = {03029743},
pages = {285--306},
pmid = {38149108966},
title = {{Anticipatory Model of Musical Style Imitation Using Collaborative and Competitive Reinforcement Learning}},
url = {http://dx.doi.org/10.1007/978-3-540-74262-3{\_}16},
year = {2007}
}
@article{d,
author = {Goto, Masataka},
journal = {Speech Communication},
keywords = {Computational auditory scene analysis,D3.1,EM algorithm,F0 estimation,MAP estimation,Melody extraction,Music information retrieval,Music understanding,Re,Source Separation},
mendeley-tags = {D3.1,Melody extraction,Re,Source Separation},
number = {4},
pages = {311--329},
publisher = {Elsevier},
title = {{A real-time music-scene-description system: predominant-f0 estimation for detecting melody and bass lines in real-world audio signals}},
url = {http://ac.els-cdn.com/S0167639304000640/1-s2.0-S0167639304000640-main.pdf?{\_}tid=1e571db8-7eb0-11e3-ae2b-00000aab0f27{\&}acdnat=1389878244{\_}ec2523153d702551f3550b022057f881},
volume = {43},
year = {2004}
}
@inproceedings{Sood:2004,
author = {Sood, Saurabh and Krishnamurthy, Ashok},
booktitle = {Proceedings of the 12th annual ACM international conference on Multimedia},
keywords = {Source Separation,pitch,pitch temporal},
mendeley-tags = {Source Separation,pitch,pitch temporal},
organization = {ACM},
pages = {280--283},
title = {{A robust on-the-fly pitch (OTFP) estimation algorithm}},
year = {2004}
}
@article{Sampaio2008a,
abstract = {CinBalada is a system for automatic creation of polyphonic rhythmic performances by mixing elements from different musical styles. This system is based on agents that act as musicians playing percussion instruments in a drum circle. Each agent has to choose from a database the rhythm pattern of its instrument that satisfies the “rhythmic role” assigned to him in order to produce a collectively- consistent rhythmic performance. A rhythmic role is a concept that we proposed here with the objective of representing culture-specific rules for creation of polyphonic performances.},
author = {Sampaio, Pablo Azevedo and Ramalho, Geber and Tedesco, Patr{\'{i}}cia},
doi = {10.1590/S0104-65002008000300004},
file = {::},
issn = {0104-6500},
journal = {Journal of the Brazilian Computer Society},
keywords = {multiagent,rhythm composition,rhythmic role},
number = {3},
pages = {19},
title = {{CinBalada: a multiagent rhythm factory}},
url = {http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0104-65002008000300004{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {14},
year = {2008}
}
@article{Pearce2000,
abstract = {The objective of this research was to design a creative$\backslash$naid to musical composition: a system that would generate a (user$\backslash$nspecified) number of drum patterns, within a specified style and$\backslash$nshowing a sufficient amount of variation, on any one run. To achieve$\backslash$nthese aims a genetic algorithm was implemented using as its critic a$\backslash$nmulti--layer perceptron, trained on a set of "drum{\&}bass"$\backslash$npatterns. Domain specifiic knowledge was incorporated into a number$\backslash$nof areas of the GA and an island model was used to generate multiple$\backslash$nsolutions. Experiments conducted, using human subjects, questioned$\backslash$nthe aesthetic merit and style of the generated patterns. These$\backslash$npartial failures of the system to acheive the stated aims were$\backslash$nattributed to shortcomings of the data used to train the ANN$\backslash$ncritic. Suggestions for future research have been presented in terms$\backslash$nof improving the critic, extending the approach to more complicated$\backslash$nrhythmic sequences and better evaluation of machine compositions.},
author = {Pearce, M T},
file = {::},
number = {September},
title = {{Generating Rhythmic Patterns: {\{}A{\}} Combined Neural and Evolutionary Approach}},
year = {2000}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}
@article{Conklin2003,
abstract = {This paper discusses the use of statisticalmodels for the problemofmusical style imitation. Statisticalmodels are created from extant pieces in a stylistic corpus, and have an objective goal which is to accurately classify new pieces. The process of music generation is equated with the problem of sampling from a statistical model. In principle there is no need to make the classical distinction between analytic and synthetic models of music. This paper presents several methods for sampling from an analytic statistical model, and proposes a new approach that maintains the intra opus pattern repetition within an extant piece. A major component of creativity is the adaptation of extant art works, and this is also an efficient way to sample pieces from complex statistical models.},
author = {Conklin, Darrell},
file = {::},
journal = {Proceedings of the AISB 2003 Symposium on Artificial Intelligence and Creativity in the Arts and Sciences},
pages = {30--35},
title = {{Music Generation from Statistical Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.2086{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@phdthesis{COHEN:1999,
annote = {http://robotics.stanford.edu/{\~{}}rubner/emd},
author = {scott COHEN},
keywords = {EMD,Source Separation,distance pattern,image similarity,perso},
mendeley-tags = {EMD,Source Separation,distance pattern,image similarity,perso},
title = {finding colors and shape patterns in images},
year = {1999}
}
@article{Glover2011,
abstract = {Real-time musical note onset detection plays a vital role in many audio analysis processes, such as score following, beat detection and various sound synthesis by analysis methods. This article provides a review of some of the most commonly used techniques for real-time onset detection. We suggest ways to improve these techniques by incorporating linear prediction as well as presenting a novel algorithm for real-time onset detection using sinusoidal modelling. We provide comprehensive results for both the detection accuracy and the computational performance of all of the described techniques, evaluated using Modal, our new open source library for musical onset detection, which comes with a free database of samples with hand-labelled note onsets.},
author = {Glover, John and Lazzarini, Victor and Timoney, Joseph},
doi = {10.1186/1687-6180-2011-68},
file = {:Users/carthach/Documents/Mendeley Desktop/Glover, Lazzarini, Timoney - 2011 - Real-time detection of musical onsets with linear prediction and sinusoidal modeling.pdf:pdf},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
language = {en},
month = {sep},
number = {1},
pages = {68},
publisher = {Springer},
title = {{Real-time detection of musical onsets with linear prediction and sinusoidal modeling}},
url = {http://asp.eurasipjournals.com/content/2011/1/68},
volume = {2011},
year = {2011}
}
@article{benetos2013multiple,
author = {Benetos, Emmanouil and Dixon, Simon},
journal = {The Journal of the Acoustical Society of America},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {1727},
title = {{Multiple-instrument polyphonic music transcription using a temporally constrained shift-invariant model}},
volume = {133},
year = {2013}
}
@inproceedings{Woodruff:2006,
author = {Woodruff, John F and Pardo, Bryan and Dannenberg, Roger B},
booktitle = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {314--319},
title = {{Remixing Stereo Music with Score-Informed Source Separation.}},
year = {2006}
}
@article{McVicar2014,
author = {McVicar, M and Santos-Rodr{\'{i}}guez, R and Ni, Yizhao and Bie, Tijl De},
file = {::},
number = {2},
pages = {1--20},
title = {{Automatic Chord Estimation from Audio: A Review of the State of the Art}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6705583},
volume = {22},
year = {2014}
}
@article{Collins2012,
abstract = {This article presents Autocousmatic, an algorithmic system that creates electroacoustic art music using machine-listening processes within the design cycle. After surveying previous projects in automated mixing and algorithmic composition, the design and implementation of the current system is outlined. An iterative, automatic effects processing system is coupled to machine-listening components, including the assessment of the “worthiness” of intermediate files to continue to a final mixing stage. Generation of the formal structure of output pieces utilizes models derived from a small corpus of exemplar electroacoustic music, and a dynamic time-warping similarity-measure technique drawn from music information retrieval is employed to decide between candidate final mixes. Evaluation of Autocousmatic has involved three main components: the entry of its output works into composition competitions, the public release of the software with an associated questionnaire and sound examples on SoundCloud, and direct ...},
author = {Collins, Nick},
file = {::},
issn = {0148-9267},
journal = {Computer Music Journal},
keywords = {Source Separation},
language = {en},
mendeley-tags = {Source Separation},
month = {sep},
number = {3},
pages = {8--23},
publisher = {MIT Press  55 Hayward St., Cambridge, MA 02142-1315 USA journals-info@mit.edu},
title = {{Automatic Composition of Electroacoustic Art Music Utilizing Machine Listening}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/COMJ{\_}a{\_}00135},
volume = {36},
year = {2012}
}
@book{Meyer2008a,
author = {Meyer, Leonard B.},
publisher = {University of chicago Press},
title = {{Emotion and meaning in music}},
year = {2008}
}
@article{Kameoka:2007,
author = {Kameoka, Hirokazu and Nishimoto, Takuya and Sagayama, Shigeki},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {---Computational acoustic scene analysis,Source Separation,harmonic temporal structured clustering (HTC),multipitch analyzer},
mendeley-tags = {Source Separation},
pages = {982--994},
title = {{A Multipitch Analyzer Based on Harmonic Temporal Structured Clustering}},
url = {http://www.brl.ntt.co.jp/people/kameoka/publications/Kameoka2007IEEETrans03.pdf},
year = {2007}
}
@article{Serr2010,
author = {Serr, Joan and Meinard, M and Grosche, Peter and Arcos, Josep Ll},
file = {:Users/carthach/Documents/Mendeley Desktop/Serr et al. - 2010 - Unsupervised Detection of Music Boundaries by Time Series Structure Features.pdf:pdf},
number = {2009},
pages = {1613--1619},
title = {{Unsupervised Detection of Music Boundaries by Time Series Structure Features}},
year = {2010}
}
@book{Serra:2013,
abstract = {This document proposes a Roadmap for Music Information Research with the aim to expand the context of this
research field from the perspectives of technological advances, user behaviour, social and cultural aspects, and exploitation
methods. The Roadmap embraces the themes of multimodality, multidisciplinarity and multiculturalism,
and promotes ideas of personalisation, interpretation, embodiment, findability and community.
From the perspective of technological advances, the Roadmap defines Music Information Research as a
research field which focuses on the processing of digital data related to music, including gathering and organisation
of machine-readable musical data, development of data representations, and methodologies to process and
understand that data. More specically, this section of the Roadmap examines (i) musically relevant data; (ii)
music representations; (iii) data processing methodologies; (iv) knowledge-driven methodologies; (v) estimation
of elements related to musical concepts; and (vi) evaluation methodologies. A series of challenges are identied,
related to each of these research subjects, including: (i) identifying all relevant types of data sources describing
music, ensuring quality of data, and addressing legal and ethical issues concerning data; (ii) investigating more
meaningful features and representations, unifying formats and extending the scope of ontologies; (iii) enabling
cross-disciplinary transfer of methodologies, integrating multiple modalities of data, and adopting recent machine
learning techniques; (iv) integrating insights from relevant disciplines, incorporating musicological knowledge and
strengthening links to music psychology and neurology; (v) separating the various sources of an audio signal,
developing style-specific musical representations and considering non-Western notation systems; (vi) promoting
best practice evaluation methodology, defining meaningful evaluation methodologies and targeting long-term
sustainability of MIR. Further challenges can be found by referring to the Specic Challenges section under
each subject in the Roadmap.
In terms of user behaviour, the Roadmap addresses the user perspective, both in order to understand the user
roles within the music communication chain and to develop technologies for the interaction of these users with
music data. User behaviour is examined by identifying the types of users related to listening, performing or creating
music. User interaction is analysed by addressing established Human Computer Interaction methodologies, and
novel methods of Tangible and Tabletop Interaction. Challenges derived from these investigations include
analysing user needs and behaviour carefully, identifying new user roles related to music activities; developing
tools and open systems which automatically adapt to the user; designing MIR-based systems more holistically;
addressing collaborative, co-creative and sharing multi-user applications, and expanding MIR interaction beyond
the multi-touch paradigm.
Social and cultural aspects define music as a social phenomenon centering on communication and on the
context in which music is created. Within this context, Music Information Research aims at processing musical data
that captures the social and cultural context and at developing data processing methodologies with which to model
the whole musical phenomenon. The Roadmap analyses specically music-related collective influences, trends and
behaviours, and multiculturalism. Identied challenges include promoting methodologies for modeling music related
social and collective behavior, adapting complex networks and dynamic systems, analysing interaction and
activity in social music networks, identifying music cultures that can be studied from a data driven perspective,
gathering culturally relevant data for different music cultures, and identifying specific music characteristics for
each culture.
The exploitation perspective considers Music Information Research as relevant for producing exploitable
technologies for organising, discovering, retrieving, delivering, and tracking information related to music, in order
to enable improved user experience and commercially viable applications and services for digital media stakeholders.
This section of the Roadmap focuses specifically on music distribution applications, creative tools, and
other exploitation areas such as applications in musicology, digital libraries, education and eHealth. Challenges
include demonstrating better exploitation possibilities of MIR technologies, developing systems that go beyond
recommendation and towards discovery, developing music similarity methods for particular applications and contexts,
developing methodologies of MIR for artistic applications, developing real-time MIR tools for performance,
developing creative tools for commercial environments, producing descriptors based on musicological concepts,
facilitating seamless access to distributed data in digital libraries, overcoming barriers to uptake of technology in
music pedagogy and expanding the scope of MIR applications in eHealth. For a full list of challenges, please refer
to the relevant sections of the Roadmap.
The Music Information Research Roadmap thus identifies current opportunities and challenges and reflects
a variety of stakeholder views, in order to inspire novel research directions for the MIR community, and further
inform policy makers in establishing key future funding strategies for this expanding research field.},
author = {Serra, Xavier and Magas, Michela and Benetos, Emmanouil and Chudy, Magdalena and Dixon, S and Flexer, Arthur and G{\'{o}}mez, Emilia and Gouyon, F and Herrera, P and Jord{\`{a}}, S and Paytuvi, Oscar and Peeters, G and Schl{\"{u}}ter, Jan and Vinet, H and Widmer, G},
file = {::},
isbn = {978-2-9540351-1-6},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Roadmap for Music Information ReSearch}},
year = {2013}
}
@book{Laurier:2011a,
author = {Laurier, C F},
file = {::},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Automatic Classification of musical mood by content-based analysis}},
url = {http://www.tesisenred.net/handle/10803/51582},
year = {2011}
}
@phdthesis{Scheirer:2000,
author = {Scheirer, Eric D},
file = {::},
keywords = {Melody extraction,Source Separation,highlevel,pitch,pitch spectral,pitch temporal},
mendeley-tags = {Melody extraction,Source Separation,highlevel,pitch,pitch spectral,pitch temporal},
school = {Massachusetts Institute of Technology},
title = {{Music-listening systems}},
year = {2000}
}
@article{Technologies:2013,
author = {Technologies, Communication and Marxer, Ricard},
file = {::},
keywords = {source separation},
mendeley-tags = {source separation},
title = {{Audio Source Separation for Music in Low-latency and High-latency Scenarios}},
year = {2013}
}
@article{durrieu2010source,
author = {Durrieu, J-L and Richard, Ga{\"{e}}l and David, Bertrand and F{\'{e}}votte, C{\'{e}}dric},
file = {::},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {D3.1,Source Separation},
mendeley-tags = {D3.1,Source Separation},
number = {3},
pages = {564--575},
publisher = {IEEE},
title = {{Source/filter model for unsupervised main melody extraction from polyphonic audio signals}},
volume = {18},
year = {2010}
}
@article{Virtanen:2000,
author = {Virtanen, Tuomas},
journal = {Master of science thesis, Tampere University of Technology, Finland},
keywords = {Melody extraction,Source Separation},
mendeley-tags = {Melody extraction,Source Separation},
title = {{Audio signal modeling with sinusoids plus noise}},
year = {2000}
}
@inproceedings{Klapuri:2006,
author = {Klapuri, Anssi},
booktitle = {ISMIR},
keywords = {Melody extraction,Source Separation,pitch,pitch polyphonic,salience},
mendeley-tags = {Melody extraction,Source Separation,pitch,pitch polyphonic,salience},
pages = {216--221},
title = {{Multiple Fundamental Frequency Estimation by Summing Harmonic Amplitudes.}},
year = {2006}
}
@article{li2009monaural,
author = {Li, Yipeng and Woodruff, John and Wang, DeLiang},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {7},
pages = {1361--1371},
publisher = {IEEE},
title = {{Monaural musical sound separation based on pitch and common amplitude modulation}},
volume = {17},
year = {2009}
}
@incollection{Jorda2007,
author = {Jord{\`{a}}, Sergi (Universitat Pompeu Fabra)},
booktitle = {The Cambridge Companion to Electronic Music},
file = {:Users/carthach/Documents/Mendeley Desktop/Jord{\`{a}} - 2007 - 5 Interactivity and live computer music.pdf:pdf},
isbn = {9780521868617},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{5 Interactivity and live computer music}},
year = {2007}
}
@article{Jin2011,
author = {Jin, Zhaozhang and Member, Student and Wang, Deliang},
file = {::},
number = {5},
pages = {1091--1102},
title = {{HMM-Based Multipitch Tracking for Noisy and Reverberant Speech}},
volume = {19},
year = {2011}
}
@article{Serra:2012,
author = {Serra, J and M{\"{u}}ller, M and Grosche, Peter and Arcos, JL L},
file = {:Users/carthach/Documents/Mendeley Desktop/Serr et al. - 2010 - Unsupervised Detection of Music Boundaries by Time Series Structure Features.pdf:pdf},
journal = {Twenty-Sixth AAAI Conference on {\ldots}},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {2009},
pages = {1613--1619},
title = {{Unsupervised detection of music boundaries by time series structure features}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPaper/4907},
year = {2012}
}
@article{Bello2005a,
abstract = { Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for {\&}{\#}8220;transient{\&}{\#}8221; regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application.},
author = {Bello, J.P. and Daudet, L. and Abdallah, S. and Duxbury, C. and Davies, M. and Sandler, M.B.},
doi = {10.1109/TSA.2005.851998},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello, Daudet - 2005 - A tutorial on onset detection in music signals.pdf:pdf},
isbn = {1063-6676},
issn = {1063-6676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Attack transcients,audio,note segmentation,novelty detection},
title = {{A Tutorial on Onset Detection in Music Signals}},
volume = {13},
year = {2005}
}
@inproceedings{Julia:2013,
abstract = {While the HCI community has been putting a lot of effort on creating physical interfaces for collaboration, studying multi-user interaction dynamics and creating specific applications to support (and test) this kind of phenomena, it has not addressed the problem of having multiple applications sharing the same interactive space. Having an ecology of rich interactive programs sharing the same interfaces poses questions on how to deal with interaction ambiguity in a cross-application way and still allow different programmers the freedom to program rich unconstrained interaction experiences. This paper describes GestureAgents, a framework demonstrating several techniques that can be used to coordinate different applications in order to have concurrent multi-user multi-tasking interaction and still dealing with gesture ambiguity across multiple applications.},
author = {Juli{\`{a}}, Carles F and Jord{\`{a}}, S and Earnshaw, Nicolas},
booktitle = {TEI 2013},
keywords = {Concurrent interaction,Source Separation,agent- exclusivity,gesture framework,multi-user},
mendeley-tags = {Concurrent interaction,Source Separation,agent- exclusivity,gesture framework,multi-user},
organization = {ACM},
publisher = {ACM},
title = {{GestureAgents: An Agent-Based Framework for Concurrent Multi-Task Multi-User Interaction}},
url = {http://www.mtg.upf.edu/system/files/publications/2013 TEI13 GestureAgents.pdf},
year = {2013}
}
@article{Bock2012a,
author = {B{\"{o}}ck, S and Krebs, Florian and Schedl, Markus},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck, Krebs, Schedl - 2012 - Evaluating the online capabilities of onset detection methods.pdf:pdf},
journal = {ISMIR},
pages = {1--6},
title = {{Evaluating the Online Capabilities of Onset Detection Methods.}},
url = {http://ismir2012.ismir.net/event/papers/049-ismir-2012.pdf/at{\_}download/file},
year = {2012}
}
@article{Munoz2016,
abstract = {Computers and artificial intelligence play a key role in the production of artwork through the designing of synthetic agents that are able to reproduce the capabilities of human artists in assembling high-quality artefacts such as paintings and sculptures. In this context, music composition represents one of the art disciplines that can greatly benefit from the appropriate use of computational intelligence, as witnessed by the large number of research activities performed in this field over the recent years. Nevertheless, the automatic composition of music is far from being completely and precisely perfected due to the intrinsic virtuosity that characterizes human musicians' capabilities. This paper reduces this gap with the proposal of an intelligent scheme for the efficient composition of melodies based on a musical method that is inspired by and strongly characterized by human virtuosity: the unfigured bass technique. In particular, we formulate this music composition technique as an optimization problem and solve it with an adaptive multiagent memetic approach comprising diverse metaheuristics, the composer agents that cooperate to create high-quality four-voice pieces of music starting from a bass line as input. A collection of experimental studies on the famous Bach's four-voice chorales showed that the cooperation among different optimization strategies yields improved performance over the solutions obtained by conventional and hybrid evolutionary algorithms.},
author = {Munoz, Enrique and Cadenas, Jose Manuel and Ong, Yew Soon and Acampora, Giovanni},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Adaptive Memetic Algorithms,Adaptive memetic algorithms,Art,Automatic Music Composition,Computers,Genetic algorithms,Memetics,Multiagent Systems,Music,Optimization,Space exploration,automatic music composition,multiagent systems},
month = {feb},
number = {1},
pages = {1--15},
shorttitle = {Evolutionary Computation, IEEE Transactions on},
title = {{Memetic Music Composition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6945353},
volume = {20},
year = {2016}
}
@article{Hennig2014a,
abstract = {Though the music produced by an ensemble is influenced by multiple factors, including musical genre, musician skill, and individual interpretation, rhythmic synchronization is at the foundation of musical interaction. Here, we study the statistical nature of the mutual interaction between two humans synchronizing rhythms. We find that the interbeat intervals of both laypeople and professional musicians exhibit scale-free (power law) cross-correlations. Surprisingly, the next beat to be played by one person is dependent on the entire history of the other person's interbeat intervals on timescales up to several minutes. To understand this finding, we propose a general stochastic model for mutually interacting complex systems, which suggests a physiologically motivated explanation for the occurrence of scale-free cross-correlations. We show that the observed long-term memory phenomenon in rhythmic synchronization can be imitated by fractal coupling of separately recorded or synthesized audio tracks and thus applied in electronic music. Though this study provides an understanding of fundamental characteristics of timing and synchronization at the interbrain level, the mutually interacting complex systems model may also be applied to study the dynamics of other complex systems where scale-free cross-correlations have been observed, including econophysics, physiological time series, and collective behavior of animal flocks.},
author = {Hennig, Holger},
doi = {10.1073/pnas.1324142111},
isbn = {1324142111},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {1--6},
pmid = {25114228},
title = {{Synchronization in human musical rhythms and mutually interacting complex systems.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25114228},
volume = {2014},
year = {2014}
}
@article{Saito:2008,
author = {Saito, Shoichiro and Kameoka, Hirokazu and Takahashi, Keigo and Nishimoto, Takuya and Sagayama, Shigeki},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {D3.1,Inverse filtering,Source Separation,iteration algorithm,multipitch analysis,pitch visualization,polyphonic music signals},
mendeley-tags = {D3.1,Source Separation},
pages = {639--650},
title = {{Specmurt Analysis of Polyphonic Music Signals}},
url = {http://www.brl.ntt.co.jp/people/kameoka/paper/Saito2008IEEE03.pdf},
year = {2008}
}
@article{Bello2005,
author = {Bello, Juan Pablo and Daudet, Laurent and Abdallah, Samer and Duxbury, Chris and Davies, Mike and Sandler, Mark B and Member, Senior},
file = {:Users/carthach/Documents/Mendeley Desktop/Bello, Daudet - 2005 - A tutorial on onset detection in music signals.pdf:pdf},
number = {5},
pages = {1035--1047},
title = {{A Tutorial on Onset Detection in Music Signals}},
volume = {13},
year = {2005}
}
@article{Wang2003,
author = {Wang, A},
journal = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{An Industrial Strength Audio Search Algorithm.}},
url = {http://www.ee.columbia.edu/{~}dpwe/papers/Wang03-shazam.pdf},
year = {2003}
}
@article{parra2003blind,
author = {Parra, Lucas and Sajda, Paul},
journal = {The Journal of Machine Learning Research},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {1261--1269},
publisher = {JMLR. org},
title = {{Blind source separation via generalized eigenvalue decomposition}},
volume = {4},
year = {2003}
}
@article{Bock2012b,
abstract = {In this paper, we evaluate various onset detection algo- rithms in terms of their online capabilities. Most methods use some kind of normalization over time, which renders them unusable for online tasks. We modified existing methods to enable online application and evaluated their performance on a large dataset consisting of 27,774 an- notated onsets. We focus particularly on the incorporated preprocessing and peak detection methods. We show that, with the right choice of parameters, the maximum achievable performance is in the same range as that of offline algorithms, and that preprocessing can improve the results considerably. Furthermore, we propose a new onset detec- tion method based on the common spectral flux and a new peak-picking method which outperforms traditional methods both online and offline and works with audio signals of various volume levels.},
author = {B{\"{o}}ck, Sebastian and Krebs, Florian and Schedl, Markus},
journal = {13th International Society for Music Information Retrieval Conference},
pages = {49--54},
title = {{Evaluating the online capabilities of onset detection methods}},
url = {http://ismir2012.ismir.net/event/papers/049-ismir-2012.pdf},
year = {2012}
}
@article{Mauch2010,
abstract = {The automatic detection and transcription of musical chords from audio is an established music computing task. The choice of chord profiles and higher-level time-series modelling have received a lot of attention, resulting in methods with an overall performance of more than 70{\%} in the MIREX Chord Detection task 2009. Research on the front end of chord transcription algorithms has often concentrated on finding good chord templates to fit the chroma features. In this paper we reverse this approach and seek to find chroma features that are more suitable for usage in a musically-motivated model. We do so by performing a prior approximate transcription using an existing technique to solve non-negative least squares problems (NNLS). The resulting NNLS chroma features are tested by using them as an input to an existing state-of-the-art high-level model for chord transcription. We achieve very good results of 80{\%} accuracy using the song collection and metric of the 2009 MIREX Chord Detection tasks. This is a significant increase over the top result (74{\%}) inMIREX2009. The nature of some chords makes their identification particularly susceptible to confusion between fundamental frequency and partials. We show that the recognition of these diffcult chords in particular is substantially improved by the prior approximate transcription using NNLS.},
author = {Mauch, Matthias and Dixon, Simon},
file = {::},
journal = {Proceedings of the International Conference on Music Information Retrieval (ISMIR)},
keywords = {chord detection,chord extraction,chromagram,nnls,non-negative least squares,tection,transcription},
number = {1},
pages = {135--140},
title = {{Approximate note transcription for the improved identification of difficult chords}},
url = {https://www.eecs.qmul.ac.uk/{~}simond/pub/2010/Mauch-Dixon-ISMIR-2010.pdf},
year = {2010}
}
@article{kurth2008efficient,
author = {Kurth, Frank and Muller, Meinard},
journal = {Audio, Speech, and Language Processing, IEEE Transactions on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {2},
pages = {382--395},
publisher = {IEEE},
title = {{Efficient index-based audio matching}},
volume = {16},
year = {2008}
}
@article{auger1995improving,
author = {Auger, Fran{\c{c}}ois and Flandrin, Patrick},
journal = {Signal Processing, IEEE Transactions on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {5},
pages = {1068--1089},
publisher = {IEEE},
title = {{Improving the readability of time-frequency and time-scale representations by the reassignment method}},
volume = {43},
year = {1995}
}
@inproceedings{McLeod:2005,
author = {McLeod, Philip and Wyvill, Geoff},
booktitle = {Proceedings of International Computer Music Conference, ICMC},
keywords = {Melody extraction,Source Separation,pitch,pitch monophonic,pitch temporal},
mendeley-tags = {Melody extraction,Source Separation,pitch,pitch monophonic,pitch temporal},
title = {{A smarter way to find pitch}},
year = {2005}
}
@article{Peeters2004a,
author = {Peeters, Geoffroy},
file = {:Users/carthach/Documents/Mendeley Desktop/Peeters - 2004 - Deriving musical structures from signal analysis for music audio summary generation sequence and state approach.pdf:pdf},
journal = {Lecture Notes in Computer Science},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Deriving musical structures from signal analysis for music audio summary generation:" sequence" and" state" approach}},
url = {http://link.springer.com/content/pdf/10.1007/978-3-540-39900-1{\_}14.pdf},
year = {2004}
}
@article{Article2011,
author = {Walter, Schulze and van der Merwe, Brink},
file = {:Users/carthach/Documents/Mendeley Desktop/Walter, van der Merwe - 2011 - Music Generation with Markov Models.pdf:pdf},
journal = {IEEE Multimedia},
number = {18},
pages = {78--86},
title = {{Music Generation with Markov Models}},
year = {2011}
}
@article{Delgado2009,
author = {Delgado, Miguel and Fajardo, Waldo and Molina-Solana, Miguel},
doi = {10.1016/j.eswa.2008.05.028},
file = {:Users/carthach/Documents/Mendeley Desktop/Delgado, Fajardo, Molina-Solana - 2009 - Inmamusys Intelligent multiagent music system.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
month = {apr},
number = {3},
pages = {4574--4580},
publisher = {Elsevier Ltd},
title = {{Inmamusys: Intelligent multiagent music system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417408002194},
volume = {36},
year = {2009}
}
@article{Salamon:2014,
abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of {\{}$\backslash$textquoteleft{\}}melody{\{}$\backslash$textquoteright{\}} from both musical and signal processing perspectives, and provide a case study which interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation and applications which build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
author = {Salamon, J and G{\'{o}}mez, Emilia and Ellis, D P W and Richard, G},
file = {::},
journal = {IEEE Signal Processing Magazine},
keywords = {Melody extraction,Source Separation,pitch predominant,salience},
mendeley-tags = {Melody extraction,Source Separation,pitch predominant,salience},
title = {{Melody Extraction from Polyphonic Music Signals: Approaches, Applications and Challenges}},
url = {files/publications/Salamon{\_}Gomez{\_}Ellis{\_}Richard{\_}MelodyExtractionReview{\_}IEEESPM{\_}2013.pdf}
}
@book{Meyer2008,
author = {Meyer, Leonard B.},
publisher = {University of chicago Press},
title = {{Emotion and meaning in music}},
year = {2008}
}
@article{Dannenberg:2002,
author = {Dannenberg, RB B and Hu, N},
file = {:Users/carthach/Documents/Mendeley Desktop/Dannenberg, Hu - 2002 - Discovering musical structure in audio recordings.pdf:pdf},
journal = {Music and Artificial Intelligence},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Discovering musical structure in audio recordings}},
url = {http://link.springer.com/chapter/10.1007/3-540-45722-4{\_}6},
year = {2002}
}
@article{De-Cheveigne:2002,
author = {{De Cheveign{\'{e}}}, Alain and Kawahara, Hideki},
file = {:Users/carthach/Documents/Mendeley Desktop/de Cheveign{\'{e}}, Kawahara - 2002 - YIN, a fundamental frequency estimator for speech and music.pdf:pdf},
journal = {The Journal of the Acoustical Society of America},
keywords = {Melody extraction,Source Separation,pitch,pitch monophonic,pitch temporal},
mendeley-tags = {Melody extraction,Source Separation,pitch,pitch monophonic,pitch temporal},
pages = {1917},
title = {{YIN, a fundamental frequency estimator for speech and music}},
volume = {111},
year = {2002}
}
@article{Pearce2005,
abstract = {The paper concerns the use of multiple viewpoint representation schemes for prediction with statistical models of monophonic music. We present an experimental comparison of the performance of two techniques for combining predictions within the multiple viewpoint framework. The results demonstrate that a new technique based on a weighted geometric mean outperforms existing techniques. This finding is discussed in terms of previous research in machine learning.},
author = {Pearce, Marcus and Conklin, Darrell and Wiggins, Geraint},
file = {::},
isbn = {3-540-24458-1},
issn = {03029743},
journal = {Computer Music Modeling and Retrieval},
pages = {295--312},
title = {{Methods for Combining Statistical Models of Music}},
url = {http://www.springerlink.com/index/CPTVYB2CC735HDX8.pdf},
volume = {3310},
year = {2005}
}
@article{Bartsch2005a,
author = {Bartsch, M.a. and Wakefield, G.H. H},
doi = {10.1109/TMM.2004.840597},
file = {:Users/carthach/Documents/Mendeley Desktop/Bartsch, Wakefield - 2005 - Audio thumbnailing of popular music using chroma-based representations.pdf:pdf},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
month = {feb},
number = {1},
pages = {96--104},
title = {{Audio thumbnailing of popular music using chroma-based representations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1386245},
volume = {7},
year = {2005}
}
@article{Toussaint2005,
abstract = {Observes that many non-Western rhythms are represented by the Euclidean algorithm which, in this form, is the way of distributing x items among n places in a most spaced out way - E(2,5)=10010 (assumed to rotate). Lists many examples of world musics where n=7 or 14 (see below).},
author = {Toussaint, Godfried},
journal = {BRIDGES: Mathematical Connections in Art, Music and Science},
keywords = {Septuple Time},
pages = {1--25},
title = {{The Euclidean Algorithm Generates Traditional Musical Rhythms}},
year = {2005}
}
@article{Witek2014,
abstract = {INMUSIC, THE RHYTHMS OF DIFFERENT INSTRUMENTS are often syncopated against each other to create ten- sion. Existing perceptual theories of syncopation cannot adequately model such kinds of syncopation since they assume monophony. This study investigates the effects of polyphonic context, instrumentation and metrical location on the salience of syncopations.Musicians and nonmusicians were asked to tap along to rhythmic pat- terns of a drum kit and rate their stability; in these patterns, syncopations occurred among different num- bers of streams, with different instrumentation and at different metrical locations. The results revealed that the stability of syncopations depends on all these factors and music training, in variously interacting ways. It is pro- posed that listeners' experiences of syncopations are shaped by polyphonic and instrumental configuration, metrical structure, and individual music training, and a number of possiblemechanisms are considered, includ- ing the rhythms' acoustic properties, ecological associa- tions, statistical learning, and timbral differentiation. Received:},
author = {Witek, Maria A G and Clarke, Eric F. and Kringelbach, Morten L. and Vuust, Peter},
doi = {10.1525/rep.2008.104.1.92.This},
file = {::},
isbn = {0520057295},
journal = {Music Perception: An Interdisciplinary Journal},
keywords = {meter,polyphony,rhythm,syncopation},
number = {2},
pages = {201 -- 217},
title = {{Effects of Polyphonic Context, Instrumentation, and Metrical Location on Syncopation in Music}},
volume = {32},
year = {2014}
}
@article{dannenberg2006music,
author = {Dannenberg, Roger B and Raphael, Christopher},
journal = {Communications of the ACM},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {8},
pages = {38--43},
publisher = {ACM},
title = {{Music score alignment and computer accompaniment}},
volume = {49},
year = {2006}
}
@article{comon1994independent,
author = {Comon, Pierre},
journal = {Signal processing},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {3},
pages = {287--314},
publisher = {Elsevier},
title = {{Independent component analysis, a new concept?}},
volume = {36},
year = {1994}
}
@inproceedings{gillet2004automatic,
author = {Gillet, Olivier and Richard, Ga{\"{e}}l},
booktitle = {Acoustics, Speech, and Signal Processing, 2004. Proceedings.(ICASSP'04). IEEE International Conference on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {iv----269},
title = {{Automatic transcription of drum loops}},
volume = {4},
year = {2004}
}
@article{Mitrovic2010,
author = {Mitrovi{\'{c}}, D and Zeppelzauer, Matthias and Breiteneder, Christian},
journal = {Advances in computers},
pages = {71--150},
title = {{Features for content-based audio retrieval}},
url = {http://www.sciencedirect.com/science/article/pii/S0065245810780037},
volume = {78},
year = {2010}
}
@article{shashanka2008probabilistic,
author = {Shashanka, Madhusudana and Raj, Bhiksha and Smaragdis, Paris},
journal = {Computational intelligence and neuroscience},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {Hindawi Publishing Corporation},
title = {{Probabilistic latent variable models as nonnegative factorizations}},
volume = {2008},
year = {2008}
}
@inproceedings{raczynski2007multipitch,
author = {Raczynski, Stanislaw A and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {in ISMIR 2007, 8th International Conference on Music Information Retrieval},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {Citeseer},
title = {{Multipitch analysis with harmonic nonnegative matrix approximation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=BB3DFED5AFA5D9BD8A156B516308D367?doi=10.1.1.205.8764{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@inproceedings{Duxbury2003,
abstract = {We present a novel method for onset detection in musical signals. It improves over previous energy-based and phase-based approaches by combining both types of information in the complex domain. It generates a detection function that is sharp at the position of onsets and smooth everywhere else. Results on a handlabelled data-set show that high detection rates can be achieved at very low error rates. The approach is more robust than its predecessors both theoretically and practically.},
author = {Duxbury, C and Bello, J P and Davies, M and Sandler, M},
booktitle = {Proc Digital Audio Effects Workshop DAFx},
pages = {6--9},
title = {{Complex domain onset detection for musical signals}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.9197{\&}rep=rep1{\&}type=pdf},
volume = {1},
year = {2003}
}
@article{Smith2002,
author = {Smith, LI},
isbn = {0471852236},
journal = {Cornell University, USA},
title = {{A Tutorial on Principal Components Analysis}},
url = {http://www.ce.yildiz.edu.tr/personal/songul/file/1097/principal{\_}components.pdf},
year = {2002}
}
@article{Laurier:2011,
author = {Laurier},
journal = {PhD},
keywords = {Source Separation,genre classification,mood classification},
mendeley-tags = {Source Separation},
title = {{Music{\_}Mood{\_}Classification}},
year = {2011}
}
@article{Dressler:2009,
author = {Dressler, Karin},
journal = {5th Music Information Retrieval Evaluation eXchange (MIREX)},
keywords = {Melody extraction,Source Separation},
mendeley-tags = {Melody extraction,Source Separation},
title = {{Audio melody extraction for mirex 2009}},
year = {2009}
}
@article{Maher:1994,
author = {Maher, Robert C and Beauchamp, James W},
journal = {The Journal of the Acoustical Society of America},
keywords = {Melody extraction,Source Separation,pattern matching,pitch,spectral pitch},
mendeley-tags = {Melody extraction,Source Separation,pattern matching,pitch,spectral pitch},
pages = {2254},
title = {{Fundamental frequency estimation of musical signals using a two-way mismatch procedure}},
volume = {95},
year = {1994}
}
@inproceedings{Assayag:2006,
abstract = {We describe a multi-agent architecture for an improvization oriented musician-machine interaction system that learns in real time from human performers. The improvization kernel is based on sequence modeling and statistical learning. The working system involves a hybrid architecture using two popular composition/perfomance environments, Max and OpenMusic, that are put to work and communicate together, each one handling the process at a different time/memory scale. The system is capable of processing real-time audio/video as well as MIDI. After discussing the general cognitive background of improvization practices, the statistical modeling tools and the concurrent agent architecture are presented. Finally, a prospective Reinforcement Learning scheme for enhancing the system's realism is described. },
author = {Assayag, G and Bloch, G and Chemillier, M and Cont, A and Dubnov., S},
booktitle = {ACM Multimedia Workshop on Audio and Music Computing for Multimedia},
file = {:Users/carthach/Documents/Mendeley Desktop/Assayag et al. - 2006 - Omax brothers A dynamic topology of agents for improvisation learning.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Omax brothers: A dynamic topology of agents for improvisation learning}},
year = {2006}
}
@article{Sturm2015,
author = {Sturm, Bob L. and Collins, Nick},
journal = {Digital Music Research Network 9},
title = {{Four Challenges for Music Information Retrieval Researchers}},
year = {2015}
}
@article{Ono:2008,
author = {Ono, Nobutaka and Miyamoto, Kenichi and Kameoka, Hirokazu and Sagayama, Shigeki},
journal = {Proceedings of the 9th International Conference on Music Information Retrieval (ISMIR)},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {139--144},
title = {{A Real-time equalizer of harmonic and percussive components in music signals}},
url = {http://ismir2008.ismir.net/papers/ISMIR2008{\_}120.pdf},
year = {2008}
}
@article{canadas2010multiple,
author = {{Ca{\~{n}}adas Quesada}, F J and {Ruiz Reyes}, N and {Vera Candeas}, P and Carabias, J J and Maldonado, S},
journal = {Journal of New Music Research},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {1},
pages = {93--107},
publisher = {Taylor {\&} Francis},
title = {{A multiple-F0 estimation approach based on Gaussian spectral modelling for polyphonic music transcription}},
volume = {39},
year = {2010}
}
@article{Honingh2015,
author = {Honingh, Aline and Panteli, Maria and Brockmeier, Thomas and {L{\'{o}}pez Mej{\'{i}}a}, David I{\~{n}}aki and Sadakata, Makiko},
doi = {10.1080/09298215.2015.1107102},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {agreement,inter-rater,music similarity,perception,rhythm,timbre},
number = {December},
pages = {1--18},
title = {{Perception of Timbre and Rhythm Similarity in Electronic Dance Music}},
volume = {8215},
year = {2015}
}
@misc{Vogel2015,
author = {Vogel, Cristian},
booktitle = {Kyma 7: Rhythmic Computation Lab},
title = {{Rhythmic Computation Lab}},
url = {http://www.cristianvogel.com/neverenginelabs/product/rhythmiccomputationlab},
urldate = {2015-12-01},
year = {2015}
}
@misc{JanSchluterandSebastianBock,
author = {B ock, Jan Schl uter and Sebastian},
keywords = {ML,OnsetDetection},
mendeley-tags = {ML,OnsetDetection},
title = {{Musical Onset Detection with Convolutional Neural Networks}},
url = {http://www.ofai.at/{~}jan.schlueter/pubs/2013{\_}mml.pdf},
urldate = {2014-03-25}
}
@article{Ariza2009,
abstract = {Procedural or algorithmic approaches to generating music have been explored in the medium of software for over fifty years. Occasionally, researchers have attempted to evaluate the success of these generative music systems by measuring the perceived quality or style conformity of isolated musical outputs. These tests are often conducted in the form of comparisons between computer-aided output and non-computer-aided output. The model of the Turing Test (TT), Alan Turing's proposed “Imitation Game” (Turing 1950), has been submitted and employed as a framework for these comparisons. In this context, it is assumed that if machine output sounds like, or is preferred to, human output, the machine has succeeded. The nature of this success is rarely questioned, and is often interpreted as evidence of a successful generative music system. Such listener surveys, within necessary statistical and psychological constraints, may be pooled to gauge common responses to and interpretations of music—yet these surveys are not TTs. This article argues that Turing's well-known proposal cannot be applied to executing and evaluating listener surveys. Whereas pre-computer generative music systems have been employed for centuries, the idea of testing the output of such systems appears to only have emerged since computer implementation. One of the earliest tests is reported in Hiller (1970, p. 92): describing the research of Havass (1964), Hiller reports that, at a conference in 1964, Havass conducted an experiment to determine if listeners could distinguish computer-generated and traditional melodies. Generative techniques derived from the fields of artificial intelligence (AI; for example, neural nets and various learning algorithms) and artificial life (e.g., genetic algorithms and cellular automata) may be associated with such tests due to explicit reference to biological systems. Yet, since only the output of the system is tested (that is, system and interface design are ignored), any generative technique can be employed. These tests may be associated with the broader historical context of human-versus-machine tests, as demonstrated in the American folk-tale of John Henry versus the steam hammer (Nelson 2006) or the more recent competition of Garry Kasparov versus Deep Blue (Hsu 2002). Some tests attempt to avoid measures of subjective quality by measuring perceived conformity to known musical artifacts. These musical artifacts are often used to create the music being tested: they are the source of important generative parameters, data, or models. The design goals of a system provide context for these types of tests. Pearce, Meredith, and Wiggins (2002, p. 120) define four motivations for the development of generative music systems: (1) composer-designed tools for personal use, (2) tools designed for general compositional use, (3) “theories of a musical style . . . implemented as computer programs,” and (4) “cognitive theories of the processes supporting compositional expertise . . . implemented as computer programs.” Such motivational distinctions may be irrelevant if the system is used outside of the context of its creation; for this reason, system-use cases, rather than developer motivations, might offer alternative distinctions. The categories proposed by Pearce, Meredith, and Wiggins can be used to generalize about two larger use cases: systems used as creative tools for making original music (motivations 1 and 2, above), and systems that are designed to computationally model theories of musical style or cognition (motivations 3 and 4). These two larger categories will be referred to as “creative tools” and “computational models.” Although design motivation is not included in the seven descriptors of computer-aided algorithmic systems proposed in Ariza (2005), the “idiom affinity” descriptor is closely related: systems with singular idiom affinities are often computational models. Explicitly testing the output of generative music systems is uncommon. As George Papadopoulos and Geraint Wiggins (1999, p. 113) observe, research in generative music systems demonstrates a “lack of experimental methodology.” Furthermore, “there is usually no evaluation of the output by real experts.” Similarly, Pearce, Meredith, and Wiggins (2002, p. 120), presumably describing all types of generative music systems, state that “researchers often fail to adopt suitable methodologies for the development and evaluation of composition programs and this, in turn, has compromised the practical or theoretical value of their research.” In the case of creative tools, the lack of empirical output evaluation is not a shortcoming: creative...},
author = {Ariza, Christopher},
doi = {10.1162/comj.2009.33.2.48},
file = {::},
issn = {0148-9267},
journal = {Computer Music Journal},
number = {2},
pages = {48--70},
title = {{The Interrogator as Critic : The Turing Test and the Evaluation of Generative Music Systems}},
url = {http://muse.jhu.edu/journals/cmj/summary/v033/33.2.ariza.html},
volume = {33},
year = {2009}
}
@inproceedings{ueda2010hmm,
author = {Ueda, Yushi and Uchiyama, Yuuki and Nishimoto, Takuya and Ono, Nobutaka and Sagayama, Shigeki},
booktitle = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on},
file = {::},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {5518--5521},
title = {{HMM-based approach for automatic chord detection using refined acoustic features}},
year = {2010}
}
@article{Milne2015a,
author = {Milne, Andrew J. and Dean, Roger T.},
file = {::},
isbn = {8187672641},
issn = {1531-5169},
journal = {Computer Music journal},
number = {1},
pages = {35--53},
title = {{Computational Creation and Morphing of Multilevel Rhythms by Control of Evenness}},
volume = {40},
year = {2015}
}
@article{Ryynanen:2008,
author = {Ryyn{\"{a}}nen, Matti P and Klapuri, Anssi P},
journal = {Computer Music Journal},
keywords = {Melody extraction,pitch polyphonic,source separation},
mendeley-tags = {Melody extraction,pitch polyphonic,source separation},
number = {3},
pages = {72--86},
publisher = {MIT Press},
title = {{Automatic transcription of melody, bass line, and chords in polyphonic music}},
volume = {32},
year = {2008}
}
@article{jutten1991blind,
author = {Jutten, Christian and Herault, Jeanny},
journal = {Signal processing},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {1},
pages = {1--10},
publisher = {Elsevier},
title = {{Blind separation of sources, part I: An adaptive algorithm based on neuromimetic architecture}},
volume = {24},
year = {1991}
}
@incollection{Sagayama:2004,
author = {Sagayama, Shigeki and Takahashi, Keigo and Kameoka, Hirokazu and Nishimoto, Takuya},
booktitle = {ISCA Tutorial and Research Workshop (ITRW) on Statistical and Perceptual Audio Processing},
keywords = {D3.1,Source Separation},
mendeley-tags = {D3.1,Source Separation},
title = {{Specmurt Anasylis: A Piano-Roll-Visualization of Polyphonic Music Signals by Deconvolution of Log-Frequency Spectrum}},
url = {http://users.cis.fiu.edu/{~}lli003/Music/mv/9.pdf},
year = {2004}
}
@inproceedings{Zhu2009,
author = {Zhu, Yongwei and Tan, Hui Li and Rahardja, Susanto},
booktitle = {2009 IEEE International Conference on Multimedia and Expo},
issn = {1945-7871},
keywords = {Audio recording,Content based retrieval,Data mining,Information analysis,Instruments,Length measurement,Music information retrieval,Recommender systems,Rhythm,Velocity measurement,audio classification,audio recordings,audio signal processing,drum loop,drum loop pattern extraction,feature extraction,meter estimation information,music,musical instruments,onset clustering information,polyphonic music audio,rhythm analysis},
language = {English},
month = {jun},
pages = {482--485},
publisher = {IEEE},
title = {{Drum loop pattern extraction from polyphonic music audio}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5202539},
year = {2009}
}
@article{Casey2008,
abstract = {The steep rise in music downloading over CD sales has created a major shift in the music industry away from physical media formats and towards online products and services. Music is one of the most popular types of online information and there are now hundreds of music streaming and download services operating on the World-Wide Web. Some of the music collections available are approaching the scale of ten million tracks and this has posed a major challenge for searching, retrieving, and organizing music content. Research efforts in music information retrieval have involved experts from music perception, cognition, musicology, engineering, and computer science engaged in truly interdisciplinary activity that has resulted in many proposed algorithmic and methodological solutions to music search using content-based methods. This paper outlines the problems of content-based music information retrieval and explores the state-of-the-art methods using audio cues (e.g., query by humming, audio fingerprinting, content-based music retrieval) and other cues (e.g., music notation and symbolic representation), and identifies some of the major challenges for the coming years.},
author = {Casey, M.A. and Veltkamp, R. and Goto, M. and Leman, M. and Rhodes, C. and Slaney, M.},
doi = {10.1109/JPROC.2008.916370},
file = {:Users/carthach/Documents/Mendeley Desktop/04472077.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Audio signal processing,Helper,Internet,SectionIII: Bag-of-frames,SectionIV: music similarity,Source Separation,Survey,cognition,content-based music information retrieval,content-based retrieval,download services,information retrieval,metadata,music content,music downloading,music industry,music information retrieval,music perception,music streaming,music tracks,musicWorld-Wide Web,musicology,online music collections,online products,online services,semantic gap,sound,symbolic processing,user interfaces},
mendeley-tags = {Helper,Internet,SectionIII: Bag-of-frames,SectionIV: music similarity,Source Separation,Survey,cognition,content-based music information retrieval,content-based retrieval,download services,information retrieval,metadata,music content,music downloading,music industry,music information retrieval,music perception,music streaming,music tracks,musicWorld-Wide Web,musicology,online music collections,online products,online services,semantic gap,sound},
title = {{Content-Based Music Information Retrieval: Current Directions and Future Challenges}},
volume = {96},
year = {2008}
}
@misc{JanSchluterandSebastianBock2,
author = {B ock, Jan Schl uter and Sebastian},
file = {:Users/carthach/Documents/Mendeley Desktop/Schl{\"{u}}ter, B{\"{o}}ck - 2013 - Musical Onset Detection with Convolutional Neural Networks.pdf:pdf},
keywords = {ML,OnsetDetection},
mendeley-tags = {ML,OnsetDetection},
title = {{Musical Onset Detection with Convolutional Neural Networks}},
url = {http://www.ofai.at/{~}jan.schlueter/pubs/2013{\_}mml.pdf},
urldate = {2014-03-25}
}
@article{robel2005onset,
author = {R{\"{o}}bel, Axel},
journal = {MIREX Online Proceedings (ISMIR 2005)},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{Onset detection in polyphonic signals by means of transient peak classification}},
url = {http://www.music-ir.org/evaluation/mirex-results/articles/all/roebel.pdf},
year = {2005}
}
@misc{Urtubia2015,
author = {Urtubia, Hector},
booktitle = {Big Robot Studios},
title = {{Robotic Drums}},
url = {http://bigrobotstudios.com/{\#}roboticdrums},
urldate = {2015-12-01},
year = {2015}
}
@article{Steedman1984,
author = {Steedman, Mark J.},
journal = {Music Perception},
pages = {52--77},
title = {{A generative grammar for jazz chord sequences}},
year = {1984}
}
@article{Honingh2015a,
abstract = {Music similarity is known to be a multi-dimensional concept, depending among others on rhythm similarity and timbre similarity. The present study aims to investigate whether such sub-dimensions of similarity can be assessed independently and how they relate to general similarity. To this end, we performed a series of web-based perceptual experiments on timbre, rhythm and general similarity in electronic dance music. Participants were asked to rate similarities of music pairs on a 4-point Likert scale. The results indicated that the ratings in the three types of similarity did not completely overlap and that participants showed slight to fair agreement in their ratings in all conditions. Together, the results suggest that it is possible to assess sub-dimensions of similarities independently to some extent. Interestingly, general music similarity was not completely explained by the summation of timbre and rhythm similarity. Based on this, a novel hypothesis of how general music similarity follows from its c...},
author = {Honingh, Aline and Panteli, Maria and Brockmeier, Thomas and {L{\'{o}}pez Mej{\'{i}}a}, David I{\~{n}}aki and Sadakata, Makiko},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
keywords = {inter-rater agreement,music similarity,perception,rhythm,timbre},
language = {en},
month = {nov},
number = {4},
pages = {373--390},
publisher = {Routledge},
title = {{Perception of Timbre and Rhythm Similarity in Electronic Dance Music}},
url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1107102},
volume = {44},
year = {2015}
}
@inproceedings{Grote2015,
author = {Grote, Florian},
booktitle = {Culture and Computer Science},
file = {::},
pages = {219 -- 230},
title = {{The Music of Machines: Investigating Culture and Technology in Musical Creativity}},
url = {http://www.vwh-verlag.de/vwh/wp-content/uploads/2015/05/kui2015{\_}final{\_}split.pdf},
year = {2015}
}
@article{Pachet,
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1007/s10601-010-9101-4},
file = {:Users/carthach/Documents/Mendeley Desktop/pachet-09c.pdf:pdf},
issn = {1383-7133},
journal = {Constraints},
keywords = {Computer Science},
number = {2},
pages = {148--172},
publisher = {Springer Netherlands},
title = {{Markov constraints: steerable generation of Markov sequences}},
url = {http://dx.doi.org/10.1007/s10601-010-9101-4},
volume = {16}
}
@book{Brooks1993,
author = {Brooks, F.P. and Hopkins, A.L. and Neumann, P.G. and Wright, W.V.},
editor = {Schwanauer, S. M. and Levitt, D. A.},
pages = {23--40},
publisher = {Cambridge: MIT},
title = {{An experiment in musical composition}},
year = {1993}
}
@article{Pearce2002,
abstract = {Our aim in this paper is to clarify the range of motivations that have inspired the development of computer programs for the composition of music. We consider this to be important since different methodologies are appropriate for different motivations and goals. We argue that a widespread failure to specify the motivations and goals involved has lead to a methodological malaise in music related research. A brief consideration of some of the earliest at- tempts to produce computational systems for the composition of music leads us to identify four activities involving the development of computer programs which compose music each of which is inspired by different practical or theoretical motivations. These activities are algorithmic composition, the design of compositional tools, the com- putational modelling of musical styles and the computational modelling of music cognition. We consider these four motivations in turn, illustrating the problems that have arisen from failing to distinguish between them. We propose a terminology that clearly differentiates the activities defined by the four motivations and present methodological suggestions for research in each domain. While it is clearly important for researchers to embrace developments in related disciplines, we argue that research in the four domains will continue to stagnate unless the motivations and aims of research projects are clearly stated and appropriate methodologies are adopted for developing and evaluating systems that compose music.},
author = {Pearce, Marcus and Meredith, David and Wiggins, Geraint},
doi = {10.1177/102986490200600203},
file = {::},
isbn = {1029-8649},
issn = {1029-8649},
journal = {Musicae Scientiae},
number = {2},
pages = {119--147},
title = {{Motivations and Methodologies for Automation of the Compositional Process}},
volume = {6},
year = {2002}
}
@inproceedings{han2007desoloing,
author = {Han, Yushen and Raphael, Christopher},
booktitle = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {145--148},
title = {{Desoloing Monaural Audio Using Mixture Models.}},
year = {2007}
}
@article{Ames1989,
abstract = {The author combines a survey of Markov-based efforts in automated composition with a tutorial demonstrating how various theoretical properties associated with Markov processes can be put to practical use. The historical background is traced from A. A. Markov's original formulation through to the present. A digression into Markov-chain theory introduces 'waiting counts' and 'stationary probabilities'. The author's "Demonstration 4" for solo clarinet illustrates how these properties affect the behavior of a melody composed using Markov chains. This simple example becomes a point of departure for increasingly general interpretations of the Markov process. The interpretation of 'states' is reevaluated in the light of recent musical efforts that employ Markov chains of higher-level objects and in the light of other efforts that incorporate relative attributes into the possible interpretations. Other efforts expand Markov's original definition to embrace 'Nth-order' transitions, evolving transition matrices and chains of chains. The remainder of this article contrasts Markov processes with alternative compositional strategies.},
author = {Ames, Charles},
doi = {10.2307/1575226},
file = {::},
issn = {0024094X},
journal = {Leonardo},
number = {2},
pages = {175--187},
title = {{The Markov process as a compositional model: a survey and tutorial}},
url = {http://www.jstor.org/stable/10.2307/1575226},
volume = {22},
year = {1989}
}
@article{Lahat:1987,
author = {Lahat, M and Niederjohn, R and Krubsack, D},
journal = {Acoustics, Speech and Signal Processing, IEEE Transactions on},
keywords = {Melody extraction,Source Separation,pitch,spectral ACF,spectral pitch},
mendeley-tags = {Melody extraction,Source Separation,pitch,spectral ACF,spectral pitch},
number = {6},
pages = {741--750},
publisher = {IEEE},
title = {{A spectral autocorrelation method for measurement of the fundamental frequency of noise-corrupted speech}},
volume = {35},
year = {1987}
}
@inproceedings{Pachet2002,
abstract = {We propose a system, the Continuator, that bridges the gap between two classes of traditionally incompatible musical systems: 1) interactive musical systems, limited in their ability to generate stylistically consistent material, and 2) music imitation systems, which are fundamentally not inter- active. Our purpose is to allow musicians to extend their technical ability with stylistically consistent, automatically learnt material. This goal requires the ability for the system to build operational representations of musical styles in a real time context. Our approach is based on a Markov model of musical styles augmented to account for musical issues such as management of rhythm, beat, harmony, and imprecision. The resulting system is able to learn and generate music in any style, either in standalone mode, as continuations of musician's input, or as interactive improvisation back up. Lastly, the very design of the system makes possible new modes of musical collaborative playing. We describe the architecture, implementation issues and experimentations conducted with the system in several real world contexts.},
author = {Pachet, Fran{\c{c}}ois},
booktitle = {Proceedings of the International Computer Music Conference},
doi = {10.1076/jnmr.32.3.333.16861},
file = {:Users/carthach/Documents/Mendeley Desktop/pachet-02f.pdf:pdf},
isbn = {0929-8215},
issn = {0929-8215},
pages = {333--341},
title = {{The Continuator: Musical Interaction With Style}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1076/jnmr.32.3.333.16861{\&}magic=crossref{\%}7C{\%}7CD404A21C5BB053405B1A640AFFD44AE3},
year = {2002}
}
@article{Yeh:2005,
author = {Yeh, Chunghsin and Robel, Axel and Rodet, Xavier},
journal = {Acoustics, Speech, and Signal Processing, 2005. Proceedings.(ICASSP'05). IEEE International Conference on},
keywords = {D3.1,Source Separation},
mendeley-tags = {D3.1,Source Separation},
pages = {iii--------225},
title = {multiple fundamental frequency estimation of polyphonic music signals},
url = {http://ispl.korea.ac.kr/conference/ICASSP2005/pdfs/0300225.pdf},
year = {2005}
}
@article{Pressing1990,
abstract = {The author discussed cybernetic issues in interactive performance interfaces and systems and their musical implications. The traditional models of instrument performance are presented in terms of information processing and subsequently contrasted with the potentials of extended, reconfigured, interactive, and intelligent instruments. The idea of dimensionality of control is proposed as an index for the real-time expressive potential of such music performance systems.},
author = {Pressing, Jeff},
doi = {10.2307/3680113},
issn = {01489267},
journal = {Computer Music Journal},
keywords = {Dimensionality of Control,Information Processing,Musical Instruments,Systems Science and Cybernetics--Economic Cybernet},
number = {1},
pages = {12--25},
title = {{Cybernetic Issues in Interactive Performance Systems}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0025402318{\&}partnerID=40{\&}md5=5c1db6a5c50ac0fa262c4aeb31eb99af{\%}5Cnhttp://www.jstor.org/stable/3680113},
volume = {14},
year = {1990}
}
@article{Smaragdis:2003,
author = {Smaragdis, Paris and Brown, Judith C},
journal = {Applications of Signal Processing to Audio and Acoustics, 2003 IEEE Workshop on.},
keywords = {D3.1,Source Separation},
mendeley-tags = {D3.1,Source Separation},
pages = {177--180},
title = {{Non-Negative Matrix Factorization for Polyphonic Music Transcription}},
url = {http://www.merl.com/papers/docs/TR2003-139.pdf},
year = {2003}
}
@article{GouyonFabien;Dixon2005b,
author = {{Gouyon, Fabien; Dixon}, Simon},
file = {:Users/carthach/Documents/Mendeley Desktop/Gouyon, Fabien Dixon - 2005 - A review of automatic rhythm description systems.pdf:pdf},
journal = {Computer music journal},
title = {{A review of automatic rhythm description systems}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/comj.2005.29.1.34},
year = {2005}
}
@inproceedings{robel2003transient,
author = {R{\"{o}}bel, Axel},
booktitle = {Proc. Int. Computer Music Conference (ICMC)},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {247--250},
title = {{Transient detection and preservation in the phase vocoder}},
year = {2003}
}
@inproceedings{Tzanetakis:2001,
abstract = {Musical genres are categorical descriptions that are used to describe music. They are commonly used to structure the increasing amounts of music available in digital form on the Web and are important for music information retrieval. Genre categorization for audio has traditionally been performed manually. A particular musical genre is characterized by statistical properties related to the instrumentation, rhythmic structure and form of its members. In this work, algorithms for the automatic genre categorization of audio signals are described. More specifically, we propose a set of features for representing texture and instrumentation. In addition a novel set of features for representing rhythmic structure and strength is proposed. The performance of those feature sets has been evaluated by training statistical pattern recognition classifiers using real world audio collections. Based on the automatic hierarchical genre classification two graphical user interfaces for browsing and interacting with large audio collections have been developed.},
author = {Tzanetakis, G and Essl, G and Cook, P},
booktitle = {Proceedings of the Second International Symposium on Music Information Retrieval},
doi = {10.1109/TSA.2002.800560},
file = {:Users/carthach/Documents/Mendeley Desktop/Tzanetakis, Essl, Cook - 2001 - Automatic musical genre classification of audio signals.pdf:pdf},
issn = {1063-6676},
keywords = {Folder - Feature extraction,Source Separation,file-import-09-02-12,music},
mendeley-tags = {Folder - Feature extraction,Source Separation,file-import-09-02-12,music},
pages = {6 total pages},
title = {{Automatic musical genre classification of audio signals}},
year = {2001}
}
@misc{RobertM.KellerandDavidMorrisonandStephenJo,
author = {{Robert M. Keller and David Morrison and Stephen Jo} and Keller, Robert M and Morrison, David and Jo, Stephen},
keywords = {Robert M. Keller and David Morrison and Stephen Jo,Source Separation},
mendeley-tags = {Source Separation},
title = {{A Computational Framework Enhancing Jazz Creativity}},
url = {http://www.cs.hmc.edu/{~}keller/jazz/improvisor/jazzCreativity.pdf}
}
@inproceedings{Ganseman:2010,
author = {Ganseman, Joachim and Mysore, Gautham J and Scheunders, Paul and Abel, Jonathan S},
booktitle = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {219--224},
publisher = {Ann Arbor, MI: MPublishing, University of Michigan Library},
title = {{Evaluation of a Score-informed Source Separation System.}},
year = {2010}
}
@article{Vuust2014,
abstract = {Musical rhythm, consisting of apparently abstract intervals of accented temporal events, has a remarkable capacity to move our minds and bodies. How does the cognitive system enable our experiences of rhythmically complex music? In this paper, we describe some common forms of rhythmic complexity in music and propose the theory of predictive coding (PC) as a framework for understanding how rhythm and rhythmic complexity are processed in the brain. We also consider why we feel so compelled by rhythmic tension in music. First, we consider theories of rhythm and meter perception, which provide hierarchical and computational approaches to modeling. Second, we present the theory of PC, which posits a hierarchical organization of brain responses reflecting fundamental, survival-related mechanisms associated with predicting future events. According to this theory, perception and learning is manifested through the brain's Bayesian minimization of the error between the input to the brain and the brain's prior expectations. Third, we develop a PC model of musical rhythm, in which rhythm perception is conceptualized as an interaction between what is heard ("rhythm") and the brain's anticipatory structuring of music ("meter"). Finally, we review empirical studies of the neural and behavioral effects of syncopation, polyrhythm and groove, and propose how these studies can be seen as special cases of the PC theory. We argue that musical rhythm exploits the brain's general principles of prediction and propose that pleasure and desire for sensorimotor synchronization from musical rhythm may be a result of such mechanisms.},
author = {Vuust, Peter and Witek, Maria A G},
doi = {10.3389/fpsyg.2014.01111.},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Meter,Pleasure,Predictive coding,Rhythm,Rhythmic complexity},
number = {OCT},
pages = {1--14},
pmid = {25324813},
title = {{Rhythmic complexity and predictive coding: A novel approach to modeling rhythm and meter perception in music}},
volume = {5},
year = {2014}
}
@phdthesis{Collins2011,
author = {Collins, T},
file = {::},
title = {{Improved methods for pattern discovery in music , with applications in automated stylistic composition}},
url = {http://oro.open.ac.uk/30103/},
year = {2011}
}
@article{SebastianBockAndreasArztFlorianKrebs2012,
author = {{Sebastian B{\"{o}}ck , Andreas Arzt , Florian Krebs}, Markus Schedl},
file = {:Users/carthach/Documents/Mendeley Desktop/B{\"{o}}ck et al. - 2012 - Online Real-time Onset Detection With Recurrent Neural Networks.pdf:pdf},
pages = {15--18},
title = {{ONLINE REAL-TIME ONSET DETECTION WITH RECURRENT NEURAL NETWORKS}},
year = {2012}
}
@article{Bruderer2006a,
author = {Bruderer, MJ J and McKinney, MF F and Kohlrausch, Armin},
file = {:Users/carthach/Documents/Mendeley Desktop/Bruderer, McKinney, Kohlrausch - 2006 - Structural boundary perception in popular music.pdf:pdf},
journal = {ISMIR},
keywords = {Source Separation,music cognition,music percep-,music segmentation,music structure,tion},
mendeley-tags = {Source Separation},
title = {{Structural boundary perception in popular music.}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.100.1661{\&}rep=rep1{\&}type=pdf},
volume = {4},
year = {2006}
}
@inproceedings{han2010informed,
author = {Han, Yushen and Raphael, Christopher},
booktitle = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {315--320},
title = {{Informed Source Separation of Orchestra and Soloist.}},
year = {2010}
}
@article{Marxer:2011,
author = {Marxer, Richard},
file = {::},
keywords = {source separation},
mendeley-tags = {source separation},
title = {{Signal decomposition by a joint pitch, timbre and wideband model}},
year = {2011}
}
@article{barry2005drum,
author = {Barry, Dan and Fitzgerald, Derry and Coyle, Eugene and Lawlor, Bob},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {IET},
title = {{Drum source separation using percussive feature detection and spectral modulation}},
year = {2005}
}
@article{Scaringella2006,
author = {Scaringella, Nicolas and Zoia, Giorgio and Mlynek, Daniel},
file = {:Users/carthach/Documents/Mendeley Desktop/Scaringella, Zoia, Mlynek - 2006 - Automatic Genre Classification of Music Content (A Survey).pdf:pdf},
journal = {Signal Processing Magazine, {\ldots}},
number = {March 2006},
pages = {133--141},
title = {{Automatic Genre Classification of Music Content (A Survey)}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1598089},
year = {2006}
}
@article{yoshii2005inter,
author = {Yoshii, Kazuyoshi and Goto, Masataka and Okuno, Hiroshi G},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
publisher = {IET},
title = {{INTER: D: a drum sound equalizer for controlling volume and timbre of drums}},
year = {2005}
}
@article{durrieu2011musically,
author = {Durrieu, J and David, Bertrand and Richard, Ga{\"{e}}l},
journal = {Selected Topics in Signal Processing, IEEE Journal of},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {6},
pages = {1180--1191},
publisher = {IEEE},
title = {{A musically motivated mid-level representation for pitch estimation and musical audio source separation}},
volume = {5},
year = {2011}
}
@article{Holzapfel2010,
abstract = {In this paper, we suggest a novel group delay based method for the onset detection of pitched instruments. It is proposed to approach the problem of onset detection by examining three dimensions separately: phase (i.e., group delay), magnitude and pitch. The evaluation of the suggested onset detectors for phase, pitch and magnitude is performed using a new publicly available and fully onset annotated database of monophonic recordings which is balanced in terms of included instruments and onset samples per instrument, while it contains different performance styles. Results show that the accuracy of onset detection depends on the type of instruments as well as on the style of performance. Combining the information contained in the three dimensions by means of a fusion at decision level leads to an improvement of onset detection by about 8{\%} in terms of F-measure, compared to the best single dimension.},
author = {Holzapfel, A. and Stylianou, Y. and Gedik, A.C. and Bozkurt, B.},
doi = {10.1109/TASL.2009.2036298},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {Automatic music transcription,group delay,music information retrieval,onset detection},
title = {{Three Dimensions of Pitched Instrument Onset Detection}},
volume = {18},
year = {2010}
}
@article{Bock2012,
abstract = {We present a new onset detection algorithm which operates online in real time without delay. Our method incorporates a recurrent neural network to model the sequence of onsets based solely on causal audio signal information. Comparative performance against existing state-of-the-art online and offline algorithms was evalu- ated using a very large database. The new method – despite being an online algorithm – shows performance only slightly short of the best existing offline methods while outperforming standard ap- proaches. 1.},
author = {B{\"{o}}ck, S and Arzt, A and Krebs, F and Schedl, M},
journal = {Proceedings of the 15th {\ldots}},
pages = {15--18},
title = {{Online realtime onset detection with recurrent neural networks}},
url = {http://dafx12.york.ac.uk/papers/dafx12{\_}submission{\_}4.pdf},
year = {2012}
}
@article{Glover2011,
abstract = {Real-time musical note onset detection plays a vital role in many audio analysis processes, such as score following, beat detection and various sound synthesis by analysis methods. This article provides a review of some of the most commonly used techniques for real-time onset detection. We suggest ways to improve these techniques by incorporating linear prediction as well as presenting a novel algorithm for real-time onset detection using sinusoidal modelling. We provide comprehensive results for both the detection accuracy and the computational performance of all of the described techniques, evaluated using Modal, our new open source library for musical onset detection, which comes with a free database of samples with hand-labelled note onsets.},
author = {Glover, John and Lazzarini, Victor and Timoney, Joseph},
doi = {10.1186/1687-6180-2011-68},
file = {:Users/carthach/Documents/Mendeley Desktop/Glover, Lazzarini, Timoney - 2011 - Real-time detection of musical onsets with linear prediction and sinusoidal modeling.pdf:pdf},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in {\ldots}},
language = {en},
month = {sep},
number = {1},
pages = {68},
publisher = {Springer},
title = {{Real-time detection of musical onsets with linear prediction and sinusoidal modeling}},
url = {http://asp.eurasipjournals.com/content/2011/1/68 http://link.springer.com/article/10.1186/1687-6180-2011-68},
volume = {2011},
year = {2011}
}
@article{Dixon2006,
author = {Dixon, S},
file = {:Users/carthach/Documents/Mendeley Desktop/Dixon - 2006 - Onset detection revisited.pdf:pdf},
journal = {Proceedings of the 9th International Conference on {\ldots}},
pages = {133--137},
title = {{Onset detection revisited}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.414.8898{\&}rep=rep1{\&}type=pdf},
year = {2006}
}
@inproceedings{Heo2013a,
author = {Heo, Hoon and Sung, Dooyong and Lee, Kyogu},
booktitle = {2013 IEEE International Conference on Multimedia and Expo (ICME)},
issn = {1945-7871},
language = {English},
month = {jul},
pages = {1--6},
publisher = {IEEE},
title = {{Note onset detection based on harmonic cepstrum regularity}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6607461},
year = {2013}
}
@article{Katz2015,
abstract = {Theories of metrical structure postulate the existence of several degrees of beat strength. While previous work has clearly established that humans are sensitive to the distinction between strong beats and weak ones, there is little evidence for a more fine grained distinction between intermediate levels. Here, we present experimental data showing that attention can be allocated to an intermediate level of beat strength. Comparing the effects of short exposures to 6/8 and 3/4 metrical structures on a tone detection task, we observe that subjects respond differently to beats of intermediate strength than to weak beats.},
author = {Katz, Jonah and Chemla, Emmanuel and Pallier, Christophe},
issn = {1932-6203},
journal = {PloS one},
keywords = {Rhythm},
mendeley-tags = {Rhythm},
month = {jan},
number = {11},
pages = {e0140895},
publisher = {Public Library of Science},
title = {{An Attentional Effect of Musical Metrical Structure.}},
url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0140895},
volume = {10},
year = {2015}
}
@article{Dressler,
author = {Dressler, Karin},
journal = {Proc. MIREX},
keywords = {D3.1,MIREX 2006,Source Separation,audio melody extraction},
mendeley-tags = {D3.1,Source Separation},
title = {{An Auditory Streaming Approach on Melody Extraction}},
url = {http://www.music-ir.org/mirex/abstracts/2006/MIREX2006Abstracts.pdf{\#}page=45},
year = {2006}
}
@article{Eyben2010,
author = {Eyben, Florian and Sebastian, B and Graves, Alex},
file = {:Users/carthach/Documents/Mendeley Desktop/Eyben et al. - 2010 - Universal Onset Detection with Bidirectional Long-Short Term Memory Neural Networks.pdf:pdf},
keywords = {ML,deepLearning},
mendeley-tags = {ML,deepLearning},
number = {Ismir},
pages = {589--594},
title = {{UNIVERSAL ONSET DETECTION WITH BIDIRECTIONAL LONG SHORT-TERM MEMORY NEURAL NETWORKS}},
year = {2010}
}
@incollection{Regnier:2009,
author = {Regnier, Lise and Peeters, Geoffroy},
booktitle = {Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on},
keywords = {D3.1,Source Separation},
mendeley-tags = {D3.1,Source Separation},
pages = {1685--1688},
title = {{SINGING VOICE DETECTION IN MUSIC TRACKS USING DIRECT VOICE VIBRATO DETECTION}},
url = {http://hal.archives-ouvertes.fr/docs/00/66/23/12/PDF/index-11.pdf},
year = {2009}
}
@article{Bay,
author = {Bay, Mert and Burgoyne, John Ashley and Crawford, Tim and Roure, David De and Downie, J Stephen and Ehmann, Andreas and Fields, Benjamin and Fujinaga, Ichiro and Page, Kevin and Smith, Jordan B L},
file = {:Users/carthach/Documents/Mendeley Desktop/Bay et al. - Unknown - Structural Analysis of Large Amounts of Music Information.pdf:pdf},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {1--15},
title = {{Structural Analysis of Large Amounts of Music Information}}
}
@article{Conklin1995,
abstract = {This paper examines the prediction and generation of music using a multiple view-point system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short-term and long-term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are used for analysis and prediction, and we conjecture that highly predictive theories will also generate original, acceptable, works. Although the quality of the works generated is hard to quantify objectively, the predictive power of models can be measured by the notion of entropy, or unpredictability. Highly predictive theories will produce low-entropy estimates of a musical language. The methods developed are applied to the Bach chorale melodies. Multiple-viewpoint systems are learned from a sample of 95 chorales, estimates of entropy are produced, and a predictive theory is used to generate new, unseen pieces.},
author = {Conklin, Darrell and Witten, Ian H.},
doi = {10.1080/09298219508570672},
file = {::},
issn = {0929-8215},
journal = {Journal of New Music Research},
number = {1},
pages = {51--73},
title = {{Multiple viewpoint systems for music prediction}},
volume = {24},
year = {1995}
}
@article{Schluter,
author = {Schl{\"{u}}ter, Jan and B{\"{o}}ck, Sebastian},
file = {::},
pages = {5--6},
title = {{CNN-BASED AUDIO ONSET DETECTION MIREX SUBMISSION}}
}
@inproceedings{hennequin2011score,
author = {Hennequin, Romain and David, Bertrand and Badeau, Roland},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {45--48},
title = {{Score informed audio source separation using a parametric model of non-negative spectrogram}},
year = {2011}
}
@article{Panagakis2013,
author = {Panagakis, Yannis and Kotropoulos, Constantine},
issn = {1687-4722},
journal = {EURASIP Journal on Audio, Speech, and Music Processing},
keywords = {Source Separation},
language = {en},
mendeley-tags = {Source Separation},
month = {jun},
number = {1},
pages = {13},
publisher = {Springer},
title = {{Music classification by low-rank semantic mappings}},
url = {http://asmp.eurasipjournals.com/content/2013/1/13},
volume = {2013},
year = {2013}
}
@inproceedings{BernardesEAW2015,
author = {Bernardes, G. and Davies, M. and Guedes, C.},
booktitle = {Proceedings of 1st International Congress for Electroacoustic Music - Electroacoustic Winds 2015},
keywords = {Timbre},
mendeley-tags = {Timbre},
title = {{A Pure Data Spectro-Morphological Analysis Toolkit for Sound-Based Composition}},
year = {2015}
}
@article{Herremans2014,
author = {Herremans, Dorien and S{\"{o}}rensen, Kenneth and Conklin, Darrell},
issn = {2223-3881},
journal = {International Computer Music Conference Proceedings},
month = {nov},
publisher = {Michigan Publishing, University of Michigan Library},
title = {{Sampling the extrema from statistical models of music with variable neighbourhood search}},
url = {http://hdl.handle.net/2027/spo.bbp2372.2014.169},
volume = {2014},
year = {2014}
}
@article{Lukashevich2008,
author = {Lukashevich, HM M},
file = {:Users/carthach/Documents/Mendeley Desktop/Lukashevich - 2008 - Towards Quantitative Measures of Evaluating Song Segmentation.pdf:pdf},
journal = {ISMIR},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {375--380},
title = {{Towards Quantitative Measures of Evaluating Song Segmentation.}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=OHp3sRnZD-oC{\&}oi=fnd{\&}pg=PA375{\&}dq=TOWARDS+QUANTITATIVE+MEASURES+OF+EVALUATING+SONG+SEGMENTATION{\&}ots=oDRSvClv83{\&}sig=STxA2yALKfQyZN1JpwjmbLt9oc4},
year = {2008}
}
@article{Pearce2005a,
abstract = {The paper concerns the use of multiple viewpoint representation schemes for prediction with statistical models of monophonic music. We present an experimental comparison of the performance of two techniques for combining predictions within the multiple viewpoint framework. The results demonstrate that a new technique based on a weighted geometric mean outperforms existing techniques. This finding is discussed in terms of previous research in machine learning.},
author = {Pearce, Marcus and Conklin, Darrell and Wiggins, Geraint},
file = {::},
isbn = {3-540-24458-1},
issn = {03029743},
journal = {Computer Music Modeling and Retrieval},
pages = {295--312},
title = {{Methods for Combining Statistical Models of Music}},
url = {http://www.springerlink.com/index/CPTVYB2CC735HDX8.pdf},
volume = {3310},
year = {2005}
}
@article{Arumi:2006,
author = {Arumi},
keywords = {Source Separation,dataflow,implementation},
mendeley-tags = {Source Separation},
title = {{A dataflow pattern catalog for sound and music computing}},
year = {2006}
}
@incollection{Covach2005,
author = {Covach, John},
booktitle = {Engaging Music: Essays in Music Analysis},
editor = {Stein, Deborah},
file = {::},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
number = {1},
publisher = {Oxford University Press},
title = {{FORM IN ROCK MUSIC A Primer}},
year = {2005}
}
@article{Sampaio2008,
abstract = {CinBalada is a system for automatic creation of polyphonic rhythmic performances by mixing elements from different musical styles. This system is based on agents that act as musicians playing percussion instruments in a drum circle. Each agent has to choose from a database the rhythm pattern of its instrument that satisfies the “rhythmic role” assigned to him in order to produce a collectively- consistent rhythmic performance. A rhythmic role is a concept that we proposed here with the objective of representing culture-specific rules for creation of polyphonic performances.},
author = {Sampaio, Pablo Azevedo and Ramalho, Geber and Tedesco, Patr{\'{i}}cia},
doi = {10.1590/S0104-65002008000300004},
file = {::},
issn = {0104-6500},
journal = {Journal of the Brazilian Computer Society},
keywords = {multiagent,rhythm composition,rhythmic role},
number = {3},
pages = {19},
title = {{CinBalada: a multiagent rhythm factory}},
url = {http://www.scielo.br/scielo.php?script=sci{\_}arttext{\&}pid=S0104-65002008000300004{\&}lng=en{\&}nrm=iso{\&}tlng=en},
volume = {14},
year = {2008}
}
@article{Smith2013,
author = {Smith, JBL B L and Chew, Elaine},
file = {:Users/carthach/Documents/Mendeley Desktop/Smith, Chew - 2013 - A Meta-Analysis of the MIREX Structure Segmentation Task.pdf:pdf},
journal = {Proc. of the 14th International Society for Music {\ldots}},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
title = {{A Meta-Analysis of the MIREX Structure Segmentation Task}},
url = {http://www.music.mcgill.ca/{~}jordan/documents/smith2013-ismir-mirex{\_}meta{\_}analysis.pdf},
year = {2013}
}
@article{Bogdanov:2011,
abstract = {Measuring music similarity is essential for multimedia retrieval. From a content-based point of view, this task can be regarded as obtaining a suitable distance measurement between songs defined on a certain feature space. In this paper, we propose three such distance measures. First, a low-level measure based on tempo-related aspects. Second, a high-level semantic measure based on regression by support vector machines producing different groups of musical dimensions such as genre and culture, moods and instruments, or rhythm and tempo. Finally, a third distance, a hybrid measure which combines the above-mentioned distance measures with two state-of-the-art low-level measures: an Euclidean distance based on principal component analysis of timbral, temporal, and tonal descriptors, and a timbral distance based on single Gaussian MFCC modeling. We evaluate our proposed measures against a number of state-of-the-art measures. We do this objectively based on a comprehensive set of ground truth musical collections, and subjectively by means of listeners{\{}$\backslash$textquoteright{\}} playlist similarity and inconsistency ratings. Results show that, in spite of being conceptually different, the presented methods achieve performance comparable to the considered baseline approaches in the case of low-level tempo-based and semantic classifier-based measures, or even higher performance in the case of the hybrid distance. Furthermore, they open up the possibility to explore distance metrics that are based on truly semantic notions.
},
author = {Bogdanov, D and Serr{\`{a}}, J and Wack, Nicolas and Herrera, P and Serra, Xavier},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
pages = {687--701},
title = {{Unifying Low-level and High-level Music Similarity Measures}},
url = {files/publications/bogadnov{\_}IEEETMult2011.pdf},
volume = {13},
year = {2011}
}
@inproceedings{li2008musical,
author = {Li, Yipeng and Wang, DeLiang},
booktitle = {Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on},
keywords = {Source Separation},
mendeley-tags = {Source Separation},
organization = {IEEE},
pages = {173--176},
title = {{Musical sound separation using pitch-based labeling and binary time-frequency masking}},
year = {2008}
}
@article{Noll:1967,
author = {Noll, A Michael},
journal = {The journal of the acoustical society of America},
keywords = {Melody extraction,Source Separation,cepstrum,pitch},
mendeley-tags = {Melody extraction,Source Separation,cepstrum,pitch},
pages = {293},
title = {{Cepstrum pitch determination}},
volume = {41},
year = {1967}
}
